{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import the gym module\n",
    "import gym\n",
    "import numpy as np\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a breakout environment\n",
    "env = gym.make('BreakoutDeterministic-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "    img = np.mean(img, axis=2).astype(np.uint8) # to gray, uint8 for low memory\n",
    "    img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "    img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    a = env.action_space.sample()\n",
    "    s2, _, done, info = env.step(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb5bfcc2860>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb5bfbda208>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(preprocess(s2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random \n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "    \n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        self.deque.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.deque, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameHistory(object):\n",
    "    # consecutive frame을 4개씩 병합하여 새로운 x를 하나씩 만드는 클래스\n",
    "    # 만들어진 x는 4개씩 concat하고 s를 만들어서 DQN의 input으로 들어감\n",
    "    def __init__(self, env, k_history=4):\n",
    "        self.state_deque = deque(maxlen=k_history) # [x1, x2, x3, x4]\n",
    "        self.frame_history_deque = deque(maxlen=k_history) #[s1, s2, s3, s4]\n",
    "        self.k_history = k_history\n",
    "        \n",
    "        # 최초에는 초기 화면(env.reset)으로 채워둠\n",
    "        self.env = env\n",
    "        self.initial_state = self.preprocess(self.env.reset())\n",
    "        self.reset()\n",
    "        \n",
    "    def preprocess(self, img):\n",
    "        # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "        img = np.mean(img, axis=2).astype(np.uint8) # to gray, uint8 for low memory\n",
    "        img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "        img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "        return img\n",
    "    \n",
    "    def append_frame(self, s):\n",
    "        # if full, aggregate and clear old frames, append new frame to history queue\n",
    "        if len(self) == self.k_history:\n",
    "            self.state_deque.append(self.aggregate_frame())\n",
    "            \n",
    "        self.frame_history_deque.append(self.preprocess(s))\n",
    "        return\n",
    "        \n",
    "    def aggregate_frame(self):\n",
    "        if len(self) < self.k_history:\n",
    "            raise ValueError('not enough frames in history, expected %s, but got %s'%(self.k_history, len(self)))\n",
    "        \n",
    "        # element-wise maximum to aggregate\n",
    "        return np.maximum.reduce([self.frame_history_deque.pop() for _ in range(len(self))])\n",
    "    \n",
    "    def get_state(self):\n",
    "        S = np.array([self.state_deque[i] for i in range(self.k_history)]) # S = [x1, x2, x3, x4], (4, 1, 80, 80)\n",
    "        S = np.swapaxes(S, 0, 1) # (1, 4, 80, 80)\n",
    "        return S\n",
    "    \n",
    "    def reset(self):\n",
    "        self.frame_history_deque = deque(maxlen=k_history)\n",
    "        self.state_deque = deque(maxlen=k_history)\n",
    "        \n",
    "        for _ in range(self.k_history):\n",
    "            self.state_deque.append(self.initial_state)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_history_deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_dim, n_action):\n",
    "        super(DQN, self).__init__()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, 32, (8, 8), stride=4), nn.ReLU(), # conv 1\n",
    "            nn.Conv2d(32, 64, (4, 4), stride=2), nn.ReLU(), # conv 2\n",
    "            nn.Conv2d(64, 64, (3, 3), stride=1), nn.ReLU() # conv 3\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*6*6, 512), nn.ReLU(), # hidden layer\n",
    "            nn.Linear(512, n_action)\n",
    "        )\n",
    "        \n",
    "    def to_tensor(self, img):\n",
    "        img = torch.tensor(img, dtype=torch.float32, device=self.device).cuda(non_blocking=True) # to tensor\n",
    "        img /= 255                                   # normalize into 0-1\n",
    "        while img.dim() < 4 :                        # 4-dim\n",
    "            img = img.unsqueeze(0)\n",
    "        return img\n",
    "        \n",
    "    def forward(self, frames):\n",
    "        frames = self.to_tensor(frames) \n",
    "        conved = self.conv(frames)\n",
    "        conved = conved.view(conved.size(0), -1)\n",
    "        output = self.fc(conved)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(2, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2304, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############\n",
    "k_history = 2 ## number of skipped frame\n",
    "#############\n",
    "\n",
    "behavior_net = DQN(in_dim=k_history, n_action=4)\n",
    "target_net = DQN(in_dim=k_history, n_action=4)\n",
    "\n",
    "behavior_net.to(behavior_net.device) # model to cuda\n",
    "target_net.to(target_net.device)     # model to cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_decay(n_total_frame):\n",
    "    return np.max([1 - 9.0*1e-07*n_total_frame, 0.1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grow_batch_size(current_memory, max_memory):\n",
    "    # grow batch size from 32 to 128 as respect to memory size\n",
    "    memory_ratio = current_memory / max_memory\n",
    "    new_batch_size = int(32 + 64*memory_ratio)\n",
    "    return new_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter():\n",
    "    def __init__(self, env, behavior_net, target_net, k_history=4, train_method='DQN'):\n",
    "        self.env = env\n",
    "        self.train_method = train_method\n",
    "        \n",
    "        # networks\n",
    "        self.behavior_net = behavior_net\n",
    "        self.target_net = target_net\n",
    "        self.behavior_net.to(self.behavior_net.device) # model to cuda\n",
    "        self.target_net.to(self.target_net.device)     # model to cuda\n",
    "        \n",
    "        # train parameters\n",
    "        self.gamma = 0.99 #as written in paper\n",
    "        self.criterion = nn.SmoothL1Loss() # huber loss (error-clipping)\n",
    "        self.optim = torch.optim.RMSprop(self.behavior_net.parameters(), lr=0.00025) #as written in paper\n",
    "        self.k_history = k_history\n",
    "        \n",
    "        # train details\n",
    "        self.total_frame = 0\n",
    "        self.total_episode = 0\n",
    "        self.batch_size = 32\n",
    "        self.max_replay = 1000000 # as written in paper\n",
    "        self.min_replay = 50000 # as written in the paper\n",
    "        self.replay_memory = ReplayMemory(self.max_replay)\n",
    "        self.frame_history = FrameHistory(env=self.env, k_history=self.k_history)\n",
    "        self.reward_ls = []\n",
    "    \n",
    "    \n",
    "    def train(self, max_total_frame):\n",
    "        while self.total_frame < max_total_frame :\n",
    "            self.total_episode += 1\n",
    "            \n",
    "            s = self.env.reset()  # small s represents each frame\n",
    "            self.frame_history.reset() #frame history reset\n",
    "            lives_remain = 5        # lives\n",
    "            n_step = 0       # each episode의 step의 횟수 \n",
    "            \n",
    "            # re-calculate epsilon, batch_size\n",
    "            e = epsilon_decay(self.total_frame)\n",
    "            batch_size = grow_batch_size(len(self.replay_memory), self.max_replay)\n",
    "            \n",
    "            done = False\n",
    "            while not done:\n",
    "                # update target_net every 10,000 steps\n",
    "                if self.total_frame%10000 == 0 :\n",
    "                    self.update_target()\n",
    "                \n",
    "                self.total_frame += 1\n",
    "                n_step += 1\n",
    "                \n",
    "                # start of every k consecutive frames, choose action!                \n",
    "                if n_step%self.k_history == 1:\n",
    "                    # S = [x1, x2, x3, x4] for neural-network input\n",
    "                    # x represents feature(element-wise max) from every k consecutive frames(k*s)\n",
    "                    S = self.frame_history.get_state()\n",
    "                    r_sum = 0   # sum of reward for the following k frames\n",
    "                    \n",
    "                    # if it is the first frame of the game, Start game!\n",
    "                    if n_step == 1:\n",
    "                        a = 1\n",
    "                    else:  # else, Choose an action by e-greedy\n",
    "                        if np.random.rand(1) < e :\n",
    "                            a = self.env.action_space.sample()\n",
    "                        else:\n",
    "                            q_pred = self.behavior_net(S)\n",
    "                            a = torch.argmax(q_pred).item()\n",
    "                \n",
    "                # repeat the same action k times\n",
    "                s_next, r, done, info, = self.env.step(a)\n",
    "                \n",
    "                if info['ale.lives'] < lives_remain:\n",
    "                    lives_remain = info['ale.lives'] # life를 잃으면 페널티\n",
    "                    r -= 1\n",
    "                \n",
    "                self.frame_history.append_frame(s_next)\n",
    "                r_sum += r\n",
    "\n",
    "                # end of every k consecutive frames\n",
    "                # genenrate new x, and update S\n",
    "                if n_step%self.k_history == 0 :\n",
    "                    S_new = self.frame_history.get_state()\n",
    "                    self.replay_memory.append(S, a, r_sum, S_new, done)\n",
    "                \n",
    "                # no training when not enough replay\n",
    "                if len(self.replay_memory) < self.min_replay:\n",
    "                    continue\n",
    "                \n",
    "                # train every k steps\n",
    "                if n_step % self.k_history == 0:\n",
    "                    mini_batch = self.replay_memory.sample(batch_size)\n",
    "                    self.train_batch(mini_batch)                \n",
    "                # single-episode(game) is now done\n",
    "            \n",
    "            # no updating target, no testing when not enough replay\n",
    "            if len(self.replay_memory) < self.min_replay:\n",
    "                continue\n",
    "\n",
    "            # update target, do test for every 10 episodes(games)\n",
    "            if self.total_episode%10 == 0:\n",
    "                print('total_frame: %s'%self.total_frame)\n",
    "                self.test()           \n",
    "                    \n",
    "    def train_batch(self, mini_batch):\n",
    "        batch_size = len(mini_batch)\n",
    "        S = np.array([tup[0] for tup in mini_batch]).squeeze(1) # State\n",
    "        A = np.array([tup[1] for tup in mini_batch]) # actions\n",
    "        R = np.array([tup[2] for tup in mini_batch]) # rewards\n",
    "        S_next = np.array([tup[3] for tup in mini_batch]).squeeze(1) # next_State\n",
    "        D = np.array([tup[4] for tup in mini_batch]) # dones\n",
    "        \n",
    "        q_targets = self.target_net(S) # Q-values of current state with targetDDQN\n",
    "        q_targets_next = self.target_net(S_next) # Q-values of next state from targetDDQN\n",
    "        \n",
    "        if self.train_method=='DQN':\n",
    "            for i in range(batch_size):\n",
    "                a, r, done = A[i], R[i], D[i]\n",
    "                if done:\n",
    "                    q_targets[i, a] = r\n",
    "                else:\n",
    "                    q_targets[i, a] = r + self.gamma*torch.max(q_targets_next[i])\n",
    "        \n",
    "        elif self.train_method=='DoubleDQN':\n",
    "            next_behavior_net_actions = torch.argmax(self.behavior_net(S_next), dim=1) # choose argmax actions from behaviorDDQN in S_next\n",
    "                    \n",
    "            for i in range(batch_size):\n",
    "                a, r, done = A[i], R[i], D[i]\n",
    "                next_behavior_net_action = next_behavior_net_actions[i].item() # choose argmax actions from behaviorDDQN in S_next\n",
    "                if done:\n",
    "                    q_targets[i, a] = r\n",
    "                else:\n",
    "                    q_targets[i, a] = r + self.gamma*q_targets_next[i, next_behavior_net_action]\n",
    "            \n",
    "        # 예측치(pred)와 목표치(true)\n",
    "        q_behaviors = self.behavior_net(S)\n",
    "        self.optim.zero_grad()\n",
    "        loss = self.criterion(q_targets, q_behaviors)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return\n",
    "\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.behavior_net.state_dict())\n",
    "        return \n",
    "    \n",
    "    \n",
    "    def test(self):\n",
    "        self.frame_history.reset()\n",
    "        initial_state = self.env.reset()  # save initial state for comparison\n",
    "        s = self.env.reset()\n",
    "        e = 0.05 # e-greedy, \n",
    "        \n",
    "        n_step = 0\n",
    "        r_sum = 0\n",
    "        \n",
    "        done=False\n",
    "        while not done:\n",
    "            n_step += 1\n",
    "            \n",
    "            # 초기 화면에서는 1의 액션을 실행\n",
    "            if n_step==1:\n",
    "                a = 1\n",
    "            else:\n",
    "                # e-greedy search, e=0.05\n",
    "                if np.random.rand(1) < e:\n",
    "                    a = self.env.action_space.sample()\n",
    "                else:\n",
    "                    S = self.frame_history.get_state()\n",
    "                    a = torch.argmax(self.behavior_net(S)).item()\n",
    "            \n",
    "            s, r, done, _ = self.env.step(a)\n",
    "            self.frame_history.append_frame(s)\n",
    "            r_sum += r\n",
    "        \n",
    "        self.reward_ls.append(r_sum)\n",
    "        print('Total Step: %s \\t Total Score: %s'%(n_step, r_sum))\n",
    "        return r_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = Fitter(env, behavior_net, target_net, k_history=k_history, train_method='DoubleDQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_frame: 1819\n",
      "Total Step: 285 \t Total Score: 1.0\n",
      "total_frame: 3948\n",
      "Total Step: 377 \t Total Score: 0.0\n",
      "total_frame: 5728\n",
      "Total Step: 325 \t Total Score: 0.0\n",
      "total_frame: 7792\n",
      "Total Step: 329 \t Total Score: 0.0\n",
      "total_frame: 9688\n",
      "Total Step: 512 \t Total Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "max_frame = 10000000\n",
    "fitter.train(max_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fitter.reward_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# how skip-frame works\n",
    "plt.imshow(fitter.S[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
