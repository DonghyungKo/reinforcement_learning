{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import the gym module\n",
    "import gym\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a breakout environment\n",
    "env = gym.make('BreakoutNoFrameskip-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "    img = np.dot(img[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "    img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "    img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "    return img.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1224"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "done = False\n",
    "n = 0\n",
    "while not done:\n",
    "    a = random.choice([0, 1, 2, 3])\n",
    "    s, _, done, _ = env.step(a)\n",
    "    n += 1\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbd983f6ac8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbd9838e550>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADHlJREFUeJzt3V2sZeVdx/HvrwNjlUKG4S0jhzqQUArGMrSTCqExyjiGIgHaFANWQwiRm2og1lTgShNN6E1LLwwJ8iIXWEAKdkKQSqYQbWpGXpWXgUIR4QhlKAMBS1Id+Hux19gjnumsc87e+5x1nu8n2dl7PXvtvZ51Vn57rWftfdY/VYWktnxguTsgafoMvtQggy81yOBLDTL4UoMMvtQggy81aEnBT3JmkmeSPJfkinF1StJkZbE/4EmyBvgesBWYBR4ELqyqp8bXPUmTcMASXvtJ4Lmqeh4gya3AucA+g79+/QdqZmZNrzd/4fGDl9A1aXXZ+Etv95pvdvZddu9+L/ubbynBPxp4ae4ygV/+aS+YmVnDtnsO7/Xmv/fhTy2+Z9Iq85f3fKfXfOec9cNe8y1ljD/fp8r/GzckuTTJQ0keen33e0tYnKRxWUrwZ4Fj5kzPAC+/f6aquq6qNlfV5sPW+yWCtBIsJYkPAscnOTbJWuACYNt4uiVpkhY9xq+qPUl+H/gWsAa4saqeHFvPJE3MUk7uUVX3APeMqS+SpsRBt9Qggy81yOBLDTL4UoMMvtQggy81yOBLDTL4UoMMvtQggy81yOBLDTL4UoMMvtQggy81yOBLDTL4UoMMvtQggy81aL/BT3Jjkl1JnpjTtj7JfUme7e4PnWw3JY1Tnz3+XwFnvq/tCmB7VR0PbO+mJQ3EfoNfVf8A7H5f87nAzd3jm4HzxtwvSRO02DH+UVX1CkB3f+T4uiRp0iZ+cs8SWtLKs9jgv5pkA0B3v2tfM1pCS1p5FpvEbcBF3eOLgG+OpzuSpqHP13lfB/4JOCHJbJJLgKuBrUmeBbZ205IGYr8ltKrqwn08tWXMffk/jvjuukm+vdQ0B91Sgwy+1CCDLzXI4EsNMvhSgwy+1CCDLzVov9/jL5fPHP7IcndBWrXc40sNMvhSgwy+1CCDLzXI4EsNMvhSgwy+1KAV+z3+mnh9PmlS3ONLDTL4UoP6XHPvmCT3J9mZ5Mkkl3XtltGSBqrPHn8P8MWqOhE4FfhCkpOwjJY0WH1KaL1SVY90j98GdgJHYxktabAWNMZPshE4BdiBZbSkweod/CQfAr4BXF5Vby3gdZbQklaYXt/jJzmQUehvqao7u+ZXk2yoqld+WhmtqroOuA7gYx87sPp27LyD/rPvrNKq9+Ke8b5fn7P6AW4AdlbVV+Y8ZRktaaD67PFPB34XeDzJY13bVYzKZt3eldR6ETh/Ml2UNG59Smh9B8g+np5oGS1Jk+Ev96QGGXypQQZfapDBlxq0Yv8f/5o3Ni53F6QV47MHPzHW93OPLzXI4EsNMvhSgwy+1CCDLzXI4EsNMvhSgwy+1KAV+wOev/vFdRN53+/d9ImJvO9q9ZGLH57I+w5pOyzkbzCp9frsFn/AI2mJDL7UIIMvNcjgSw0y+FKD+lxl94NJ/jnJv3S18/60az82yY6udt5tSdZOvruSxqHPHv/HwBlVdTKwCTgzyanAl4GvdrXz3gAumVw3JY1Tn6vsFrC3usWB3a2AM4Df7tpvBv4EuHb8XRyvSX0vrYVZrdthYuv14njfrtcYP8ma7pr6u4D7gO8Db1bV3voes4wKac73WktoSStMr+BX1btVtQmYAT4JnDjfbPt47XVVtbmqNh+23nOJ0kqwoCRW1ZvAA8CpwLoke4cKM8DL4+2apEnpc1b/iCTrusc/C/w6sBO4H/hcN5u186QB6fNPOhuAm5OsYfRBcXtV3Z3kKeDWJH8GPMqosKakAehzVv9fgVPmaX+e0Xhf0sB4tk1qkMGXGmTwpQYZfKlBBl9qkMGXGmTwpQYZfKlBBl9qkMGXGmTwpQYZfKlBBl9qkMGXGmTwpQYZfKlBBl9qkMGXGtQ7+N219R9Ncnc3bQktaaAWsse/jNHVdfeyhJY0UH0r6cwAvwlc302HUQmtO7pZbgbOm0QHJY1f3z3+NcCXgL01sA7DElrSYPUpqHE2sKuq5lYDzDyzWkJLGog+BTVOB85JchbwQeAQRkcA65Ic0O31LaElDch+d8FVdWVVzVTVRuAC4NtV9XksoSUN1lKOvf8Y+MMkzzEa81tCSxqIPof6/6uqHmBULdcSWtKAebZNapDBlxpk8KUGGXypQQZfapDBlxpk8KUGGXypQQZfapDBlxpk8KUGGXypQQZfapDBlxpk8KUGGXypQQZfapDBlxrU69JbSV4A3gbeBfZU1eYk64HbgI3AC8BvVdUbk+mmpHFayB7/16pqU1Vt7qavALZ3JbS2d9OSBmAph/rnMiqdBZbQkgalb/AL+PskDye5tGs7qqpeAejuj5xEByWNX9/La59eVS8nORK4L8nTfRfQfVBcCvDzR3suUVoJeiWxql7u7ncBdzG6nv6rSTYAdPe79vFaa+dJK0yfopkHJTl472PgN4AngG2MSmeBJbSkQelzqH8UcFeSvfP/dVXdm+RB4PYklwAvAudPrpuSxmm/we9KZZ08T/vrwJZJdErSZDnolhpk8KUGGXypQQZfapDBlxpk8KUGGXypQQZfapDBlxpk8KUGGXypQQZfapDBlxpk8KUGGXypQQZfapDBlxpk8KUG9Qp+knVJ7kjydJKdSU5Lsj7JfUme7e4PnXRnJY1H3z3+14B7q+qjjK6/txNLaEmD1efy2ocAvwLcAFBV/1VVb2IJLWmw+uzxjwNeA25K8miS67vr61tCSxqoPsE/APg4cG1VnQL8iAUc1ie5NMlDSR56ffd7i+ympHHqE/xZYLaqdnTTdzD6ILCEljRQ+01iVf0AeCnJCV3TFuApLKElDVbfarl/ANySZC3wPHAxow8NS2hJA9Qr+FX1GLB5nqcsoSUNkINuqUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2pQn4IaJyR5bM7trSSXW0JLGq4+V9l9pqo2VdUm4BPAO8BdWEJLGqyFHupvAb5fVf+OJbSkwVpo8C8Avt49toSWNFC9g99dU/8c4G8WsgBLaEkrz0L2+J8GHqmqV7tpS2hJA7WQJF7ITw7zwRJa0mD1Cn6SnwO2AnfOab4a2Jrk2e65q8ffPUmT0LeE1jvAYe9rex1LaEmD5KBbapDBlxpk8KUGGXypQQZfapDBlxpk8KUGGXypQQZfapDBlxpk8KUGGXypQQZfapDBlxpk8KUGGXypQb0uxDEub7/3MzzwzsZpLlJasY747rre8141e3av+f7jv/+213zu8aUGGXypQQZfapDBlxqUqprewpLXgB8BP5zaQqfrcFbnurlew/ELVXXE/maaavABkjxUVZunutApWa3r5nqtPh7qSw0y+FKDliP41y3DMqdlta6b67XKTH2ML2n5eagvNWiqwU9yZpJnkjyX5IppLnuckhyT5P4kO5M8meSyrn19kvuSPNvdH7rcfV2MJGuSPJrk7m762CQ7uvW6Lcna5e7jYiRZl+SOJE932+601bLNFmpqwU+yBvgL4NPAScCFSU6a1vLHbA/wxao6ETgV+EK3LlcA26vqeGB7Nz1ElwE750x/Gfhqt15vAJcsS6+W7mvAvVX1UeBkRuu4WrbZwlTVVG7AacC35kxfCVw5reVPeN2+CWwFngE2dG0bgGeWu2+LWJcZRgE4A7gbCKMfuRww33Ycyg04BPg3uvNac9oHv80Wc5vmof7RwEtzpme7tkFLshE4BdgBHFVVrwB090cuX88W7RrgS8B73fRhwJtVtaebHup2Ow54DbipG8Zcn+QgVsc2W7BpBj/ztA36K4UkHwK+AVxeVW8td3+WKsnZwK6qenhu8zyzDnG7HQB8HLi2qk5h9NPxNg7r5zHN4M8Cx8yZngFenuLyxyrJgYxCf0tV3dk1v5pkQ/f8BmDXcvVvkU4HzknyAnAro8P9a4B1SfZetGWo220WmK2qHd30HYw+CIa+zRZlmsF/EDi+O0O8FrgA2DbF5Y9NkgA3ADur6itzntoGXNQ9vojR2H8wqurKqpqpqo2Mts+3q+rzwP3A57rZBrdeAFX1A+ClJCd0TVuApxj4Nlusaf933lmM9iBrgBur6s+ntvAxSvIp4B+Bx/nJWPgqRuP824EPAy8C51fV7mXp5BIl+VXgj6rq7CTHMToCWA88CvxOVf14Ofu3GEk2AdcDa4HngYsZ7fxWxTZbCH+5JzXIX+5JDTL4UoMMvtQggy81yOBLDTL4UoMMvtQggy816H8ACvUlCGkfN6oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(preprocess(s)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random \n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        self.deque.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.deque, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameHistory(object):\n",
    "    # consecutive frame을 4개씩 병합하여 새로운 x를 하나씩 만드는 클래스\n",
    "    # 만들어진 x는 4개씩 concat하고 s를 만들어서 DQN의 input으로 들어감\n",
    "    def __init__(self, env, k=4):\n",
    "        self.state_deque = deque(maxlen=k) # [x1, x2, x3, x4]\n",
    "        self.frame_history_deque = deque(maxlen=k) #[s1, s2, s3, s4]\n",
    "        self.k = k\n",
    "        \n",
    "        # 최초에는 초기 화면(env.reset)으로 채워둠\n",
    "        self.env = env\n",
    "        self.initial_state = self.preprocess(self.env.reset())\n",
    "        self.reset()\n",
    "        \n",
    "    def preprocess(self, img):\n",
    "        # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "        img = np.dot(img[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "        img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "        img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "        return img.astype(np.uint8)\n",
    "    \n",
    "    def append_frame(self, s):\n",
    "        # if full, aggregate and clear old frames, append new frame to history queue\n",
    "        if len(self) == self.k:\n",
    "            self.state_deque.append(self.aggregate_frame())\n",
    "            \n",
    "        self.frame_history_deque.append(self.preprocess(s))\n",
    "        return\n",
    "        \n",
    "    def aggregate_frame(self):\n",
    "        if len(self) < self.k:\n",
    "            raise ValueError('not enough frames in history, expected %s, but got %s'%(self.k, len(self)))\n",
    "        \n",
    "        # element-wise maximum to aggregate\n",
    "        return np.maximum.reduce([self.frame_history_deque.pop() for _ in range(len(self))][-k//2:])\n",
    "    \n",
    "    def get_state(self):\n",
    "        S = np.array([self.state_deque[i] for i in range(self.k)]) # S = [x1, x2, x3, x4], (4, 1, 80, 80)\n",
    "        S = np.swapaxes(S, 0, 1) # (1, 4, 80, 80)\n",
    "        return S\n",
    "    \n",
    "    def reset(self):\n",
    "        self.frame_history_deque = deque(maxlen=k)\n",
    "        self.state_deque = deque(maxlen=k)\n",
    "        \n",
    "        for _ in range(self.k):\n",
    "            self.state_deque.append(self.initial_state)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_history_deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(nn.Module):\n",
    "    # Dueling DQN\n",
    "    def __init__(self, in_dim, n_action):\n",
    "        super(DDQN, self).__init__()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, 32, (8, 8), stride=4), nn.ReLU(), # conv 1\n",
    "            nn.Conv2d(32, 64, (4, 4), stride=2), nn.ReLU(), # conv 2\n",
    "            nn.Conv2d(64, 64, (3, 3), stride=1), nn.ReLU() # conv 3\n",
    "        )\n",
    "        # V(s) : value-stream\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(64*6*6, 512), nn.ReLU(), # hidden layer\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "        # A(s,a) : action-stream\n",
    "        self.action_stream = nn.Sequential(\n",
    "            nn.Linear(64*6*6, 512), nn.ReLU(),\n",
    "            nn.Linear(512, n_action)\n",
    "        )\n",
    "        \n",
    "    def prep_for_input(self, img):\n",
    "        img = torch.tensor(img, dtype=torch.float32, device=self.device).cuda(non_blocking=True) # to tensor\n",
    "        img /= 255                                   # normalize into 0-1\n",
    "        while img.dim() < 4 :                        # 4-dim\n",
    "            img = img.unsqueeze(0)\n",
    "        return img\n",
    "        \n",
    "    def forward(self, frames):\n",
    "        frames = self.prep_for_input(frames) \n",
    "        conved = self.conv(frames)\n",
    "        conved = conved.view(conved.size(0), -1)\n",
    "        \n",
    "        # key of Dueling DQN : Q(s,a_i) = V(S) + A(s,a_i) - mean(A)\n",
    "        value = self.value_stream(conved) # V(s), dim=(batch_size, 1)\n",
    "        action = self.action_stream(conved) # A(s,a), dim=(batch_size, n_action)\n",
    "        \n",
    "        output = value + (action - torch.mean(action, dim=1, keepdim=True)) # V(s) + (A(s,a_i) - mean(A)), dim=(batch_size, n_action)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "def weight_init(m):\n",
    "    '''\n",
    "    Usage:\n",
    "        model = Model()\n",
    "        model.apply(weight_init)\n",
    "    '''\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        init.normal_(m.bias.data)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DDQN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (value_stream): Sequential(\n",
       "    (0): Linear(in_features=2304, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (action_stream): Sequential(\n",
       "    (0): Linear(in_features=2304, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############\n",
    "k = 4 ## number of skipped frame\n",
    "#############\n",
    "\n",
    "behavior_net = DDQN(in_dim=k, n_action=4)\n",
    "target_net = DDQN(in_dim=k, n_action=4)\n",
    "\n",
    "behavior_net.to(behavior_net.device) # model to cuda\n",
    "target_net.to(target_net.device)     # model to cuda\n",
    "\n",
    "behavior_net.apply(weight_init)\n",
    "target_net.apply(weight_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grow_batch_size(current_memory, max_memory):\n",
    "    # grow batch size from 32 to 128 as respect to memory size\n",
    "    memory_ratio = current_memory / max_memory\n",
    "    new_batch_size = int(32 + 32*memory_ratio)\n",
    "    return new_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter():\n",
    "    def __init__(self, env, behavior_net, target_net, k=4, train_method='DQN'):\n",
    "        self.env = env\n",
    "        self.train_method = train_method\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        # networks\n",
    "        self.behavior_net = behavior_net\n",
    "        self.target_net = target_net\n",
    "        self.behavior_net.to(self.behavior_net.device) # model to cuda\n",
    "        self.target_net.to(self.target_net.device)     # model to cuda\n",
    "        \n",
    "        # train parameters\n",
    "        self.gamma = 0.99 #as written in paper\n",
    "        self.criterion = nn.SmoothL1Loss() # huber loss (error-clipping)\n",
    "        self.optim = torch.optim.RMSprop(self.behavior_net.parameters(), lr=0.00025, alpha=0.95, eps=0.01) #as written in paper in Rainbow\n",
    "        self.k = k\n",
    "        \n",
    "        # train details\n",
    "        self.total_frame = 0\n",
    "        self.total_episode = 0\n",
    "        \n",
    "        self.test_e = 0.05\n",
    "        self.batch_size = 32\n",
    "        self.max_replay = 1000000 # as written in paper\n",
    "        self.min_replay = 50000 # as written in the paper\n",
    "        self.replay_memory = ReplayMemory(self.max_replay)\n",
    "        self.frame_history = FrameHistory(env=self.env, k=self.k)\n",
    "        \n",
    "        self.train_reward_ls = []\n",
    "        self.test_reward_ls = []\n",
    "    \n",
    "    # train-epsilon\n",
    "    @property\n",
    "    def train_e(self):\n",
    "        return np.max([1 - 9.0*1e-07*self.total_frame, 0.1])\n",
    "    \n",
    "    \n",
    "    def reset_episode(self):\n",
    "        # game(episode) begins\n",
    "        self.env.reset()  \n",
    "        self.frame_history.reset() #frame history reset\n",
    "        self.current_episode_frame = 0         # each episode의 frame 수 \n",
    "        self.current_episode_reward = 0        # each episode의 reward 합\n",
    "    \n",
    "    \n",
    "    def choose_action(self, S, is_training=True):\n",
    "        if is_training:\n",
    "            epsilon = self.train_e\n",
    "        else:\n",
    "            epsilon = self.test_e\n",
    "            \n",
    "        # Choose an action by e-greedy\n",
    "        if np.random.rand(1) < epsilon :\n",
    "            a = self.env.action_space.sample()\n",
    "        else:\n",
    "            q_behavior = self.behavior_net(S)\n",
    "            a = torch.argmax(q_behavior).item()\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def run_k_frames(self, is_training=True):\n",
    "        # choose action!\n",
    "        S = self.frame_history.get_state()\n",
    "        a = self.choose_action(S, is_training=is_training)\n",
    "        r_sum = 0\n",
    "        \n",
    "        # repeat the action k-times\n",
    "        for _ in range(self.k):\n",
    "            s_next, r, done, _ = self.env.step(a)\n",
    "            self.frame_history.append_frame(s_next)\n",
    "            r_sum += self.clip_reward(r) # store clipped reward for future training replay(experience)\n",
    "            \n",
    "            # accumulate history\n",
    "            self.current_episode_frame += 1\n",
    "            self.current_episode_reward += r\n",
    "            \n",
    "            # if episode(game) ends, return done(True)\n",
    "            if done: \n",
    "                return done\n",
    "            \n",
    "        # concat last k-frames into a next state S_next\n",
    "        S_next = self.frame_history.get_state()\n",
    "        self.replay_memory.append(S, a, r_sum, S_next, done) # save replay(experience)\n",
    "        return done\n",
    "\n",
    "    \n",
    "    def train(self, max_total_frame):\n",
    "        while self.total_frame < max_total_frame :\n",
    "            # episode starts!\n",
    "            self.reset_episode()\n",
    "            \n",
    "            done = False\n",
    "            while not done:\n",
    "                done = self.run_k_frames(is_training=True)\n",
    "                \n",
    "                # training when enough replay-memory, every k-frames\n",
    "                if len(self.replay_memory) > self.min_replay:\n",
    "                    self.train_batch(self.batch_size)   \n",
    "                    self.total_frame += self.k\n",
    "                    \n",
    "                # update target_net every 10,000 steps\n",
    "                if self.total_frame%10000== 0 :\n",
    "                    self.update_target()\n",
    "                \n",
    "            # single-episode(game) is now done\n",
    "            # if not enough memory, save more\n",
    "            if len(self.replay_memory) < self.min_replay:\n",
    "                print('Not enough replay yet, expected more than %s, but %s instead'%(self.min_replay, len(self.replay_memory)))\n",
    "            \n",
    "            # if enough memory, print train result\n",
    "            else:\n",
    "                print('Train Episode :%s, Total Frame : %s, Train reward : %s,'%(self.total_episode, self.total_frame, self.current_episode_reward))\n",
    "                self.total_episode += 1\n",
    "                self.train_reward_ls.append(self.current_episode_reward)\n",
    "            \n",
    "                # testing, every 10 episodes when enough replay\n",
    "                if self.total_episode%10 == 0:\n",
    "                    self.test()           \n",
    "\n",
    "    def train_batch(self, batch_size):\n",
    "        # get mini-batch from replay-memory\n",
    "        S, A, R, S_next, D = self.replay_memory.sample(batch_size)\n",
    "        A = self.to_tensor(A, dtype=torch.long).unsqueeze(1)\n",
    "        R = self.to_tensor(R, dtype=torch.float32)\n",
    "        D = self.to_tensor(D, dtype=torch.float32)\n",
    "        \n",
    "        q_behaviors = self.behavior_net(S)                # Q-values for every possible actions\n",
    "        q_behavior = self.select_indices(q_behaviors, A) # select Q-value for given actions\n",
    "        \n",
    "        if self.train_method=='DQN':\n",
    "            q_targets_next = self.target_net(S_next)   # Q-values of every possible actions next state (targetDQN)\n",
    "            q_target_next = q_targets_next.max(1)[0]   # max Q-values of next state\n",
    "            q_target = R + self.gamma*q_target_next*(1-D) \n",
    "        \n",
    "        elif self.train_method=='DoubleDQN':\n",
    "            next_actions = torch.argmax(self.behavior_net(S_next), dim=1, keepdim=True) # choose argmax behavior actions at S_next\n",
    "            \n",
    "            q_targets_next = self.target_net(S_next) # cal Q-values of every possible actions next state (targetDQN)\n",
    "            q_target_next = self.select_indices(q_targets_next, next_actions) # select Q-value for next behavior actions\n",
    "            q_target = R + self.gamma*q_target_next*(1-D)\n",
    "        \n",
    "        # update weights\n",
    "        self.optim.zero_grad()\n",
    "        loss = self.criterion(q_target, q_behavior)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return\n",
    "        \n",
    "    def to_tensor(self, x, dtype=torch.float):\n",
    "        return torch.tensor(x, dtype=dtype, device=self.device)\n",
    "    \n",
    "    def select_indices(self, tensor, indices, dim=1):\n",
    "        return tensor.gather(dim, indices).squeeze(1)\n",
    "    \n",
    "    def clip_reward(self, r):\n",
    "        return np.sign(r)\n",
    "    \n",
    "    \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.behavior_net.state_dict())\n",
    "        return \n",
    "    \n",
    "    def test(self):\n",
    "        self.reset_episode()\n",
    "        \n",
    "        while True:\n",
    "            done = self.run_k_frames(self.test_e) # e-greedy\n",
    "            if done : break\n",
    "                \n",
    "        self.test_reward_ls.append(self.current_episode_reward)\n",
    "        print('※Test※ \\t Frames: %s \\t Score: %s'%(self.current_episode_frame, self.current_episode_reward))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = Fitter(env, behavior_net, target_net, k=k, train_method='DoubleDQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough replay yet, expected more than 50000, but 148 instead\n",
      "Not enough replay yet, expected more than 50000, but 310 instead\n",
      "Not enough replay yet, expected more than 50000, but 539 instead\n",
      "Not enough replay yet, expected more than 50000, but 689 instead\n",
      "Not enough replay yet, expected more than 50000, but 985 instead\n",
      "Not enough replay yet, expected more than 50000, but 1264 instead\n",
      "Not enough replay yet, expected more than 50000, but 1398 instead\n",
      "Not enough replay yet, expected more than 50000, but 1531 instead\n",
      "Not enough replay yet, expected more than 50000, but 1783 instead\n",
      "Not enough replay yet, expected more than 50000, but 1942 instead\n",
      "Not enough replay yet, expected more than 50000, but 2240 instead\n",
      "Not enough replay yet, expected more than 50000, but 2461 instead\n",
      "Not enough replay yet, expected more than 50000, but 2716 instead\n",
      "Not enough replay yet, expected more than 50000, but 2986 instead\n",
      "Not enough replay yet, expected more than 50000, but 3140 instead\n",
      "Not enough replay yet, expected more than 50000, but 3360 instead\n",
      "Not enough replay yet, expected more than 50000, but 3561 instead\n",
      "Not enough replay yet, expected more than 50000, but 3768 instead\n",
      "Not enough replay yet, expected more than 50000, but 3906 instead\n",
      "Not enough replay yet, expected more than 50000, but 4125 instead\n",
      "Not enough replay yet, expected more than 50000, but 4315 instead\n",
      "Not enough replay yet, expected more than 50000, but 4444 instead\n",
      "Not enough replay yet, expected more than 50000, but 4572 instead\n",
      "Not enough replay yet, expected more than 50000, but 4697 instead\n",
      "Not enough replay yet, expected more than 50000, but 4833 instead\n",
      "Not enough replay yet, expected more than 50000, but 4982 instead\n",
      "Not enough replay yet, expected more than 50000, but 5148 instead\n",
      "Not enough replay yet, expected more than 50000, but 5325 instead\n",
      "Not enough replay yet, expected more than 50000, but 5524 instead\n",
      "Not enough replay yet, expected more than 50000, but 5658 instead\n",
      "Not enough replay yet, expected more than 50000, but 5877 instead\n",
      "Not enough replay yet, expected more than 50000, but 6094 instead\n",
      "Not enough replay yet, expected more than 50000, but 6281 instead\n",
      "Not enough replay yet, expected more than 50000, but 6429 instead\n",
      "Not enough replay yet, expected more than 50000, but 6706 instead\n",
      "Not enough replay yet, expected more than 50000, but 6844 instead\n",
      "Not enough replay yet, expected more than 50000, but 7009 instead\n",
      "Not enough replay yet, expected more than 50000, but 7155 instead\n",
      "Not enough replay yet, expected more than 50000, but 7336 instead\n",
      "Not enough replay yet, expected more than 50000, but 7505 instead\n",
      "Not enough replay yet, expected more than 50000, but 7762 instead\n",
      "Not enough replay yet, expected more than 50000, but 7887 instead\n",
      "Not enough replay yet, expected more than 50000, but 8020 instead\n",
      "Not enough replay yet, expected more than 50000, but 8177 instead\n",
      "Not enough replay yet, expected more than 50000, but 8370 instead\n"
     ]
    }
   ],
   "source": [
    "max_frame = 10000000\n",
    "fitter.train(max_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fitter.frame_history.get_state()[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
