{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import the gym module\n",
    "import gym\n",
    "import numpy as np\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a breakout environment\n",
    "env = gym.make('BreakoutNoFrameskip-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "    img = np.mean(img, axis=2).astype(np.uint8) # to gray, uint8 for low memory\n",
    "    img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "    img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    a = env.action_space.sample()\n",
    "    s2, _, done, info = env.step(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f82deb9e7f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f82deab6198>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(preprocess(s2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random \n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        self.deque.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.deque, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameHistory(object):\n",
    "    # consecutive frame을 4개씩 병합하여 새로운 x를 하나씩 만드는 클래스\n",
    "    # 만들어진 x는 4개씩 concat하고 s를 만들어서 DQN의 input으로 들어감\n",
    "    def __init__(self, env, k=4):\n",
    "        self.state_deque = deque(maxlen=k) # [x1, x2, x3, x4]\n",
    "        self.frame_history_deque = deque(maxlen=k) #[s1, s2, s3, s4]\n",
    "        self.k = k\n",
    "        \n",
    "        # 최초에는 초기 화면(env.reset)으로 채워둠\n",
    "        self.env = env\n",
    "        self.initial_state = self.preprocess(self.env.reset())\n",
    "        self.reset()\n",
    "        \n",
    "    def preprocess(self, img):\n",
    "        # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "        img = np.dot(img[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "        img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "        img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "        return img.astype(np.uint8)\n",
    "    \n",
    "    def append_frame(self, s):\n",
    "        # if full, aggregate and clear old frames, append new frame to history queue\n",
    "        if len(self) == self.k:\n",
    "            self.state_deque.append(self.aggregate_frame())\n",
    "            \n",
    "        self.frame_history_deque.append(self.preprocess(s))\n",
    "        return\n",
    "        \n",
    "    def aggregate_frame(self):\n",
    "        if len(self) < self.k:\n",
    "            raise ValueError('not enough frames in history, expected %s, but got %s'%(self.k, len(self)))\n",
    "        \n",
    "        # element-wise maximum to aggregate\n",
    "        return np.maximum.reduce([self.frame_history_deque.pop() for _ in range(len(self))][-k//2:])\n",
    "    \n",
    "    def get_state(self):\n",
    "        S = np.array([self.state_deque[i] for i in range(self.k)]) # S = [x1, x2, x3, x4], (4, 1, 80, 80)\n",
    "        S = np.swapaxes(S, 0, 1) # (1, 4, 80, 80)\n",
    "        return S\n",
    "    \n",
    "    def reset(self):\n",
    "        self.frame_history_deque = deque(maxlen=k)\n",
    "        self.state_deque = deque(maxlen=k)\n",
    "        \n",
    "        for _ in range(self.k):\n",
    "            self.state_deque.append(self.initial_state)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_history_deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_dim, n_action):\n",
    "        super(DQN, self).__init__()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, 32, (8, 8), stride=4), nn.ReLU(), # conv 1\n",
    "            nn.Conv2d(32, 64, (4, 4), stride=2), nn.ReLU(), # conv 2\n",
    "            nn.Conv2d(64, 64, (3, 3), stride=1), nn.ReLU() # conv 3\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*6*6, 512), nn.ReLU(), # hidden layer\n",
    "            nn.Linear(512, n_action)\n",
    "        )\n",
    "        \n",
    "    def to_tensor(self, img):\n",
    "        img = torch.tensor(img, dtype=torch.float32, device=self.device).cuda(non_blocking=True) # to tensor\n",
    "        img /= 255                                   # normalize into 0-1\n",
    "        while img.dim() < 4 :                        # 4-dim\n",
    "            img = img.unsqueeze(0)\n",
    "        return img\n",
    "        \n",
    "    def forward(self, frames):\n",
    "        frames = self.to_tensor(frames) \n",
    "        conved = self.conv(frames)\n",
    "        conved = conved.view(conved.size(0), -1)\n",
    "        output = self.fc(conved)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "def weight_init(m):\n",
    "    '''\n",
    "    Usage:\n",
    "        model = Model()\n",
    "        model.apply(weight_init)\n",
    "    '''\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        init.normal_(m.bias.data)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2304, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############\n",
    "k = 4 ## number of skipped frame\n",
    "#############\n",
    "\n",
    "behavior_net = DQN(in_dim=k, n_action=4)\n",
    "target_net = DQN(in_dim=k, n_action=4)\n",
    "\n",
    "behavior_net.to(behavior_net.device) # model to cuda\n",
    "target_net.to(target_net.device)     # model to cuda\n",
    "\n",
    "behavior_net.apply(weight_init)\n",
    "target_net.apply(weight_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grow_batch_size(current_memory, max_memory):\n",
    "    # grow batch size from 32 to 64 as respect to memory size\n",
    "    memory_ratio = current_memory / max_memory\n",
    "    new_batch_size = int(32 + 32*memory_ratio)\n",
    "    return new_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter():\n",
    "    def __init__(self, env, behavior_net, target_net, k=4, train_method='DQN'):\n",
    "        self.env = env\n",
    "        self.train_method = train_method\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        # networks\n",
    "        self.behavior_net = behavior_net\n",
    "        self.target_net = target_net\n",
    "        self.behavior_net.to(self.behavior_net.device) # model to cuda\n",
    "        self.target_net.to(self.target_net.device)     # model to cuda\n",
    "        \n",
    "        # train parameters\n",
    "        self.gamma = 0.99 #as written in paper\n",
    "        self.criterion = nn.SmoothL1Loss() # huber loss (error-clipping)\n",
    "        self.optim = torch.optim.RMSprop(self.behavior_net.parameters(), lr=0.00025, alpha=0.95, eps=0.01) #as written in paper in Rainbow\n",
    "        self.k = k\n",
    "        \n",
    "        # train details\n",
    "        self.total_frame = 0\n",
    "        self.total_episode = 0\n",
    "        \n",
    "        self.test_e = 0.05\n",
    "        self.batch_size = 32\n",
    "        self.max_replay = 1000000 # as written in paper\n",
    "        self.min_replay = 50000 # as written in the paper\n",
    "        self.replay_memory = ReplayMemory(self.max_replay)\n",
    "        self.frame_history = FrameHistory(env=self.env, k=self.k)\n",
    "        \n",
    "        self.train_reward_ls = []\n",
    "        self.test_reward_ls = []\n",
    "    \n",
    "    # train-epsilon\n",
    "    @property\n",
    "    def train_e(self):\n",
    "        return np.max([1 - 9.0*1e-07*self.total_frame, 0.1])\n",
    "    \n",
    "    \n",
    "    def reset_episode(self):\n",
    "        # game(episode) begins\n",
    "        self.env.reset()  \n",
    "        self.frame_history.reset() #frame history reset\n",
    "        self.current_episode_frame = 0         # each episode의 frame 수 \n",
    "        self.current_episode_reward = 0        # each episode의 reward 합\n",
    "    \n",
    "    \n",
    "    def choose_action(self, S, is_training=True):\n",
    "        if is_training:\n",
    "            epsilon = self.train_e\n",
    "        else:\n",
    "            epsilon = self.test_e\n",
    "            \n",
    "        # Choose an action by e-greedy\n",
    "        if np.random.rand(1) < epsilon :\n",
    "            a = self.env.action_space.sample()\n",
    "        else:\n",
    "            q_behavior = self.behavior_net(S)\n",
    "            a = torch.argmax(q_behavior).item()\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def run_k_frames(self, is_training=True):\n",
    "        # choose action!\n",
    "        S = self.frame_history.get_state()\n",
    "        a = self.choose_action(S, is_training=is_training)\n",
    "        r_sum = 0\n",
    "        \n",
    "        # repeat the action k-times\n",
    "        for _ in range(self.k):\n",
    "            s_next, r, done, _ = self.env.step(a)\n",
    "            self.frame_history.append_frame(s_next)\n",
    "            r_sum += self.clip_reward(r) # store clipped reward for future training replay(experience)\n",
    "            \n",
    "            # accumulate history\n",
    "            self.current_episode_frame += 1\n",
    "            self.current_episode_reward += r\n",
    "            \n",
    "            # if episode(game) ends, return done(True)\n",
    "            if done: \n",
    "                return done\n",
    "            \n",
    "        # concat last k-frames into a next state S_next\n",
    "        S_next = self.frame_history.get_state()\n",
    "        self.replay_memory.append(S, a, r_sum, S_next, done) # save replay(experience)\n",
    "        return done\n",
    "\n",
    "    \n",
    "    def train(self, max_total_frame):\n",
    "        while self.total_frame < max_total_frame :\n",
    "            # episode starts!\n",
    "            self.reset_episode()\n",
    "            \n",
    "            done = False\n",
    "            while not done:\n",
    "                done = self.run_k_frames(is_training=True)\n",
    "                \n",
    "                # training when enough replay-memory, every k-frames\n",
    "                if len(self.replay_memory) > self.min_replay:\n",
    "                    self.train_batch(self.batch_size)   \n",
    "                    self.total_frame += self.k\n",
    "                    \n",
    "                # update target_net every 10,000 steps\n",
    "                if self.total_frame%10000== 0 :\n",
    "                    self.update_target()\n",
    "                \n",
    "            # single-episode(game) is now done\n",
    "            # if not enough memory, save more\n",
    "            if len(self.replay_memory) < self.min_replay:\n",
    "                print('Not enough replay yet, expected more than %s, but %s instead'%(self.min_replay, len(self.replay_memory)))\n",
    "            \n",
    "            # if enough memory, print train result\n",
    "            else:\n",
    "                print('Train Episode :%s, Total Frame : %s, Train reward : %s,'%(self.total_episode, self.total_frame, self.current_episode_reward))\n",
    "                self.total_episode += 1\n",
    "                self.train_reward_ls.append(self.current_episode_reward)\n",
    "            \n",
    "                # testing, every 10 episodes when enough replay\n",
    "                if self.total_episode%10 == 0:\n",
    "                    self.test()           \n",
    "\n",
    "    def train_batch(self, batch_size):\n",
    "        # get mini-batch from replay-memory\n",
    "        S, A, R, S_next, D = self.replay_memory.sample(batch_size)\n",
    "        A = self.to_tensor(A, dtype=torch.long).unsqueeze(1)\n",
    "        R = self.to_tensor(R, dtype=torch.float32)\n",
    "        D = self.to_tensor(D, dtype=torch.float32)\n",
    "        \n",
    "        q_behaviors = self.behavior_net(S)                # Q-values for every possible actions\n",
    "        q_behavior = self.select_indices(q_behaviors, A) # select Q-value for given actions\n",
    "        \n",
    "        if self.train_method=='DQN':\n",
    "            q_targets_next = self.target_net(S_next)   # Q-values of every possible actions next state (targetDQN)\n",
    "            q_target_next = q_targets_next.max(1)[0]   # max Q-values of next state\n",
    "            q_target = R + self.gamma*q_target_next*(1-D) \n",
    "        \n",
    "        elif self.train_method=='DoubleDQN':\n",
    "            next_actions = torch.argmax(self.behavior_net(S_next), dim=1, keepdim=True) # choose argmax behavior actions at S_next\n",
    "            \n",
    "            q_targets_next = self.target_net(S_next) # cal Q-values of every possible actions next state (targetDQN)\n",
    "            q_target_next = self.select_indices(q_targets_next, next_actions) # select Q-value for next behavior actions\n",
    "            q_target = R + self.gamma*q_target_next*(1-D)\n",
    "        \n",
    "        # update weights\n",
    "        self.optim.zero_grad()\n",
    "        loss = self.criterion(q_target, q_behavior)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return\n",
    "        \n",
    "    def to_tensor(self, x, dtype=torch.float):\n",
    "        return torch.tensor(x, dtype=dtype, device=self.device)\n",
    "    \n",
    "    def select_indices(self, tensor, indices, dim=1):\n",
    "        return tensor.gather(dim, indices).squeeze(1)\n",
    "    \n",
    "    def clip_reward(self, r):\n",
    "        return np.sign(r)\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.behavior_net.state_dict())\n",
    "        return \n",
    "    \n",
    "    def test(self):\n",
    "        self.reset_episode()\n",
    "        \n",
    "        while True:\n",
    "            done = self.run_k_frames(self.test_e) # e-greedy\n",
    "            if done : break\n",
    "                \n",
    "        self.test_reward_ls.append(self.current_episode_reward)\n",
    "        print('※Test※ \\t Frames: %s \\t Score: %s'%(self.current_episode_frame, self.current_episode_reward))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = Fitter(env, behavior_net, target_net, k=k, train_method='DoubleDQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough replay yet, expected more than 50000, but 132 instead\n",
      "Not enough replay yet, expected more than 50000, but 258 instead\n",
      "Not enough replay yet, expected more than 50000, but 388 instead\n",
      "Not enough replay yet, expected more than 50000, but 568 instead\n",
      "Not enough replay yet, expected more than 50000, but 744 instead\n",
      "Not enough replay yet, expected more than 50000, but 897 instead\n",
      "Not enough replay yet, expected more than 50000, but 1026 instead\n"
     ]
    }
   ],
   "source": [
    "max_frame = 10000000\n",
    "fitter.train(max_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fitter.frame_history.get_state()[0][3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
