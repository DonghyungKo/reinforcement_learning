{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import the gym module\n",
    "import gym\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a breakout environment\n",
    "env = gym.make('PongNoFrameskip-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "    img = np.dot(img[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "    #img = np.mean(img, axis=2, dtype=np.uint8)\n",
    "    img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "    img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "    return img.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3375"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "done = False\n",
    "n = 0\n",
    "while not done:\n",
    "    a = env.action_space.sample()\n",
    "    s, _, done, _ = env.step(a)\n",
    "    n += 1\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb1b47029e8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb1b469a4a8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(preprocess(s)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random \n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "    \n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        self.deque.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.deque, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameHistory(object):\n",
    "    # consecutive frame을 4개씩 병합하여 새로운 x를 하나씩 만드는 클래스\n",
    "    # 만들어진 x는 4개씩 concat하고 s를 만들어서 DQN의 input으로 들어감\n",
    "    def __init__(self, env, k_history=4):\n",
    "        self.state_deque = deque(maxlen=k_history) # [x1, x2, x3, x4]\n",
    "        self.frame_history_deque = deque(maxlen=k_history) #[s1, s2, s3, s4]\n",
    "        self.k_history = k_history\n",
    "        \n",
    "        # 최초에는 초기 화면(env.reset)으로 채워둠\n",
    "        self.env = env\n",
    "        self.initial_state = self.preprocess(self.env.reset())\n",
    "        self.reset()\n",
    "        \n",
    "    def preprocess(self, img):\n",
    "        # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "        img = np.dot(img[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "        img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "        img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "        return img.astype(np.uint8)\n",
    "    \n",
    "    def append_frame(self, s):\n",
    "        # if full, aggregate and clear old frames, append new frame to history queue\n",
    "        if len(self) == self.k_history:\n",
    "            self.state_deque.append(self.aggregate_frame())\n",
    "            \n",
    "        self.frame_history_deque.append(self.preprocess(s))\n",
    "        return\n",
    "        \n",
    "    def aggregate_frame(self):\n",
    "        if len(self) < self.k_history:\n",
    "            raise ValueError('not enough frames in history, expected %s, but got %s'%(self.k_history, len(self)))\n",
    "        \n",
    "        # element-wise maximum to aggregate\n",
    "        return np.maximum.reduce([self.frame_history_deque.pop() for _ in range(len(self))])\n",
    "    \n",
    "    def get_state(self):\n",
    "        S = np.array([self.state_deque[i] for i in range(self.k_history)]) # S = [x1, x2, x3, x4], (4, 1, 80, 80)\n",
    "        S = np.swapaxes(S, 0, 1) # (1, 4, 80, 80)\n",
    "        return S\n",
    "    \n",
    "    def reset(self):\n",
    "        self.frame_history_deque = deque(maxlen=k_history)\n",
    "        self.state_deque = deque(maxlen=k_history)\n",
    "        \n",
    "        for _ in range(self.k_history):\n",
    "            self.state_deque.append(self.initial_state)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_history_deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(nn.Module):\n",
    "    # Dueling DQN\n",
    "    def __init__(self, in_dim, n_action):\n",
    "        super(DDQN, self).__init__()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, 32, (8, 8), stride=4), nn.ReLU(), # conv 1\n",
    "            nn.Conv2d(32, 64, (4, 4), stride=2), nn.ReLU(), # conv 2\n",
    "            nn.Conv2d(64, 64, (3, 3), stride=1), nn.ReLU() # conv 3\n",
    "        )\n",
    "        # V(s) : value-stream\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(64*6*6, 512), nn.ReLU(), # hidden layer\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "        # A(s,a) : action-stream\n",
    "        self.action_stream = nn.Sequential(\n",
    "            nn.Linear(64*6*6, 512), nn.ReLU(),\n",
    "            nn.Linear(512, n_action)\n",
    "        )\n",
    "        \n",
    "    def to_tensor(self, img):\n",
    "        img = torch.tensor(img, dtype=torch.float32, device=self.device).cuda(non_blocking=True) # to tensor\n",
    "        img /= 255                                   # normalize into 0-1\n",
    "        while img.dim() < 4 :                        # 4-dim\n",
    "            img = img.unsqueeze(0)\n",
    "        return img\n",
    "        \n",
    "    def forward(self, frames):\n",
    "        frames = self.to_tensor(frames) \n",
    "        conved = self.conv(frames)\n",
    "        conved = conved.view(conved.size(0), -1)\n",
    "        \n",
    "        # key of Dueling DQN : Q(s,a_i) = V(S) + A(s,a_i) - mean(A)\n",
    "        value = self.value_stream(conved) # V(s), dim=(batch_size, 1)\n",
    "        action = self.action_stream(conved) # A(s,a), dim=(batch_size, n_action)\n",
    "        \n",
    "        output = value + (action - torch.mean(action, dim=1, keepdim=True)) # V(s) + (A(s,a_i) - mean(A)), dim=(batch_size, n_action)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DDQN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (value_stream): Sequential(\n",
       "    (0): Linear(in_features=2304, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (action_stream): Sequential(\n",
       "    (0): Linear(in_features=2304, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############\n",
    "k_history = 4 ## number of skipped frame\n",
    "#############\n",
    "\n",
    "behavior_net = DDQN(in_dim=k_history, n_action=env.action_space.n)\n",
    "target_net = DDQN(in_dim=k_history, n_action=env.action_space.n)\n",
    "\n",
    "behavior_net.to(behavior_net.device) # model to cuda\n",
    "target_net.to(target_net.device)     # model to cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_decay(n_total_frame):\n",
    "    return np.max([1 - 9.0*1e-07*n_total_frame, 0.1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grow_batch_size(current_memory, max_memory):\n",
    "    # grow batch size from 32 to 128 as respect to memory size\n",
    "    memory_ratio = current_memory / max_memory\n",
    "    new_batch_size = int(32 + 32*memory_ratio)\n",
    "    return new_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter():\n",
    "    def __init__(self, env, behavior_net, target_net, k_history=4, train_method='DQN'):\n",
    "        self.env = env\n",
    "        self.train_method = train_method\n",
    "        \n",
    "        # networks\n",
    "        self.behavior_net = behavior_net\n",
    "        self.target_net = target_net\n",
    "        self.behavior_net.to(self.behavior_net.device) # model to cuda\n",
    "        self.target_net.to(self.target_net.device)     # model to cuda\n",
    "        \n",
    "        # train parameters\n",
    "        self.gamma = 0.99 #as written in paper\n",
    "        self.criterion = nn.SmoothL1Loss() # huber loss (error-clipping)\n",
    "        self.optim = torch.optim.RMSprop(self.behavior_net.parameters(), lr=0.00025, alpha=0.95, eps=0.01) #as written in paper in Rainbow\n",
    "        self.k_history = k_history\n",
    "        \n",
    "        # train details\n",
    "        self.total_frame = 0\n",
    "        self.total_episode = 0\n",
    "        self.batch_size = 32\n",
    "        self.max_replay = 1000000 # as written in paper\n",
    "        self.min_replay = 50000 # as written in the paper\n",
    "        self.replay_memory = ReplayMemory(self.max_replay)\n",
    "        self.frame_history = FrameHistory(env=self.env, k_history=self.k_history)\n",
    "        \n",
    "        self.train_reward_ls = []\n",
    "        self.test_reward_ls = []\n",
    "    \n",
    "    \n",
    "    def train(self, max_total_frame):\n",
    "        while self.total_frame < max_total_frame :\n",
    "            # game(episode) begins\n",
    "            self.env.reset()  \n",
    "            self.frame_history.reset() #frame history reset\n",
    "            e = epsilon_decay(self.total_frame)\n",
    "            \n",
    "            n_step = 0       # each episode의 step의 횟수 \n",
    "            r_sum = 0        # each episode의 reward sum\n",
    "            \n",
    "            done = False\n",
    "            while not done:\n",
    "                n_step += 1\n",
    "                \n",
    "                # start of every k consecutive frames, choose action!                \n",
    "                if n_step%self.k_history == 1:\n",
    "                    # S = [x1, x2, x3, x4] for neural-network input\n",
    "                    # x represents feature(element-wise max) from every k consecutive frames(k*s)\n",
    "                    S = self.frame_history.get_state()\n",
    "                    r_sum = 0   # sum of reward for the following k frames\n",
    "                    \n",
    "                    # Choose an action by e-greedy\n",
    "                    if np.random.rand(1) < e :\n",
    "                        a = self.env.action_space.sample()\n",
    "                    else:\n",
    "                        q_pred = self.behavior_net(S)\n",
    "                        a = torch.argmax(q_pred).item()\n",
    "                \n",
    "                # repeat the same action k times\n",
    "                s_next, r, done, info, = self.env.step(a)\n",
    "                self.frame_history.append_frame(s_next)\n",
    "                r_sum += self.clip_reward(r)\n",
    "\n",
    "                # end of every k consecutive frames\n",
    "                # genenrate new x, and update S\n",
    "                if n_step%self.k_history == 0 :\n",
    "                    S_new = self.frame_history.get_state()\n",
    "                    self.replay_memory.append(S, a, r_sum, S_new, done)\n",
    "                \n",
    "                # no training when not enough replay\n",
    "                if len(self.replay_memory) < self.min_replay:\n",
    "                    continue\n",
    "                \n",
    "                # train every k steps\n",
    "                if n_step % self.k_history == 0:\n",
    "                    mini_batch = self.replay_memory.sample(self.batch_size)\n",
    "                    self.train_batch(mini_batch)   \n",
    "                \n",
    "                self.total_frame += 1\n",
    "                # update target_net every 10,000 steps\n",
    "                if self.total_frame%10000== 0 :\n",
    "                    self.update_target()\n",
    "                \n",
    "            \n",
    "            # single-episode(game) is now done\n",
    "            # if not enough memory, save more\n",
    "            if len(self.replay_memory) < self.min_replay:\n",
    "                print('Not enough replay yet, expected more than %s, but %s instead'%(self.min_replay, len(self.replay_memory)))\n",
    "            \n",
    "            # if enough memory, check train reward\n",
    "            else:\n",
    "                print('Train Episode :%s, Train reward : %s, Total Frame : %s'%(self.total_episode, r_sum, self.total_frame))\n",
    "                self.total_episode += 1\n",
    "                self.train_reward_ls.append(r_sum)\n",
    "            \n",
    "                # testing when enough replay, every 10 episodes\n",
    "                if self.total_episode%10 == 0:\n",
    "                    self.test()           \n",
    "    \n",
    "    \n",
    "    def clip_reward(self, r):\n",
    "        return np.sign(r)\n",
    "    \n",
    "    \n",
    "    def train_batch(self, mini_batch):\n",
    "        batch_size = len(mini_batch)\n",
    "        S = np.array([tup[0] for tup in mini_batch]).squeeze(1) # State\n",
    "        A = np.array([tup[1] for tup in mini_batch]) # actions\n",
    "        R = np.array([tup[2] for tup in mini_batch]) # rewards\n",
    "        S_next = np.array([tup[3] for tup in mini_batch]).squeeze(1) # next_State\n",
    "        D = np.array([tup[4] for tup in mini_batch]) # dones\n",
    "        \n",
    "        q_targets = self.target_net(S) # Q-values of current state with targetDDQN\n",
    "        q_targets_next = self.target_net(S_next) # Q-values of next state from targetDDQN\n",
    "        \n",
    "        if self.train_method=='DQN':\n",
    "            for i in range(batch_size):\n",
    "                a, r, done = A[i], R[i], D[i]\n",
    "                if done:\n",
    "                    q_targets[i, a] = r\n",
    "                else:\n",
    "                    q_targets[i, a] = r + self.gamma*torch.max(q_targets_next[i])\n",
    "        \n",
    "        elif self.train_method=='DoubleDQN':\n",
    "            next_behavior_net_actions = torch.argmax(self.behavior_net(S_next), dim=1) # choose argmax actions from behaviorDDQN in S_next\n",
    "                    \n",
    "            for i in range(batch_size):\n",
    "                a, r, done = A[i], R[i], D[i]\n",
    "                next_behavior_net_action = next_behavior_net_actions[i].item() # choose argmax actions from behaviorDDQN in S_next\n",
    "                if done:\n",
    "                    q_targets[i, a] = r\n",
    "                else:\n",
    "                    q_targets[i, a] = r + self.gamma*q_targets_next[i, next_behavior_net_action]\n",
    "            \n",
    "        # 예측치(pred)와 목표치(true)\n",
    "        q_behaviors = self.behavior_net(S)\n",
    "        self.optim.zero_grad()\n",
    "        loss = self.criterion(q_targets, q_behaviors)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return\n",
    "\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.behavior_net.state_dict())\n",
    "        return \n",
    "    \n",
    "    \n",
    "    def test(self):\n",
    "        self.env.reset()\n",
    "        self.frame_history.reset()\n",
    "        e = 0.05 # e-greedy, \n",
    "        \n",
    "        n_step = 0\n",
    "        r_sum = 0\n",
    "        \n",
    "        done=False\n",
    "        while not done:\n",
    "            n_step += 1\n",
    "            \n",
    "            # at the beginning of every 4-th frame\n",
    "            # choose an action\n",
    "            if n_step%self.k_history==1:\n",
    "                # e-greedy search, e=0.05 \n",
    "                if np.random.rand(1) < e:\n",
    "                    a = self.env.action_space.sample()\n",
    "                else:\n",
    "                    S = self.frame_history.get_state()\n",
    "                    a = torch.argmax(self.behavior_net(S)).item()\n",
    "            \n",
    "            s, r, done, _ = self.env.step(a)\n",
    "            self.frame_history.append_frame(s)\n",
    "            r_sum += r\n",
    "        \n",
    "        self.test_reward_ls.append(r_sum)\n",
    "        print('Total Step: %s \\t Total Score: %s'%(n_step, r_sum))\n",
    "        return r_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = Fitter(env, behavior_net, target_net, k_history=k_history, train_method='DoubleDQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_frame: 15463\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "total_frame: 54577\n",
      "Total Step: 3168 \t Total Score: -21.0\n",
      "total_frame: 90737\n",
      "Total Step: 3357 \t Total Score: -21.0\n",
      "total_frame: 129061\n",
      "Total Step: 3367 \t Total Score: -20.0\n",
      "total_frame: 163002\n",
      "Total Step: 3131 \t Total Score: -21.0\n",
      "total_frame: 197268\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "total_frame: 229475\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "total_frame: 262214\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "total_frame: 296328\n",
      "Total Step: 3350 \t Total Score: -20.0\n",
      "total_frame: 331509\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "total_frame: 364315\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "total_frame: 400849\n",
      "Total Step: 3168 \t Total Score: -21.0\n",
      "total_frame: 437365\n",
      "Total Step: 3536 \t Total Score: -21.0\n",
      "total_frame: 472899\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "total_frame: 507024\n",
      "Total Step: 3481 \t Total Score: -20.0\n",
      "total_frame: 538810\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "total_frame: 571034\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "total_frame: 605482\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "total_frame: 638858\n",
      "Total Step: 3056 \t Total Score: -21.0\n"
     ]
    }
   ],
   "source": [
    "max_frame = 10000000\n",
    "fitter.train(max_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
