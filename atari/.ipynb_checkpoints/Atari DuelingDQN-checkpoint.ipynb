{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import the gym module\n",
    "import gym\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        '''첫 번째 트릭 No-Operation. 초기화 후 일정 단계에 이를때까지 아무 행동도 하지않고\n",
    "        게임 초기 상태를 다양하게 하여 특정 시작 상태만 학습하는 것을 방지한다'''\n",
    "\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = self.unwrapped.np_random.randint(\n",
    "                1, self.noop_max + 1)  # pylint: disable=E1101\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a breakout environment\n",
    "env = gym.make('BreakoutNoFrameskip-v4')\n",
    "env = NoopResetEnv(env, noop_max=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    img = cv2.resize(img, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "    return np.expand_dims(img, 0) # (1, 80, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "for _ in range(30):\n",
    "    img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    action = env.action_space.sample()\n",
    "    env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2c02d7d0f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "for _ in range(30):\n",
    "    s, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.imshow(preprocess(s)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random \n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        self.deque.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.deque, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameHistory(object):\n",
    "    # consecutive frame을 4개씩 병합하여 새로운 x를 하나씩 만드는 클래스\n",
    "    # 만들어진 x는 4개씩 concat하고 s를 만들어서 DQN의 input으로 들어감\n",
    "    def __init__(self, env, k=4):\n",
    "        self.state_deque = deque(maxlen=k) # [x1, x2, x3, x4]\n",
    "        self.frame_history_deque = deque(maxlen=k) #[s1, s2, s3, s4]\n",
    "        self.k = k\n",
    "        \n",
    "        # 최초에는 초기 화면(env.reset)으로 채워둠\n",
    "        self.env = env\n",
    "        self.initial_state = self.preprocess(self.env.reset())\n",
    "        self.reset()\n",
    "        \n",
    "    def preprocess(self, img):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        img = cv2.resize(img, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "        return np.expand_dims(img, 0) # (1, 80, 80)\n",
    "    \n",
    "    def append_frame(self, s):\n",
    "        # if full, aggregate and clear old frames, append new frame to history queue\n",
    "        self.frame_history_deque.append(self.preprocess(s))\n",
    "        \n",
    "        if len(self) == self.k:\n",
    "            self.state_deque.append(self.aggregate_frame())\n",
    "        return\n",
    "        \n",
    "    def aggregate_frame(self):\n",
    "        if len(self) < self.k:\n",
    "            raise ValueError('not enough frames in history, expected %s, but got %s'%(self.k, len(self)))\n",
    "        \n",
    "        # element-wise maximum to aggregate\n",
    "        frame_history = [self.frame_history_deque.popleft() for _ in range(self.k)]\n",
    "        return np.maximum.reduce(frame_history[-2:]) #마지막 2 프레임 사용 \n",
    "    \n",
    "    def get_state(self):\n",
    "        S = np.array([self.state_deque[i] for i in range(self.k)]) # S = [x1, x2, x3, x4], (4, 1, 80, 80)\n",
    "        S = np.swapaxes(S, 0, 1) # (1, 4, 80, 80)\n",
    "        return S\n",
    "    \n",
    "    def reset(self):\n",
    "        self.frame_history_deque = deque(maxlen=k)\n",
    "        self.state_deque = deque(maxlen=k)\n",
    "        \n",
    "        for _ in range(self.k):\n",
    "            self.state_deque.append(self.initial_state)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_history_deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(nn.Module):\n",
    "    # Dueling DQN\n",
    "    def __init__(self, in_dim, n_action):\n",
    "        super(DDQN, self).__init__()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, 32, (8, 8), stride=4), nn.ReLU(), # conv 1\n",
    "            nn.Conv2d(32, 64, (4, 4), stride=2), nn.ReLU(), # conv 2\n",
    "            nn.Conv2d(64, 64, (3, 3), stride=1), nn.ReLU() # conv 3\n",
    "        )\n",
    "        # V(s) : value-stream\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(64*7*7, 512), nn.ReLU(), # hidden layer\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "        # A(s,a) : action-stream\n",
    "        self.action_stream = nn.Sequential(\n",
    "            nn.Linear(64*7*7, 512), nn.ReLU(),\n",
    "            nn.Linear(512, n_action)\n",
    "        )\n",
    "        \n",
    "    def prep_for_input(self, img):\n",
    "        img = torch.tensor(img, dtype=torch.float32, device=self.device).cuda(non_blocking=True) # to tensor\n",
    "        img /= 255                                   # normalize into 0-1\n",
    "        while img.dim() < 4 :                        # 4-dim\n",
    "            img = img.unsqueeze(0)\n",
    "        return img\n",
    "        \n",
    "    def forward(self, frames):\n",
    "        frames = self.prep_for_input(frames) \n",
    "        conved = self.conv(frames)\n",
    "        conved = conved.view(conved.size(0), -1)\n",
    "        \n",
    "        # key of Dueling DQN : Q(s,a_i) = V(S) + A(s,a_i) - mean(A)\n",
    "        value = self.value_stream(conved) # V(s), dim=(batch_size, 1)\n",
    "        action = self.action_stream(conved) # A(s,a), dim=(batch_size, n_action)\n",
    "        \n",
    "        output = value + (action - torch.mean(action, dim=1, keepdim=True)) # V(s) + (A(s,a_i) - mean(A)), dim=(batch_size, n_action)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "def weight_init(m):\n",
    "    '''\n",
    "    Usage:\n",
    "        model = Model()\n",
    "        model.apply(weight_init)\n",
    "    '''\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        init.normal_(m.bias.data)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DDQN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (value_stream): Sequential(\n",
       "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (action_stream): Sequential(\n",
       "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############\n",
    "k = 4 ## number of skipped frame\n",
    "#############\n",
    "\n",
    "behavior_net = DDQN(in_dim=k, n_action=4)\n",
    "target_net = DDQN(in_dim=k, n_action=4)\n",
    "\n",
    "behavior_net.to(behavior_net.device) # model to cuda\n",
    "target_net.to(target_net.device)     # model to cuda\n",
    "\n",
    "behavior_net.apply(weight_init)\n",
    "target_net.apply(weight_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter():\n",
    "    def __init__(self, env, behavior_net, target_net, k=4, train_method='DQN'):\n",
    "        self.env = env\n",
    "        self.train_method = train_method\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        # networks\n",
    "        self.behavior_net = behavior_net\n",
    "        self.target_net = target_net\n",
    "        self.behavior_net.to(self.behavior_net.device) # model to cuda\n",
    "        self.target_net.to(self.target_net.device)     # model to cuda\n",
    "        \n",
    "        # train parameters\n",
    "        self.gamma = 0.99 #as written in paper\n",
    "        self.criterion = nn.SmoothL1Loss() # huber loss (error-clipping)\n",
    "        self.optim = torch.optim.RMSprop(self.behavior_net.parameters(), lr=0.00025, alpha=0.95, eps=0.01) #as written in paper in Rainbow\n",
    "        self.k = k\n",
    "        \n",
    "        # train details\n",
    "        self.total_frame = 0\n",
    "        self.total_episode = 0\n",
    "        \n",
    "        self.test_e = 0.05\n",
    "        self.batch_size = 32\n",
    "        self.max_replay = 250000 # not as written in paper\n",
    "        self.min_replay = 50000 # not as written in the paper\n",
    "        self.replay_memory = ReplayMemory(self.max_replay)\n",
    "        self.frame_history = FrameHistory(env=self.env, k=self.k)\n",
    "        \n",
    "        self.train_reward_ls = []\n",
    "        self.test_reward_ls = []\n",
    "    \n",
    "    # train-epsilon\n",
    "    @property\n",
    "    def train_e(self):\n",
    "        return np.max([1 - 9.0*1e-07*self.total_frame, 0.1])\n",
    "    \n",
    "    \n",
    "    def reset_episode(self):\n",
    "        # game(episode) begins\n",
    "        self.env.reset()  \n",
    "        self.frame_history.reset() #frame history reset\n",
    "        self.current_episode_frame = 0         # each episode의 frame 수 \n",
    "        self.current_episode_reward = 0        # each episode의 reward 합\n",
    "    \n",
    "    \n",
    "    def choose_action(self, S, is_training=True):\n",
    "        if is_training:\n",
    "            epsilon = self.train_e\n",
    "        else:\n",
    "            epsilon = self.test_e\n",
    "        # at the beginning of an episode, do something\n",
    "        if self.current_episode_frame == 0:\n",
    "            a = 1\n",
    "        else:\n",
    "            # Choose an action by e-greedy\n",
    "            if np.random.rand(1) < epsilon :\n",
    "                a = self.env.action_space.sample()\n",
    "            else:\n",
    "                q_behavior = self.behavior_net(S)\n",
    "                a = torch.argmax(q_behavior).item()\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def run_k_frames(self, is_training=True):\n",
    "        # choose action!\n",
    "        S = self.frame_history.get_state()\n",
    "        a = self.choose_action(S, is_training=is_training)\n",
    "        r_sum = 0\n",
    "        \n",
    "        # repeat the action k-times\n",
    "        for _ in range(self.k):\n",
    "            s_next, r, done, _ = self.env.step(a)\n",
    "            self.frame_history.append_frame(s_next)\n",
    "            r_sum += self.clip_reward(r) # store clipped reward for future training replay(experience)\n",
    "            \n",
    "            # accumulate history\n",
    "            self.current_episode_frame += 1\n",
    "            self.current_episode_reward += r\n",
    "            \n",
    "            # if episode(game) ends, return done(True)\n",
    "            if done: \n",
    "                return done\n",
    "            \n",
    "        # concat last k-frames into a next state S_next\n",
    "        S_next = self.frame_history.get_state()\n",
    "        self.replay_memory.append(S, a, r_sum, S_next, done) # save replay(experience)\n",
    "        return done\n",
    "\n",
    "    \n",
    "    def train(self, max_total_frame):\n",
    "        while self.total_frame < max_total_frame :\n",
    "            # episode starts!\n",
    "            self.reset_episode()\n",
    "            \n",
    "            done = False\n",
    "            while not done:\n",
    "                done = self.run_k_frames(is_training=True)\n",
    "                \n",
    "                # training when enough replay-memory, every k-frames\n",
    "                if len(self.replay_memory) > self.min_replay:\n",
    "                    self.train_batch(self.batch_size)   \n",
    "                    self.total_frame += self.k\n",
    "                    \n",
    "                    # update target_net every 10,000 updates\n",
    "                    if self.total_frame%(self.k*10000)== 0 :\n",
    "                        self.update_target()\n",
    "                \n",
    "            # single-episode(game) is now done\n",
    "            # if not enough memory, save more\n",
    "            if len(self.replay_memory) < self.min_replay:\n",
    "                print('Not enough replay yet, expected more than %s, but %s instead'%(self.min_replay, len(self.replay_memory)))\n",
    "            \n",
    "            # if enough memory, print train result\n",
    "            else:\n",
    "                print('Train Episode :%s, Total Frame : %s, Train reward : %s,'%(self.total_episode, self.total_frame, self.current_episode_reward))\n",
    "                self.total_episode += 1\n",
    "                self.train_reward_ls.append(self.current_episode_reward)\n",
    "            \n",
    "                # testing, every 10 episodes when enough replay\n",
    "                if self.total_episode%10 == 0:\n",
    "                    self.test()           \n",
    "\n",
    "    def train_batch(self, batch_size):\n",
    "        # get mini-batch from replay-memory\n",
    "        S, A, R, S_next, D = self.replay_memory.sample(batch_size)\n",
    "        A = self.to_tensor(A, dtype=torch.long)\n",
    "        R = self.to_tensor(R, dtype=torch.float32)\n",
    "        D = self.to_tensor(D, dtype=torch.float32)\n",
    "        \n",
    "        q_behaviors = self.behavior_net(S)                # Q-values for every possible actions\n",
    "        q_behavior = self.select_indices(q_behaviors, A) # select Q-value for given actions\n",
    "        \n",
    "        if self.train_method=='DQN':\n",
    "            q_targets_next = self.target_net(S_next)   # Q-values of every possible actions next state (targetDQN)\n",
    "            q_target_next = q_targets_next.max(1)[0]   # max Q-values of next state\n",
    "            q_target = R + self.gamma*q_target_next*(1-D) \n",
    "        \n",
    "        elif self.train_method=='DoubleDQN':\n",
    "            next_actions = torch.argmax(self.behavior_net(S_next), dim=1) # choose argmax behavior actions at S_next\n",
    "            \n",
    "            q_targets_next = self.target_net(S_next) # cal Q-values of every possible actions next state (targetDQN)\n",
    "            q_target_next = self.select_indices(q_targets_next, next_actions) # select Q-value for next behavior actions\n",
    "            q_target = R + self.gamma*q_target_next*(1-D)\n",
    "        \n",
    "        # update weights\n",
    "        self.optim.zero_grad()\n",
    "        loss = self.criterion(q_target, q_behavior)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return\n",
    "        \n",
    "    def to_tensor(self, x, dtype=torch.float):\n",
    "        return torch.tensor(x, dtype=dtype, device=self.device)\n",
    "    \n",
    "    def select_indices(self, tensor, indices, dim=1):\n",
    "        if type(indices) != torch.Tensor:\n",
    "            indices = self.to_tensor(indices, dtype=torch.long)\n",
    "        if indices.dim() < 2 :\n",
    "            indices = indices.unsqueeze(1)\n",
    "        return tensor.gather(dim, indices).squeeze(1)\n",
    "    \n",
    "    def clip_reward(self, r):\n",
    "        return np.sign(r)\n",
    "    \n",
    "    \n",
    "    def update_target(self):\n",
    "        print('Update Target')\n",
    "        self.target_net.load_state_dict(self.behavior_net.state_dict())\n",
    "        return \n",
    "    \n",
    "    def test(self):\n",
    "        self.reset_episode()\n",
    "        done=False\n",
    "        while not done:\n",
    "            done = self.run_k_frames(is_training=False) # e-greedy\n",
    "                \n",
    "        self.test_reward_ls.append(self.current_episode_reward)\n",
    "        print('※Test※ \\t Frames: %s \\t Score: %s'%(self.current_episode_frame, self.current_episode_reward))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = Fitter(env, behavior_net, target_net, k=k, train_method='DoubleDQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough replay yet, expected more than 5000, but 156 instead\n",
      "Not enough replay yet, expected more than 5000, but 459 instead\n",
      "Not enough replay yet, expected more than 5000, but 581 instead\n",
      "Not enough replay yet, expected more than 5000, but 833 instead\n",
      "Not enough replay yet, expected more than 5000, but 991 instead\n",
      "Not enough replay yet, expected more than 5000, but 1169 instead\n",
      "Not enough replay yet, expected more than 5000, but 1343 instead\n",
      "Not enough replay yet, expected more than 5000, but 1482 instead\n",
      "Not enough replay yet, expected more than 5000, but 1643 instead\n",
      "Not enough replay yet, expected more than 5000, but 1817 instead\n",
      "Not enough replay yet, expected more than 5000, but 1993 instead\n",
      "Not enough replay yet, expected more than 5000, but 2186 instead\n",
      "Not enough replay yet, expected more than 5000, but 2419 instead\n",
      "Not enough replay yet, expected more than 5000, but 2692 instead\n",
      "Not enough replay yet, expected more than 5000, but 2836 instead\n",
      "Not enough replay yet, expected more than 5000, but 3043 instead\n",
      "Not enough replay yet, expected more than 5000, but 3176 instead\n",
      "Not enough replay yet, expected more than 5000, but 3352 instead\n",
      "Not enough replay yet, expected more than 5000, but 3510 instead\n",
      "Not enough replay yet, expected more than 5000, but 3711 instead\n",
      "Not enough replay yet, expected more than 5000, but 3864 instead\n",
      "Not enough replay yet, expected more than 5000, but 4076 instead\n",
      "Not enough replay yet, expected more than 5000, but 4198 instead\n",
      "Not enough replay yet, expected more than 5000, but 4360 instead\n",
      "Not enough replay yet, expected more than 5000, but 4649 instead\n",
      "Not enough replay yet, expected more than 5000, but 4779 instead\n",
      "Not enough replay yet, expected more than 5000, but 4964 instead\n",
      "Train Episode :0, Total Frame : 744, Train reward : 2.0,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d7039d1ee483>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmax_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50000000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-85d67a85a8fc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, max_total_frame)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;31m# training when enough replay-memory, every k-frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_memory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_replay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_frame\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-85d67a85a8fc>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_method\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'DoubleDQN'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mnext_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehavior_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# choose argmax behavior actions at S_next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mq_targets_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_next\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# cal Q-values of every possible actions next state (targetDQN)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9a7b14f3fd0c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_for_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mconved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mconved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconved\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconved\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9a7b14f3fd0c>\u001b[0m in \u001b[0;36mprep_for_input\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprep_for_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# to tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m255\u001b[0m                                   \u001b[0;31m# normalize into 0-1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m:\u001b[0m                        \u001b[0;31m# 4-dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_frame = 50000000\n",
    "fitter.train(max_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "ls = []\n",
    "for _ in range(100):\n",
    "    S = fitter.replay_memory.sample(1)[0][0]\n",
    "    print(fitter.behavior_net(S).data)\n",
    "    #print(fitter.behavior_net(S).argmax().item())\n",
    "    ls.append(torch.argmax(fitter.behavior_net(S), dim=1).item())\n",
    "    #plt.imshow(S[3])\n",
    "#plt.imshow(S[0])\n",
    "Counter(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
