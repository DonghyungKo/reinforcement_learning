{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import the gym module\n",
    "import gym\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a breakout environment\n",
    "env = gym.make('BreakoutDeterministic-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "    img = np.mean(img, axis=2).astype(np.uint8) # to gray, uint8 for low memory\n",
    "    img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "    img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "done = False\n",
    "n = 0\n",
    "while not done:\n",
    "    if n == 0:\n",
    "        a = 1\n",
    "    else :\n",
    "        a = random.choice([0, 1, 2, 3])\n",
    "    \n",
    "    s, _, done, _ = env.step(a)\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc87c4edb70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc87c48b518>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADIZJREFUeJzt3V+sHOV9xvHvU2OXJik1JoBc7NQgUQKVgqEuNaKqUqgbkiLIRaigaYsiJG7SCtRUKeSmrdRK5CYhFxUSAlIuaIASUBBKoMgBJZUqh/8kYAiEUnxkYzv8EWmQUuz8erFDc0qP4znn7O45c97vR1rtzruzZ945o2ffmd3Z+aWqkNSWX1jqDkiaPoMvNcjgSw0y+FKDDL7UIIMvNcjgSw1aVPCTnJ/kuSQvJLl6XJ2SNFlZ6Ak8SVYB3we2ATPAw8ClVfXM+LonaRKOWMRrzwJeqKoXAZLcBlwEHDL471+3qjZtXN3rj3//qfcsomvSyvLrH3qr13wv7XqbH752MIebbzHBPwHYNWt6Bvjtn/eCTRtX8537N/b64x/51c0L75m0wtx//xO95jvrI7sOPxOLO8af613l/x03JLkiySNJHtn/6sFFLE7SuCwm+DPA7OF7A7D73TNV1Q1VtaWqthx7zKpFLE7SuCwm+A8DJyc5Mcka4BLgnvF0S9IkLfgYv6oOJPlz4H5gFXBzVT09tp5JmpjFfLhHVX0d+PqY+iJpSjxzT2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcadNjgJ7k5yb4k35vVti7JA0me7+6Pnmw3JY1TnxH/n4Dz39V2NbC9qk4GtnfTkgbisMGvqm8Br72r+SLglu7xLcDHx9wvSRO00GP846tqD0B3f9z4uiRp0ib+4Z4ltKTlZ6HB35tkPUB3v+9QM1pCS1p+Fhr8e4DLuseXAV8bT3ckTUOfr/O+Avw7cEqSmSSXA9cC25I8D2zrpiUNxGFLaFXVpYd46rwx9+X/2Prk25P881LTPHNPapDBlxpk8KUGGXypQQZfapDBlxpk8KUGHfZ7/KVy4i/uX+ouSCuWI77UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw1att/j7337V5a6C9Iycsir2y2II77UIIMvNajPNfc2Jnkwyc4kTye5smu3jJY0UH1G/APAZ6rqVGAr8Okkp2EZLWmw+pTQ2lNVj3WPfwTsBE7AMlrSYM3rGD/JJuAMYAeW0ZIGq3fwk7wP+CpwVVW9OY/XWUJLWmZ6fY+fZDWj0N9aVXd1zXuTrK+qPT+vjFZV3QDcALDl9COrb8fWr36976yS5qnPp/oBbgJ2VtUXZj1lGS1poPqM+OcAfwp8N8kTXdvnGJXNuqMrqfUycPFkuihp3PqU0Po3IId4eqJltCRNhmfuSQ0y+FKDDL7UIIMvNWjZ/h7/jld+a6m7IM3b2x/e03ve1Q+t7z3vnx31jYV055Ac8aUGGXypQQZfapDBlxpk8KUGGXypQQZfapDBlxq0bE/gOfWoV5a6C4Py1Jm9r3HChx471I8tNZf5/G/nYz4n+7B7vMt2xJcaZPClBhl8qUEGX2qQwZca1Ocqu0cm+U6SJ7vaeX/XtZ+YZEdXO+/2JGsm311J49BnxP8JcG5VnQ5sBs5PshX4PPDFrnbe68Dlk+umpHHqc5XdAv6rm1zd3Qo4F/jjrv0W4G+B68fVsUl9dyr/t+p5jJ9kVXdN/X3AA8APgDeq6kA3ywyjQppzvdYSWtIy0yv4VXWwqjYDG4CzgFPnmu0Qr72hqrZU1ZZjj1m18J5KGpt5fapfVW8ADwFbgbVJ3jlU2MDYTyqUNCl9PtU/Nsna7vEvAb8P7AQeBD7RzWbtPGlA+vxIZz1wS5JVjN4o7qiqe5M8A9yW5O+BxxkV1pQ0AH0+1X8KOGOO9hcZHe9LGhjP3JMaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBvUOfndt/ceT3NtNW0JLGqj5jPhXMrq67jssoSUNVN9KOhuAPwRu7KbDqITWnd0stwAfn0QHJY1f3xH/OuCzwE+76WOwhJY0WH0KalwA7KuqR2c3zzGrJbSkgehTUOMc4MIkHwOOBI5itAewNskR3ahvCS1pQA474lfVNVW1oao2AZcA36yqT2IJLWmwFvM9/l8Df5nkBUbH/JbQkgaiz67+/6qqhxhVy7WEljRgnrknNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw3qdemtJC8BPwIOAgeqakuSdcDtwCbgJeCPqur1yXRT0jjNZ8T/varaXFVbuumrge1dCa3t3bSkAVjMrv5FjEpngSW0pEHpG/wC/jXJo0mu6NqOr6o9AN39cZPooKTx63t57XOqaneS44AHkjzbdwHdG8UVAB84YV5X85Y0Ib1G/Kra3d3vA+5mdD39vUnWA3T3+w7xWmvnSctMn6KZ703yy+88Bv4A+B5wD6PSWWAJLWlQ+ux7Hw/cneSd+f+5qu5L8jBwR5LLgZeBiyfXTUnjdNjgd6WyTp+j/VXgvEl0StJkeeae1CCDLzXI4EsNMvhSgwy+1CCDLzXI4EsNMvhSgwy+1CCDLzXI4EsNMvhSgwy+1CCDLzXI4EsNMvhSgwy+1CCDLzWoV/CTrE1yZ5Jnk+xMcnaSdUkeSPJ8d3/0pDsraTz6jvhfAu6rqg8yuv7eTiyhJQ1Wn8trHwX8LnATQFX9d1W9gSW0pMHqM+KfBOwHvpzk8SQ3dtfXt4SWNFB9gn8EcCZwfVWdAfyYeezWJ7kiySNJHtn/6sEFdlPSOPUJ/gwwU1U7uuk7Gb0RWEJLGqjDBr+qXgF2JTmlazoPeAZLaEmD1bd87V8AtyZZA7wIfIrRm4YltKQB6hX8qnoC2DLHU5bQkgbIM/ekBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUF9CmqckuSJWbc3k1xlCS1puPpcZfe5qtpcVZuB3wTeAu7GElrSYM13V/884AdV9Z9YQksarPkG/xLgK91jS2hJA9U7+N019S8E/mU+C7CElrT8zGfE/yjwWFXt7aYtoSUN1HyCfyk/280HS2hJg9Ur+EneA2wD7prVfC2wLcnz3XPXjr97kiahbwmtt4Bj3tX2KpbQkgbJM/ekBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZca1OtCHOOy9+Aarnt90zQXKS0rW598e0Gv+5v9v9Frvt0H9veazxFfapDBlxpk8KUGGXypQamq6S0s2Q/8GPjh1BY6Xe9nZa6b6zUcv1ZVxx5upqkGHyDJI1W1ZaoLnZKVum6u18rjrr7UIIMvNWgpgn/DEixzWlbqurleK8zUj/ElLT139aUGTTX4Sc5P8lySF5JcPc1lj1OSjUkeTLIzydNJruza1yV5IMnz3f3RS93XhUiyKsnjSe7tpk9MsqNbr9uTrFnqPi5EkrVJ7kzybLftzl4p22y+phb8JKuAfwQ+CpwGXJrktGktf8wOAJ+pqlOBrcCnu3W5GtheVScD27vpIboS2Dlr+vPAF7v1eh24fEl6tXhfAu6rqg8CpzNax5WyzeanqqZyA84G7p81fQ1wzbSWP+F1+xqwDXgOWN+1rQeeW+q+LWBdNjAKwLnAvUAYneRyxFzbcSg34CjgP+g+15rVPvhttpDbNHf1TwB2zZqe6doGLckm4AxgB3B8Ve0B6O6PW7qeLdh1wGeBn3bTxwBvVNWBbnqo2+0kYD/w5e4w5sYk72VlbLN5m2bwM0fboL9SSPI+4KvAVVX15lL3Z7GSXADsq6pHZzfPMesQt9sRwJnA9VV1BqNTx9vYrZ/DNIM/A2ycNb0B2D3F5Y9VktWMQn9rVd3VNe9Nsr57fj2wb6n6t0DnABcmeQm4jdHu/nXA2iTvXLRlqNttBpipqh3d9J2M3giGvs0WZJrBfxg4ufuEeA1wCXDPFJc/NkkC3ATsrKovzHrqHuCy7vFljI79B6OqrqmqDVW1idH2+WZVfRJ4EPhEN9vg1gugql4BdiU5pWs6D3iGgW+zhZr2r/M+xmgEWQXcXFX/MLWFj1GS3wG+DXyXnx0Lf47Rcf4dwAeAl4GLq+q1JenkIiX5MPBXVXVBkpMY7QGsAx4H/qSqfrKU/VuIJJuBG4E1wIvApxgNfitim82HZ+5JDfLMPalBBl9qkMGXGmTwpQYZfKlBBl9qkMGXGmTwpQb9D2w9KDPx46tzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(preprocess(s)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random \n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "    \n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        self.deque.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.deque, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameHistory(object):\n",
    "    # consecutive frame을 4개씩 병합하여 새로운 x를 하나씩 만드는 클래스\n",
    "    # 만들어진 x는 4개씩 concat하고 s를 만들어서 DQN의 input으로 들어감\n",
    "    def __init__(self, env, k_history=4):\n",
    "        self.state_deque = deque(maxlen=k_history) # [x1, x2, x3, x4]\n",
    "        self.frame_history_deque = deque(maxlen=k_history) #[s1, s2, s3, s4]\n",
    "        self.k_history = k_history\n",
    "        \n",
    "        # 최초에는 초기 화면(env.reset)으로 채워둠\n",
    "        self.env = env\n",
    "        self.initial_state = self.preprocess(self.env.reset())\n",
    "        self.reset()\n",
    "        \n",
    "    def preprocess(self, img):\n",
    "        # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "        img = np.mean(img, axis=2).astype(np.uint8) # to gray, uint8 for low memory\n",
    "        img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "        img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "        return img\n",
    "    \n",
    "    def append_frame(self, s):\n",
    "        # if full, aggregate and clear old frames, append new frame to history queue\n",
    "        if len(self) == self.k_history:\n",
    "            self.state_deque.append(self.aggregate_frame())\n",
    "            \n",
    "        self.frame_history_deque.append(self.preprocess(s))\n",
    "        return\n",
    "        \n",
    "    def aggregate_frame(self):\n",
    "        if len(self) < self.k_history:\n",
    "            raise ValueError('not enough frames in history, expected %s, but got %s'%(self.k_history, len(self)))\n",
    "        \n",
    "        # element-wise maximum to aggregate\n",
    "        return np.maximum.reduce([self.frame_history_deque.pop() for _ in range(len(self))])\n",
    "    \n",
    "    def get_state(self):\n",
    "        S = np.array([self.state_deque[i] for i in range(self.k_history)]) # S = [x1, x2, x3, x4], (4, 1, 80, 80)\n",
    "        S = np.swapaxes(S, 0, 1) # (1, 4, 80, 80)\n",
    "        return S\n",
    "    \n",
    "    def reset(self):\n",
    "        self.frame_history_deque = deque(maxlen=k_history)\n",
    "        self.state_deque = deque(maxlen=k_history)\n",
    "        \n",
    "        for _ in range(self.k_history):\n",
    "            self.state_deque.append(self.initial_state)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_history_deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(nn.Module):\n",
    "    # Dueling DQN\n",
    "    def __init__(self, in_dim, n_action):\n",
    "        super(DDQN, self).__init__()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, 32, (8, 8), stride=4), nn.ReLU(), # conv 1\n",
    "            nn.Conv2d(32, 64, (4, 4), stride=2), nn.ReLU(), # conv 2\n",
    "            nn.Conv2d(64, 64, (3, 3), stride=1), nn.ReLU() # conv 3\n",
    "        )\n",
    "        # V(s) : value-stream\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(64*6*6, 512), nn.ReLU(), # hidden layer\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "        # A(s,a) : action-stream\n",
    "        self.action_stream = nn.Sequential(\n",
    "            nn.Linear(64*6*6, 512), nn.ReLU(),\n",
    "            nn.Linear(512, n_action)\n",
    "        )\n",
    "        \n",
    "    def to_tensor(self, img):\n",
    "        img = torch.tensor(img, dtype=torch.float32, device=self.device).cuda(non_blocking=True) # to tensor\n",
    "        img /= 255                                   # normalize into 0-1\n",
    "        while img.dim() < 4 :                        # 4-dim\n",
    "            img = img.unsqueeze(0)\n",
    "        return img\n",
    "        \n",
    "    def forward(self, frames):\n",
    "        frames = self.to_tensor(frames) \n",
    "        conved = self.conv(frames)\n",
    "        conved = conved.view(conved.size(0), -1)\n",
    "        \n",
    "        # key of Dueling DQN : Q(s,a_i) = V(S) + A(s,a_i) - mean(A)\n",
    "        value = self.value_stream(conved) # V(s), dim=(batch_size, 1)\n",
    "        action = self.action_stream(conved) # A(s,a), dim=(batch_size, n_action)\n",
    "        \n",
    "        output = value + (action - torch.mean(action, dim=1, keepdim=True)) # V(s) + (A(s,a_i) - mean(A)), dim=(batch_size, n_action)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DDQN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(2, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (value_stream): Sequential(\n",
       "    (0): Linear(in_features=2304, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (action_stream): Sequential(\n",
       "    (0): Linear(in_features=2304, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############\n",
    "k_history = 2 ## number of skipped frame\n",
    "#############\n",
    "\n",
    "behavior_net = DDQN(in_dim=k_history, n_action=4)\n",
    "target_net = DDQN(in_dim=k_history, n_action=4)\n",
    "\n",
    "behavior_net.to(behavior_net.device) # model to cuda\n",
    "target_net.to(target_net.device)     # model to cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_decay(n_total_frame):\n",
    "    return np.max([1 - 9.0*1e-07*n_total_frame, 0.1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grow_batch_size(current_memory, max_memory):\n",
    "    # grow batch size from 32 to 128 as respect to memory size\n",
    "    memory_ratio = current_memory / max_memory\n",
    "    new_batch_size = int(32 + 96*memory_ratio)\n",
    "    return new_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter():\n",
    "    def __init__(self, env, behavior_net, target_net, k_history=4, train_method='DQN'):\n",
    "        self.env = env\n",
    "        self.train_method = train_method\n",
    "        \n",
    "        # networks\n",
    "        self.behavior_net = behavior_net\n",
    "        self.target_net = target_net\n",
    "        self.behavior_net.to(self.behavior_net.device) # model to cuda\n",
    "        self.target_net.to(self.target_net.device)     # model to cuda\n",
    "        \n",
    "        # train parameters\n",
    "        self.gamma = 0.99 #as written in paper\n",
    "        self.criterion = nn.SmoothL1Loss() # huber loss (error-clipping)\n",
    "        self.optim = torch.optim.RMSprop(self.behavior_net.parameters(), lr=0.00025) #as written in paper\n",
    "        self.k_history = k_history\n",
    "        \n",
    "        # train details\n",
    "        self.total_frame = 0\n",
    "        self.total_episode = 0\n",
    "        self.batch_size = 32\n",
    "        self.max_replay = 1000000 # as written in paper\n",
    "        self.min_replay = 50000 # as written in the paper\n",
    "        self.replay_memory = ReplayMemory(self.max_replay)\n",
    "        self.frame_history = FrameHistory(env=self.env, k_history=self.k_history)\n",
    "        self.reward_ls = []\n",
    "    \n",
    "    \n",
    "    def train(self, max_total_frame):\n",
    "        while self.total_frame < max_total_frame :\n",
    "            self.total_episode += 1\n",
    "            \n",
    "            s = self.env.reset()  # small s represents each frame\n",
    "            self.frame_history.reset() #frame history reset\n",
    "            lives_remain = 5        # lives\n",
    "            n_step = 0       # each episode의 step의 횟수 \n",
    "            \n",
    "            # re-calculate epsilon, batch_size\n",
    "            e = epsilon_decay(self.total_frame)\n",
    "            batch_size = grow_batch_size(len(self.replay_memory), self.max_replay)\n",
    "            \n",
    "            done = False\n",
    "            while not done:\n",
    "                # update target_net every 10,000 steps\n",
    "                if self.total_frame%10000 == 0 :\n",
    "                    self.update_target()\n",
    "                \n",
    "                self.total_frame += 1\n",
    "                n_step += 1\n",
    "                \n",
    "                # start of every k consecutive frames, choose action!                \n",
    "                if n_step%self.k_history == 1:\n",
    "                    # S = [x1, x2, x3, x4] for neural-network input\n",
    "                    # x represents feature(element-wise max) from every k consecutive frames(k*s)\n",
    "                    S = self.frame_history.get_state()\n",
    "                    r_sum = 0   # sum of reward for the following k frames\n",
    "                    \n",
    "                    # if it is the first frame of the game, Start game!\n",
    "                    if n_step == 1:\n",
    "                        a = 1\n",
    "                    else:  # else, Choose an action by e-greedy\n",
    "                        if np.random.rand(1) < e :\n",
    "                            a = self.env.action_space.sample()\n",
    "                        else:\n",
    "                            q_pred = self.behavior_net(S)\n",
    "                            a = torch.argmax(q_pred).item()\n",
    "                \n",
    "                # repeat the same action k times\n",
    "                s_next, r, done, info, = self.env.step(a)\n",
    "                \n",
    "                if info['ale.lives'] < lives_remain:\n",
    "                    lives_remain = info['ale.lives'] # life를 잃으면 페널티\n",
    "                    r -= 1\n",
    "                \n",
    "                self.frame_history.append_frame(s_next)\n",
    "                r_sum += r\n",
    "\n",
    "                # end of every k consecutive frames\n",
    "                # genenrate new x, and update S\n",
    "                if n_step%self.k_history == 0 :\n",
    "                    S_new = self.frame_history.get_state()\n",
    "                    self.replay_memory.append(S, a, r_sum, S_new, done)\n",
    "                \n",
    "                # no training when not enough replay\n",
    "                if len(self.replay_memory) < self.min_replay:\n",
    "                    continue\n",
    "                \n",
    "                # train every k steps\n",
    "                if n_step % self.k_history == 0:\n",
    "                    mini_batch = self.replay_memory.sample(batch_size)\n",
    "                    self.train_batch(mini_batch)                \n",
    "                # single-episode(game) is now done\n",
    "            \n",
    "            # no updating target, no testing when not enough replay\n",
    "            if len(self.replay_memory) < self.min_replay:\n",
    "                continue\n",
    "\n",
    "            # update target, do test for every 10 episodes(games)\n",
    "            if self.total_episode%10 == 0:\n",
    "                print('total_frame: %s'%self.total_frame)\n",
    "                self.test()           \n",
    "                    \n",
    "    def train_batch(self, mini_batch):\n",
    "        batch_size = len(mini_batch)\n",
    "        S = np.array([tup[0] for tup in mini_batch]).squeeze(1) # State\n",
    "        A = np.array([tup[1] for tup in mini_batch]) # actions\n",
    "        R = np.array([tup[2] for tup in mini_batch]) # rewards\n",
    "        S_next = np.array([tup[3] for tup in mini_batch]).squeeze(1) # next_State\n",
    "        D = np.array([tup[4] for tup in mini_batch]) # dones\n",
    "        \n",
    "        q_targets = self.target_net(S) # Q-values of current state with targetDDQN\n",
    "        q_targets_next = self.target_net(S_next) # Q-values of next state from targetDDQN\n",
    "        \n",
    "        if self.train_method=='DQN':\n",
    "            for i in range(batch_size):\n",
    "                a, r, done = A[i], R[i], D[i]\n",
    "                if done:\n",
    "                    q_targets[i, a] = r\n",
    "                else:\n",
    "                    q_targets[i, a] = r + self.gamma*torch.max(q_targets_next[i])\n",
    "        \n",
    "        elif self.train_method=='DoubleDQN':\n",
    "            next_behavior_net_actions = torch.argmax(self.behavior_net(S_next), dim=1) # choose argmax actions from behaviorDDQN in S_next\n",
    "                    \n",
    "            for i in range(batch_size):\n",
    "                a, r, done = A[i], R[i], D[i]\n",
    "                next_behavior_net_action = next_behavior_net_actions[i].item() # choose argmax actions from behaviorDDQN in S_next\n",
    "                if done:\n",
    "                    q_targets[i, a] = r\n",
    "                else:\n",
    "                    q_targets[i, a] = r + self.gamma*q_targets_next[i, next_behavior_net_action]\n",
    "            \n",
    "        # 예측치(pred)와 목표치(true)\n",
    "        q_behaviors = self.behavior_net(S)\n",
    "        self.optim.zero_grad()\n",
    "        loss = self.criterion(q_targets, q_behaviors)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return\n",
    "\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.behavior_net.state_dict())\n",
    "        return \n",
    "    \n",
    "    \n",
    "    def test(self):\n",
    "        self.frame_history.reset()\n",
    "        initial_state = self.env.reset()  # save initial state for comparison\n",
    "        s = self.env.reset()\n",
    "        e = 0.05 # e-greedy, \n",
    "        \n",
    "        n_step = 0\n",
    "        r_sum = 0\n",
    "        \n",
    "        done=False\n",
    "        while not done:\n",
    "            n_step += 1\n",
    "            \n",
    "            # 초기 화면에서는 1의 액션을 실행\n",
    "            if n_step==1:\n",
    "                a = 1\n",
    "            else:\n",
    "                # e-greedy search, e=0.05\n",
    "                if np.random.rand(1) < e:\n",
    "                    a = self.env.action_space.sample()\n",
    "                else:\n",
    "                    S = self.frame_history.get_state()\n",
    "                    a = torch.argmax(self.behavior_net(S)).item()\n",
    "            \n",
    "            s, r, done, _ = self.env.step(a)\n",
    "            self.frame_history.append_frame(s)\n",
    "            r_sum += r\n",
    "        \n",
    "        self.reward_ls.append(r_sum)\n",
    "        print('Total Step: %s \\t Total Score: %s'%(n_step, r_sum))\n",
    "        return r_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = Fitter(env, behavior_net, target_net, k_history=k_history, train_method='DoubleDQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_frame = 10000000\n",
    "fitter.train(max_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# how skip-frame works\n",
    "plt.imshow(fitter.S[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
