{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import the gym module\n",
    "import gym\n",
    "import numpy as np\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a breakout environment\n",
    "env = gym.make('BreakoutDeterministic-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "    img = np.mean(img, axis=2).astype(np.uint8) # to gray, uint8 for low memory\n",
    "    img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "    img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "\n",
    "s2, _, _, _ = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    a = env.action_space.sample()\n",
    "    s2, _, done, _ = env.step(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f412810d6a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f412801eda0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(preprocess(s2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random \n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "    \n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        self.deque.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.deque, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryTransformer(object):\n",
    "    def __init__(self, n_history=4):\n",
    "        self.n_history = n_history\n",
    "        self.deque = deque(maxlen=n_history)\n",
    "        self.env = gym.make('BreakoutDeterministic-v4')\n",
    "        \n",
    "        # 최초에는 처음 화면(env.reset) 0으로 채워둠\n",
    "        padding_state = preprocess(self.env.reset())\n",
    "        for _ in range(n_history):\n",
    "            self.deque.append(padding_state)\n",
    "    \n",
    "    def preprocess(self, img):\n",
    "        # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "        img = np.mean(img, axis=2).astype(np.uint8) # to gray, uint8 for low memory\n",
    "        img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "        img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "        return img\n",
    "    \n",
    "    def transform(self, s):\n",
    "        # transform할 때 마다, 현재 state를 기록하고, 과거 state를 밀어내면서 저장함\n",
    "        self.deque.append(self.preprocess(s))\n",
    "        return np.stack([self.deque[i] for i in range(self.n_history)], axis=1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(nn.Module):\n",
    "    def __init__(self, in_dim, n_action):\n",
    "        super(DDQN, self).__init__()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, 32, (8, 8), stride=4), nn.ReLU(), # conv 1\n",
    "            nn.Conv2d(32, 64, (4, 4), stride=2), nn.ReLU(), # conv 2\n",
    "            nn.Conv2d(64, 64, (3, 3), stride=1), nn.ReLU() # conv 3\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*6*6, 512), nn.ReLU(), # hidden layer\n",
    "            nn.Linear(512, n_action)\n",
    "        )\n",
    "        \n",
    "    def to_tensor(self, img):\n",
    "        img = torch.tensor(img, dtype=torch.float32, device=self.device).cuda(non_blocking=True) # to tensor\n",
    "        img /= 255                                   # normalize into 0-1\n",
    "        while img.dim() < 4 :                        # 4-dim\n",
    "            img = img.unsqueeze(0)\n",
    "        return img\n",
    "        \n",
    "    def forward(self, frames):\n",
    "        frames = self.to_tensor(frames) \n",
    "        conved = self.conv(frames)\n",
    "        conved = conved.view(conved.size(0), -1)\n",
    "        output = self.fc(conved)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DDQN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2304, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviorDDQN = DDQN(in_dim=4, n_action=4)\n",
    "targetDDQN = DDQN(in_dim=4, n_action=4)\n",
    "\n",
    "behaviorDDQN.to(behaviorDDQN.device) # model to cuda\n",
    "targetDDQN.to(targetDDQN.device)     # model to cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter():\n",
    "    def __init__(self, behaviorDDQN, targetDDQN, n_history=4):\n",
    "        self.behaviorDDQN = behaviorDDQN\n",
    "        self.targetDDQN = targetDDQN\n",
    "        self.out_dim = env.action_space.n\n",
    "        self.env = gym.make('BreakoutDeterministic-v4')\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        self.criterion = nn.SmoothL1Loss()\n",
    "        self.optim = torch.optim.RMSprop(\n",
    "            self.behaviorDDQN.parameters(), \n",
    "            lr=0.00025,\n",
    "            eps=0.01,\n",
    "            momentum=0.95\n",
    "        )\n",
    "            \n",
    "        \n",
    "    def train(self, train_batch):\n",
    "        batch_size = len(train_batch)\n",
    "        S = np.array([tup[0] for tup in train_batch]).squeeze(1) # states\n",
    "        A = np.array([tup[1] for tup in train_batch]) # actions\n",
    "        R = np.array([tup[2] for tup in train_batch]) # rewards\n",
    "        S_next = np.array([tup[3] for tup in train_batch]).squeeze(1) # next_states \n",
    "        D = np.array([tup[4] for tup in train_batch]) # dones\n",
    "        \n",
    "        q_targets = self.targetDDQN(S)           # Q-values of current state with targetDDQN\n",
    "        q_targets_next = self.targetDDQN(S_next) # Q-values of next state from targetDDQN\n",
    "        next_behavior_network_actions = torch.argmax(self.behaviorDDQN(S_next), dim=1) # choose argmax actions from behaviorDDQN in S_next\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            a, r, done = A[i], R[i], D[i]\n",
    "            next_action = next_behavior_network_actions[i].item() # choose argmax actions from behaviorDDQN in S_next\n",
    "            if done:\n",
    "                q_targets[i, a] = r\n",
    "            else:\n",
    "                q_targets[i, a] = r + self.gamma*q_targets_next[i, next_action]\n",
    "            \n",
    "        # 예측치(pred)와 목표치(true)\n",
    "        q_behavior = self.behaviorDDQN(S)\n",
    "        \n",
    "        self.optim.zero_grad()\n",
    "        loss = self.criterion(q_targets, q_behavior)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.targetDDQN.load_state_dict(self.behaviorDDQN.state_dict())\n",
    "        return \n",
    "    \n",
    "    def test(self):\n",
    "        temp_hist_transformer = HistoryTransformer()\n",
    "        initial_state = self.env.reset()\n",
    "        n_step = 0\n",
    "        r_sum = 0\n",
    "        done=False\n",
    "        \n",
    "        while not done:\n",
    "            n_step += 1\n",
    "                \n",
    "            # 게임의 맨 처음에는 움직여야 게임이 시작됨\n",
    "            if n_step == 1:\n",
    "                a = random.choice([1,2,3])\n",
    "                s, _, _, _ = self.env.step(a)  \n",
    "                continue\n",
    "            \n",
    "            # 게임이 시작된 후, 공을 놓치면, life가 깍이고 다시 초기화면으로 돌아옴 \n",
    "            # 초기 화면에서는 다시 또 1,2,3 중 임의의 액션을 실행\n",
    "            if np.array_equal(s, initial_state):\n",
    "                a = random.choice([1, 2, 3])\n",
    "            else:\n",
    "                s_trans = temp_hist_transformer.transform(s)\n",
    "                a = torch.argmax(self.behaviorAtari(s_trans)).item()\n",
    "            \n",
    "            s, r, done, _ = self.env.step(a)\n",
    "            r_sum += r\n",
    "            \n",
    "        print('Total Step: %s \\t Total Score: %s'%(n_step, r_sum))\n",
    "        return r_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = Fitter(behaviorDDQN, targetDDQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a breakout environment\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "n_frame = 0\n",
    "n_episode = 0\n",
    "batch_size = 32\n",
    "\n",
    "max_memory = 1000000\n",
    "replay_memory = ReplayMemory(max_memory)\n",
    "\n",
    "min_replay = 50000 # 50,000 in the paper\n",
    "reward_ls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_decay(episode_idx):\n",
    "    return np.max([1 - 9.0*1e-07*episode_idx, 0.1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grow_batch_size(current_memory):\n",
    "    # grow batch size from 32 to 128 as respect to memory size\n",
    "    memory_ratio = current_memory / max_memory\n",
    "    new_batch_size = int(32 + 96*memory_ratio)\n",
    "    return new_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_Frame : 881883\n",
      "Total Step: 155 \t Total Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "while n_frame < 15000000 :\n",
    "    n_episode += 1\n",
    "    hist_transformer = HistoryTransformer() # history recorder (past 4 state)\n",
    "    s = env.reset()\n",
    "    s = hist_transformer.transform(s)\n",
    "    e = epsilon_decay(n_frame)\n",
    "    done = False\n",
    "    n_step = 0\n",
    "    \n",
    "    while not done:\n",
    "        n_frame += 1\n",
    "        n_step += 1\n",
    "        \n",
    "        # 초기 화면에서도 역시 일단 움직여야 게임이 시작됨\n",
    "        if n_step == 1:\n",
    "            a = random.choice([1, 2, 3])\n",
    "        else:\n",
    "            # Choose an action by e-greedy\n",
    "            q_pred = fitter.behaviorDDQN(s)\n",
    "            \n",
    "            if np.random.rand(1) < e :\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                a = torch.argmax(q_pred).item()\n",
    "        \n",
    "        # action 실행 후, replay_memory에 기록\n",
    "        s_next, r, done, _ = env.step(a)\n",
    "        if done:  \n",
    "            r = -10\n",
    "        \n",
    "        # s_next를 변형 후 저장\n",
    "        s_next = hist_transformer.transform(s_next)\n",
    "        replay_memory.append(s, a , r, s_next, done)\n",
    "        \n",
    "        s = s_next\n",
    "        \n",
    "        # minimum replay가 min_replay클 때까지는 학습 안함\n",
    "        if len(replay_memory) < min_replay:\n",
    "            continue\n",
    "        \n",
    "        # 4-th step 마다, 동일한 batch를 4번씩 학습\n",
    "        if n_step%4 == 0:\n",
    "            for _ in range(4):\n",
    "                batch_size = grow_batch_size(len(replay_memory))\n",
    "                mini_batch = replay_memory.sample(batch_size)\n",
    "                loss = fitter.train(mini_batch)\n",
    "\n",
    "    # minimum replay가 min_replay클 때까지는 테스트와 업데이트 안함\n",
    "    if len(replay_memory) < min_replay:\n",
    "        continue\n",
    "\n",
    "    # update target every 5 episode\n",
    "    if n_episode%10 == 0:\n",
    "        fitter.update_target()\n",
    "        \n",
    "    # test every 50 episode\n",
    "    if n_episode%10 == 0:\n",
    "        print('N_Frame : %s'%n_frame)\n",
    "        reward = fitter.test()\n",
    "        reward_ls.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
