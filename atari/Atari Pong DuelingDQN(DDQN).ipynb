{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import the gym module\n",
    "import gym\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a breakout environment\n",
    "env = gym.make('PongNoFrameskip-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "    img = np.dot(img[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "    #img = np.mean(img, axis=2, dtype=np.uint8)\n",
    "    img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "    img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "    return img.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3373"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "done = False\n",
    "n = 0\n",
    "while not done:\n",
    "    a = env.action_space.sample()\n",
    "    s, _, done, _ = env.step(a)\n",
    "    n += 1\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ffaf41e8a90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ffaf417e550>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(preprocess(s)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random \n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "    \n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        self.deque.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.deque, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameHistory(object):\n",
    "    # consecutive frame을 4개씩 병합하여 새로운 x를 하나씩 만드는 클래스\n",
    "    # 만들어진 x는 4개씩 concat하고 s를 만들어서 DQN의 input으로 들어감\n",
    "    def __init__(self, env, k_history=4):\n",
    "        self.state_deque = deque(maxlen=k_history) # [x1, x2, x3, x4]\n",
    "        self.frame_history_deque = deque(maxlen=k_history) #[s1, s2, s3, s4]\n",
    "        self.k_history = k_history\n",
    "        \n",
    "        # 최초에는 초기 화면(env.reset)으로 채워둠\n",
    "        self.env = env\n",
    "        self.initial_state = self.preprocess(self.env.reset())\n",
    "        self.reset()\n",
    "        \n",
    "    def preprocess(self, img):\n",
    "        # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "        img = np.dot(img[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "        img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "        img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "        return img.astype(np.uint8)\n",
    "    \n",
    "    def append_frame(self, s):\n",
    "        # if full, aggregate and clear old frames, append new frame to history queue\n",
    "        if len(self) == self.k_history:\n",
    "            self.state_deque.append(self.aggregate_frame())\n",
    "            \n",
    "        self.frame_history_deque.append(self.preprocess(s))\n",
    "        return\n",
    "        \n",
    "    def aggregate_frame(self):\n",
    "        if len(self) < self.k_history:\n",
    "            raise ValueError('not enough frames in history, expected %s, but got %s'%(self.k_history, len(self)))\n",
    "        \n",
    "        # element-wise maximum to aggregate\n",
    "        return np.maximum.reduce([self.frame_history_deque.pop() for _ in range(len(self))][-k_history//2:])\n",
    "    \n",
    "    def get_state(self):\n",
    "        S = np.array([self.state_deque[i] for i in range(self.k_history)]) # S = [x1, x2, x3, x4], (4, 1, 80, 80)\n",
    "        S = np.swapaxes(S, 0, 1) # (1, 4, 80, 80)\n",
    "        return S\n",
    "    \n",
    "    def reset(self):\n",
    "        self.frame_history_deque = deque(maxlen=k_history)\n",
    "        self.state_deque = deque(maxlen=k_history)\n",
    "        \n",
    "        for _ in range(self.k_history):\n",
    "            self.state_deque.append(self.initial_state)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_history_deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(nn.Module):\n",
    "    # Dueling DQN\n",
    "    def __init__(self, in_dim, n_action):\n",
    "        super(DDQN, self).__init__()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, 32, (8, 8), stride=4), nn.ReLU(), # conv 1\n",
    "            nn.Conv2d(32, 64, (4, 4), stride=2), nn.ReLU(), # conv 2\n",
    "            nn.Conv2d(64, 64, (3, 3), stride=1), nn.ReLU() # conv 3\n",
    "        )\n",
    "        # V(s) : value-stream\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(64*6*6, 512), nn.ReLU(), # hidden layer\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "        # A(s,a) : action-stream\n",
    "        self.action_stream = nn.Sequential(\n",
    "            nn.Linear(64*6*6, 512), nn.ReLU(),\n",
    "            nn.Linear(512, n_action)\n",
    "        )\n",
    "        \n",
    "    def to_tensor(self, img):\n",
    "        img = torch.tensor(img, dtype=torch.float32, device=self.device).cuda(non_blocking=True) # to tensor\n",
    "        img /= 255                                   # normalize into 0-1\n",
    "        while img.dim() < 4 :                        # 4-dim\n",
    "            img = img.unsqueeze(0)\n",
    "        return img\n",
    "        \n",
    "    def forward(self, frames):\n",
    "        frames = self.to_tensor(frames) \n",
    "        conved = self.conv(frames)\n",
    "        conved = conved.view(conved.size(0), -1)\n",
    "        \n",
    "        # key of Dueling DQN : Q(s,a_i) = V(S) + A(s,a_i) - mean(A)\n",
    "        value = self.value_stream(conved) # V(s), dim=(batch_size, 1)\n",
    "        action = self.action_stream(conved) # A(s,a), dim=(batch_size, n_action)\n",
    "        \n",
    "        output = value + (action - torch.mean(action, dim=1, keepdim=True)) # V(s) + (A(s,a_i) - mean(A)), dim=(batch_size, n_action)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DDQN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (value_stream): Sequential(\n",
       "    (0): Linear(in_features=2304, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (action_stream): Sequential(\n",
       "    (0): Linear(in_features=2304, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############\n",
    "k_history = 4 ## number of skipped frame\n",
    "#############\n",
    "\n",
    "behavior_net = DDQN(in_dim=k_history, n_action=env.action_space.n)\n",
    "target_net = DDQN(in_dim=k_history, n_action=env.action_space.n)\n",
    "\n",
    "behavior_net.to(behavior_net.device) # model to cuda\n",
    "target_net.to(target_net.device)     # model to cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_decay(n_total_frame):\n",
    "    return np.max([1 - 9.0*1e-07*n_total_frame, 0.1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grow_batch_size(current_memory, max_memory):\n",
    "    # grow batch size from 32 to 128 as respect to memory size\n",
    "    memory_ratio = current_memory / max_memory\n",
    "    new_batch_size = int(32 + 32*memory_ratio)\n",
    "    return new_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter():\n",
    "    def __init__(self, env, behavior_net, target_net, k_history=4, train_method='DQN'):\n",
    "        self.env = env\n",
    "        self.train_method = train_method\n",
    "        \n",
    "        # networks\n",
    "        self.behavior_net = behavior_net\n",
    "        self.target_net = target_net\n",
    "        self.behavior_net.to(self.behavior_net.device) # model to cuda\n",
    "        self.target_net.to(self.target_net.device)     # model to cuda\n",
    "        \n",
    "        # train parameters\n",
    "        self.gamma = 0.99 #as written in paper\n",
    "        self.criterion = nn.SmoothL1Loss() # huber loss (error-clipping)\n",
    "        self.optim = torch.optim.RMSprop(self.behavior_net.parameters(), lr=0.00025, alpha=0.95, eps=0.01) #as written in paper in Rainbow\n",
    "        self.k_history = k_history\n",
    "        \n",
    "        # train details\n",
    "        self.total_frame = 0\n",
    "        self.total_episode = 0\n",
    "        self.batch_size = 32\n",
    "        self.max_replay = 1000000 # as written in paper\n",
    "        self.min_replay = 50000 # as written in the paper\n",
    "        self.replay_memory = ReplayMemory(self.max_replay)\n",
    "        self.frame_history = FrameHistory(env=self.env, k_history=self.k_history)\n",
    "        \n",
    "        self.train_reward_ls = []\n",
    "        self.test_reward_ls = []\n",
    "    \n",
    "    \n",
    "    def train(self, max_total_frame):\n",
    "        while self.total_frame < max_total_frame :\n",
    "            # game(episode) begins\n",
    "            self.env.reset()  \n",
    "            self.frame_history.reset() #frame history reset\n",
    "            e = epsilon_decay(self.total_frame)\n",
    "            \n",
    "            episode_frame = 0         # each episode의 step의 횟수 \n",
    "            episode_r_total = 0   # each episode의 reward sum\n",
    "            \n",
    "            done = False\n",
    "            while not done:\n",
    "                episode_frame += 1\n",
    "                \n",
    "                # start of every k consecutive frames, choose action!                \n",
    "                if episode_frame%self.k_history == 1:\n",
    "                    # S = [x1, x2, x3, x4] for neural-network input\n",
    "                    # x represents feature(element-wise max) from every k consecutive frames(k*s)\n",
    "                    S = self.frame_history.get_state()\n",
    "                    immediate_r = 0   # sum of reward for the following k frames\n",
    "                    \n",
    "                    # Choose an action by e-greedy\n",
    "                    if np.random.rand(1) < e :\n",
    "                        a = self.env.action_space.sample()\n",
    "                    else:\n",
    "                        q_pred = self.behavior_net(S)\n",
    "                        a = torch.argmax(q_pred).item()\n",
    "                \n",
    "                # repeat the same action k times\n",
    "                s_next, r, done, info, = self.env.step(a)\n",
    "                self.frame_history.append_frame(s_next)\n",
    "                \n",
    "                immediate_r += self.clip_reward(r)\n",
    "                episode_r_total += r\n",
    "                \n",
    "                # end of every k consecutive frames\n",
    "                # genenrate new x, and update S\n",
    "                if episode_frame%self.k_history == 0 :\n",
    "                    S_new = self.frame_history.get_state()\n",
    "                    self.replay_memory.append(S, a, immediate_r, S_new, done)\n",
    "                    \n",
    "                # no training when not enough replay\n",
    "                if len(self.replay_memory) < self.min_replay:\n",
    "                    continue\n",
    "                \n",
    "                # train every k steps\n",
    "                if episode_frame % self.k_history == 0:\n",
    "                    mini_batch = self.replay_memory.sample(self.batch_size)\n",
    "                    self.train_batch(mini_batch)   \n",
    "                \n",
    "                self.total_frame += 1\n",
    "                # update target_net every 10,000 steps\n",
    "                if self.total_frame%10000== 0 :\n",
    "                    self.update_target()\n",
    "                \n",
    "            \n",
    "            # single-episode(game) is now done\n",
    "            # if not enough memory, save more\n",
    "            if len(self.replay_memory) < self.min_replay:\n",
    "                print('Not enough replay yet, expected more than %s, but %s instead'%(self.min_replay, len(self.replay_memory)))\n",
    "            \n",
    "            # if enough memory, check train reward\n",
    "            else:\n",
    "                print('Train Episode :%s, Train reward : %s, Total Frame : %s'%(self.total_episode, episode_r_total, self.total_frame))\n",
    "                self.total_episode += 1\n",
    "                self.train_reward_ls.append(episode_r_total)\n",
    "            \n",
    "                # testing when enough replay, every 10 episodes\n",
    "                if self.total_episode%10 == 0:\n",
    "                    self.test()           \n",
    "    \n",
    "    \n",
    "    def clip_reward(self, r):\n",
    "        return np.sign(r)\n",
    "    \n",
    "    \n",
    "    def train_batch(self, mini_batch):\n",
    "        batch_size = len(mini_batch)\n",
    "        S = np.array([tup[0] for tup in mini_batch]).squeeze(1) # State\n",
    "        A = np.array([tup[1] for tup in mini_batch]) # actions\n",
    "        R = np.array([tup[2] for tup in mini_batch]) # rewards\n",
    "        S_next = np.array([tup[3] for tup in mini_batch]).squeeze(1) # next_State\n",
    "        D = np.array([tup[4] for tup in mini_batch]) # dones\n",
    "        \n",
    "        q_targets = self.target_net(S) # Q-values of current state with targetDDQN\n",
    "        q_targets_next = self.target_net(S_next) # Q-values of next state from targetDDQN\n",
    "        \n",
    "        if self.train_method=='DQN':\n",
    "            for i in range(batch_size):\n",
    "                a, r, done = A[i], R[i], D[i]\n",
    "                if done:\n",
    "                    q_targets[i, a] = r\n",
    "                else:\n",
    "                    q_targets[i, a] = r + self.gamma*torch.max(q_targets_next[i])\n",
    "        \n",
    "        elif self.train_method=='DoubleDQN':\n",
    "            next_behavior_net_actions = torch.argmax(self.behavior_net(S_next), dim=1) # choose argmax actions from behaviorDDQN in S_next\n",
    "                    \n",
    "            for i in range(batch_size):\n",
    "                a, r, done = A[i], R[i], D[i]\n",
    "                next_behavior_net_action = next_behavior_net_actions[i].item() # choose argmax actions from behaviorDDQN in S_next\n",
    "                if done:\n",
    "                    q_targets[i, a] = r\n",
    "                else:\n",
    "                    q_targets[i, a] = r + self.gamma*q_targets_next[i, next_behavior_net_action]\n",
    "            \n",
    "        # 예측치(pred)와 목표치(true)\n",
    "        q_behaviors = self.behavior_net(S)\n",
    "        self.optim.zero_grad()\n",
    "        loss = self.criterion(q_targets, q_behaviors)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return\n",
    "\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.behavior_net.state_dict())\n",
    "        return \n",
    "    \n",
    "    \n",
    "    def test(self):\n",
    "        self.env.reset()\n",
    "        self.frame_history.reset()\n",
    "        e = 0.05 # e-greedy, \n",
    "        \n",
    "        episode_frame = 0\n",
    "        episode_r_total = 0\n",
    "        \n",
    "        done=False\n",
    "        while not done:\n",
    "            episode_frame += 1\n",
    "            \n",
    "            # at the beginning of every 4-th frame\n",
    "            # choose an action\n",
    "            if episode_frame%self.k_history==1:\n",
    "                # e-greedy search, e=0.05 \n",
    "                if np.random.rand(1) < e:\n",
    "                    a = self.env.action_space.sample()\n",
    "                else:\n",
    "                    S = self.frame_history.get_state()\n",
    "                    a = torch.argmax(self.behavior_net(S)).item()\n",
    "            \n",
    "            s, r, done, _ = self.env.step(a)\n",
    "            self.frame_history.append_frame(s)\n",
    "            episode_r_total += r\n",
    "        \n",
    "        self.test_reward_ls.append(episode_r_total)\n",
    "        print('Total Step: %s \\t Total Score: %s'%(episode_frame, episode_r_total))\n",
    "        return episode_r_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = Fitter(env, behavior_net, target_net, k_history=k_history, train_method='DoubleDQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough replay yet, expected more than 50000, but 764 instead\n",
      "Not enough replay yet, expected more than 50000, but 1694 instead\n",
      "Not enough replay yet, expected more than 50000, but 2546 instead\n",
      "Not enough replay yet, expected more than 50000, but 3490 instead\n",
      "Not enough replay yet, expected more than 50000, but 4468 instead\n",
      "Not enough replay yet, expected more than 50000, but 5665 instead\n",
      "Not enough replay yet, expected more than 50000, but 6457 instead\n",
      "Not enough replay yet, expected more than 50000, but 7250 instead\n",
      "Not enough replay yet, expected more than 50000, but 8074 instead\n",
      "Not enough replay yet, expected more than 50000, but 8941 instead\n",
      "Not enough replay yet, expected more than 50000, but 9751 instead\n",
      "Not enough replay yet, expected more than 50000, but 10588 instead\n",
      "Not enough replay yet, expected more than 50000, but 11430 instead\n",
      "Not enough replay yet, expected more than 50000, but 12327 instead\n",
      "Not enough replay yet, expected more than 50000, but 13207 instead\n",
      "Not enough replay yet, expected more than 50000, but 13971 instead\n",
      "Not enough replay yet, expected more than 50000, but 15009 instead\n",
      "Not enough replay yet, expected more than 50000, but 15940 instead\n",
      "Not enough replay yet, expected more than 50000, but 16792 instead\n",
      "Not enough replay yet, expected more than 50000, but 17826 instead\n",
      "Not enough replay yet, expected more than 50000, but 18723 instead\n",
      "Not enough replay yet, expected more than 50000, but 19575 instead\n",
      "Not enough replay yet, expected more than 50000, but 20540 instead\n",
      "Not enough replay yet, expected more than 50000, but 21657 instead\n",
      "Not enough replay yet, expected more than 50000, but 22526 instead\n",
      "Not enough replay yet, expected more than 50000, but 23290 instead\n",
      "Not enough replay yet, expected more than 50000, but 24082 instead\n",
      "Not enough replay yet, expected more than 50000, but 24936 instead\n",
      "Not enough replay yet, expected more than 50000, but 25718 instead\n",
      "Not enough replay yet, expected more than 50000, but 26630 instead\n",
      "Not enough replay yet, expected more than 50000, but 27484 instead\n",
      "Not enough replay yet, expected more than 50000, but 28394 instead\n",
      "Not enough replay yet, expected more than 50000, but 29218 instead\n",
      "Not enough replay yet, expected more than 50000, but 30070 instead\n",
      "Not enough replay yet, expected more than 50000, but 30834 instead\n",
      "Not enough replay yet, expected more than 50000, but 31837 instead\n",
      "Not enough replay yet, expected more than 50000, but 32663 instead\n",
      "Not enough replay yet, expected more than 50000, but 33455 instead\n",
      "Not enough replay yet, expected more than 50000, but 34219 instead\n",
      "Not enough replay yet, expected more than 50000, but 34983 instead\n",
      "Not enough replay yet, expected more than 50000, but 35835 instead\n",
      "Not enough replay yet, expected more than 50000, but 36715 instead\n",
      "Not enough replay yet, expected more than 50000, but 37782 instead\n",
      "Not enough replay yet, expected more than 50000, but 38774 instead\n",
      "Not enough replay yet, expected more than 50000, but 39720 instead\n",
      "Not enough replay yet, expected more than 50000, but 40658 instead\n",
      "Not enough replay yet, expected more than 50000, but 41529 instead\n",
      "Not enough replay yet, expected more than 50000, but 42469 instead\n",
      "Not enough replay yet, expected more than 50000, but 43460 instead\n",
      "Not enough replay yet, expected more than 50000, but 44284 instead\n",
      "Not enough replay yet, expected more than 50000, but 45162 instead\n",
      "Not enough replay yet, expected more than 50000, but 46063 instead\n",
      "Not enough replay yet, expected more than 50000, but 47057 instead\n",
      "Not enough replay yet, expected more than 50000, but 47939 instead\n",
      "Not enough replay yet, expected more than 50000, but 48889 instead\n",
      "Not enough replay yet, expected more than 50000, but 49788 instead\n",
      "Train Episode :0, Train reward : -21.0, Total Frame : 2209\n",
      "Train Episode :1, Train reward : -20.0, Total Frame : 5930\n",
      "Train Episode :2, Train reward : -21.0, Total Frame : 8986\n",
      "Train Episode :3, Train reward : -18.0, Total Frame : 13089\n",
      "Train Episode :4, Train reward : -21.0, Total Frame : 16145\n",
      "Train Episode :5, Train reward : -21.0, Total Frame : 19313\n",
      "Train Episode :6, Train reward : -20.0, Total Frame : 23332\n",
      "Train Episode :7, Train reward : -20.0, Total Frame : 26699\n",
      "Train Episode :8, Train reward : -21.0, Total Frame : 30219\n",
      "Train Episode :9, Train reward : -21.0, Total Frame : 33635\n",
      "Total Step: 3304 \t Total Score: -21.0\n",
      "Train Episode :10, Train reward : -21.0, Total Frame : 36885\n",
      "Train Episode :11, Train reward : -21.0, Total Frame : 40293\n",
      "Train Episode :12, Train reward : -21.0, Total Frame : 43701\n",
      "Train Episode :13, Train reward : -21.0, Total Frame : 47349\n",
      "Train Episode :14, Train reward : -19.0, Total Frame : 51517\n",
      "Train Episode :15, Train reward : -20.0, Total Frame : 55667\n",
      "Train Episode :16, Train reward : -20.0, Total Frame : 59749\n",
      "Train Episode :17, Train reward : -21.0, Total Frame : 63045\n",
      "Train Episode :18, Train reward : -21.0, Total Frame : 67069\n",
      "Train Episode :19, Train reward : -20.0, Total Frame : 70991\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "Train Episode :20, Train reward : -21.0, Total Frame : 74639\n",
      "Train Episode :21, Train reward : -21.0, Total Frame : 77815\n",
      "Train Episode :22, Train reward : -21.0, Total Frame : 81471\n",
      "Train Episode :23, Train reward : -20.0, Total Frame : 85430\n",
      "Train Episode :24, Train reward : -21.0, Total Frame : 89236\n",
      "Train Episode :25, Train reward : -19.0, Total Frame : 93383\n",
      "Train Episode :26, Train reward : -19.0, Total Frame : 97439\n",
      "Train Episode :27, Train reward : -20.0, Total Frame : 101220\n",
      "Train Episode :28, Train reward : -20.0, Total Frame : 104570\n",
      "Train Episode :29, Train reward : -20.0, Total Frame : 107937\n",
      "Total Step: 3304 \t Total Score: -21.0\n",
      "Train Episode :30, Train reward : -21.0, Total Frame : 111233\n",
      "Train Episode :31, Train reward : -21.0, Total Frame : 114641\n",
      "Train Episode :32, Train reward : -19.0, Total Frame : 118370\n",
      "Train Episode :33, Train reward : -21.0, Total Frame : 121666\n",
      "Train Episode :34, Train reward : -19.0, Total Frame : 126173\n",
      "Train Episode :35, Train reward : -21.0, Total Frame : 129469\n",
      "Train Episode :36, Train reward : -21.0, Total Frame : 133013\n",
      "Train Episode :37, Train reward : -21.0, Total Frame : 136309\n",
      "Train Episode :38, Train reward : -21.0, Total Frame : 139853\n",
      "Train Episode :39, Train reward : -21.0, Total Frame : 142981\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "Train Episode :40, Train reward : -20.0, Total Frame : 146466\n",
      "Train Episode :41, Train reward : -21.0, Total Frame : 149597\n",
      "Train Episode :42, Train reward : -19.0, Total Frame : 153345\n",
      "Train Episode :43, Train reward : -20.0, Total Frame : 157192\n",
      "Train Episode :44, Train reward : -20.0, Total Frame : 160925\n",
      "Train Episode :45, Train reward : -21.0, Total Frame : 164221\n",
      "Train Episode :46, Train reward : -21.0, Total Frame : 167629\n",
      "Train Episode :47, Train reward : -21.0, Total Frame : 171037\n",
      "Train Episode :48, Train reward : -21.0, Total Frame : 175413\n",
      "Train Episode :49, Train reward : -21.0, Total Frame : 178581\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "Train Episode :50, Train reward : -21.0, Total Frame : 182200\n",
      "Train Episode :51, Train reward : -20.0, Total Frame : 185790\n",
      "Train Episode :52, Train reward : -20.0, Total Frame : 189157\n",
      "Train Episode :53, Train reward : -21.0, Total Frame : 192453\n",
      "Train Episode :54, Train reward : -20.0, Total Frame : 196128\n",
      "Train Episode :55, Train reward : -21.0, Total Frame : 199926\n",
      "Train Episode :56, Train reward : -21.0, Total Frame : 203230\n",
      "Train Episode :57, Train reward : -21.0, Total Frame : 207118\n",
      "Train Episode :58, Train reward : -20.0, Total Frame : 210905\n",
      "Train Episode :59, Train reward : -21.0, Total Frame : 214915\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "Train Episode :60, Train reward : -20.0, Total Frame : 218693\n",
      "Train Episode :61, Train reward : -20.0, Total Frame : 222060\n",
      "Train Episode :62, Train reward : -20.0, Total Frame : 226454\n",
      "Train Episode :63, Train reward : -20.0, Total Frame : 229824\n",
      "Train Episode :64, Train reward : -20.0, Total Frame : 233679\n",
      "Train Episode :65, Train reward : -21.0, Total Frame : 236979\n",
      "Train Episode :66, Train reward : -21.0, Total Frame : 240765\n",
      "Train Episode :67, Train reward : -21.0, Total Frame : 244413\n",
      "Train Episode :68, Train reward : -20.0, Total Frame : 247780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Episode :69, Train reward : -20.0, Total Frame : 251711\n",
      "Total Step: 3408 \t Total Score: -21.0\n",
      "Train Episode :70, Train reward : -20.0, Total Frame : 255386\n",
      "Train Episode :71, Train reward : -20.0, Total Frame : 259587\n",
      "Train Episode :72, Train reward : -20.0, Total Frame : 263407\n",
      "Train Episode :73, Train reward : -21.0, Total Frame : 267199\n",
      "Train Episode :74, Train reward : -20.0, Total Frame : 271109\n",
      "Train Episode :75, Train reward : -21.0, Total Frame : 274165\n",
      "Train Episode :76, Train reward : -21.0, Total Frame : 277461\n",
      "Train Episode :77, Train reward : -21.0, Total Frame : 280517\n",
      "Train Episode :78, Train reward : -21.0, Total Frame : 283685\n",
      "Train Episode :79, Train reward : -19.0, Total Frame : 287940\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "Train Episode :80, Train reward : -20.0, Total Frame : 291402\n",
      "Train Episode :81, Train reward : -21.0, Total Frame : 294458\n",
      "Train Episode :82, Train reward : -21.0, Total Frame : 297514\n",
      "Train Episode :83, Train reward : -21.0, Total Frame : 300906\n",
      "Train Episode :84, Train reward : -19.0, Total Frame : 305059\n",
      "Train Episode :85, Train reward : -21.0, Total Frame : 308115\n",
      "Train Episode :86, Train reward : -21.0, Total Frame : 311171\n",
      "Train Episode :87, Train reward : -20.0, Total Frame : 314650\n",
      "Train Episode :88, Train reward : -21.0, Total Frame : 317706\n",
      "Train Episode :89, Train reward : -21.0, Total Frame : 320762\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "Train Episode :90, Train reward : -20.0, Total Frame : 324437\n",
      "Train Episode :91, Train reward : -21.0, Total Frame : 327493\n",
      "Train Episode :92, Train reward : -20.0, Total Frame : 331084\n",
      "Train Episode :93, Train reward : -21.0, Total Frame : 334732\n",
      "Train Episode :94, Train reward : -21.0, Total Frame : 337788\n",
      "Train Episode :95, Train reward : -19.0, Total Frame : 341580\n",
      "Train Episode :96, Train reward : -21.0, Total Frame : 345063\n",
      "Train Episode :97, Train reward : -21.0, Total Frame : 348319\n",
      "Train Episode :98, Train reward : -21.0, Total Frame : 351375\n",
      "Train Episode :99, Train reward : -21.0, Total Frame : 354431\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "Train Episode :100, Train reward : -21.0, Total Frame : 357487\n",
      "Train Episode :101, Train reward : -21.0, Total Frame : 360618\n",
      "Train Episode :102, Train reward : -21.0, Total Frame : 363973\n",
      "Train Episode :103, Train reward : -20.0, Total Frame : 367692\n",
      "Train Episode :104, Train reward : -21.0, Total Frame : 371228\n",
      "Train Episode :105, Train reward : -21.0, Total Frame : 374471\n",
      "Train Episode :106, Train reward : -21.0, Total Frame : 377956\n",
      "Train Episode :107, Train reward : -21.0, Total Frame : 381855\n",
      "Train Episode :108, Train reward : -21.0, Total Frame : 385263\n",
      "Train Episode :109, Train reward : -21.0, Total Frame : 388559\n",
      "Total Step: 3296 \t Total Score: -21.0\n",
      "Train Episode :110, Train reward : -21.0, Total Frame : 392044\n",
      "Train Episode :111, Train reward : -21.0, Total Frame : 395100\n",
      "Train Episode :112, Train reward : -21.0, Total Frame : 398343\n",
      "Train Episode :113, Train reward : -21.0, Total Frame : 402111\n",
      "Train Episode :114, Train reward : -20.0, Total Frame : 405618\n",
      "Train Episode :115, Train reward : -21.0, Total Frame : 409398\n",
      "Train Episode :116, Train reward : -21.0, Total Frame : 412454\n",
      "Train Episode :117, Train reward : -21.0, Total Frame : 415510\n",
      "Train Episode :118, Train reward : -21.0, Total Frame : 418566\n",
      "Train Episode :119, Train reward : -21.0, Total Frame : 422228\n",
      "Total Step: 3350 \t Total Score: -20.0\n",
      "Train Episode :120, Train reward : -21.0, Total Frame : 425764\n",
      "Train Episode :121, Train reward : -21.0, Total Frame : 428820\n",
      "Train Episode :122, Train reward : -21.0, Total Frame : 432177\n",
      "Train Episode :123, Train reward : -21.0, Total Frame : 435473\n",
      "Train Episode :124, Train reward : -21.0, Total Frame : 438769\n",
      "Train Episode :125, Train reward : -21.0, Total Frame : 441825\n",
      "Train Episode :126, Train reward : -20.0, Total Frame : 445372\n",
      "Train Episode :127, Train reward : -21.0, Total Frame : 448668\n",
      "Train Episode :128, Train reward : -21.0, Total Frame : 452156\n",
      "Train Episode :129, Train reward : -20.0, Total Frame : 455506\n",
      "Total Step: 3350 \t Total Score: -20.0\n",
      "Train Episode :130, Train reward : -20.0, Total Frame : 459890\n",
      "Train Episode :131, Train reward : -21.0, Total Frame : 463306\n",
      "Train Episode :132, Train reward : -20.0, Total Frame : 466656\n",
      "Train Episode :133, Train reward : -20.0, Total Frame : 470526\n",
      "Train Episode :134, Train reward : -21.0, Total Frame : 473822\n",
      "Train Episode :135, Train reward : -21.0, Total Frame : 477470\n",
      "Train Episode :136, Train reward : -19.0, Total Frame : 481530\n",
      "Train Episode :137, Train reward : -21.0, Total Frame : 484834\n",
      "Train Episode :138, Train reward : -20.0, Total Frame : 488201\n",
      "Train Episode :139, Train reward : -20.0, Total Frame : 491808\n",
      "Total Step: 3304 \t Total Score: -21.0\n",
      "Train Episode :140, Train reward : -20.0, Total Frame : 495499\n",
      "Train Episode :141, Train reward : -21.0, Total Frame : 498803\n",
      "Train Episode :142, Train reward : -20.0, Total Frame : 502900\n",
      "Train Episode :143, Train reward : -20.0, Total Frame : 506267\n",
      "Train Episode :144, Train reward : -20.0, Total Frame : 509874\n",
      "Train Episode :145, Train reward : -20.0, Total Frame : 513488\n",
      "Train Episode :146, Train reward : -20.0, Total Frame : 517062\n",
      "Train Episode :147, Train reward : -20.0, Total Frame : 520669\n",
      "Train Episode :148, Train reward : -20.0, Total Frame : 524722\n",
      "Train Episode :149, Train reward : -21.0, Total Frame : 527778\n",
      "Total Step: 3350 \t Total Score: -20.0\n",
      "Train Episode :150, Train reward : -21.0, Total Frame : 530946\n",
      "Train Episode :151, Train reward : -21.0, Total Frame : 534242\n",
      "Train Episode :152, Train reward : -21.0, Total Frame : 537410\n",
      "Train Episode :153, Train reward : -21.0, Total Frame : 540706\n",
      "Train Episode :154, Train reward : -21.0, Total Frame : 543762\n",
      "Train Episode :155, Train reward : -21.0, Total Frame : 546818\n",
      "Train Episode :156, Train reward : -21.0, Total Frame : 549874\n",
      "Train Episode :157, Train reward : -21.0, Total Frame : 553418\n",
      "Train Episode :158, Train reward : -20.0, Total Frame : 556900\n",
      "Train Episode :159, Train reward : -21.0, Total Frame : 559956\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "Train Episode :160, Train reward : -21.0, Total Frame : 563012\n",
      "Train Episode :161, Train reward : -21.0, Total Frame : 566068\n",
      "Train Episode :162, Train reward : -21.0, Total Frame : 569124\n",
      "Train Episode :163, Train reward : -21.0, Total Frame : 572428\n",
      "Train Episode :164, Train reward : -21.0, Total Frame : 575484\n",
      "Train Episode :165, Train reward : -21.0, Total Frame : 578788\n",
      "Train Episode :166, Train reward : -19.0, Total Frame : 582968\n",
      "Train Episode :167, Train reward : -21.0, Total Frame : 586024\n",
      "Train Episode :168, Train reward : -21.0, Total Frame : 589080\n",
      "Train Episode :169, Train reward : -21.0, Total Frame : 592136\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "Train Episode :170, Train reward : -21.0, Total Frame : 595192\n",
      "Train Episode :171, Train reward : -21.0, Total Frame : 598248\n",
      "Train Episode :172, Train reward : -21.0, Total Frame : 601416\n",
      "Train Episode :173, Train reward : -21.0, Total Frame : 605072\n",
      "Train Episode :174, Train reward : -21.0, Total Frame : 608368\n",
      "Train Episode :175, Train reward : -21.0, Total Frame : 611424\n",
      "Train Episode :176, Train reward : -21.0, Total Frame : 614480\n",
      "Train Episode :177, Train reward : -21.0, Total Frame : 617536\n",
      "Train Episode :178, Train reward : -21.0, Total Frame : 620832\n",
      "Train Episode :179, Train reward : -21.0, Total Frame : 624128\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "Train Episode :180, Train reward : -21.0, Total Frame : 627184\n",
      "Train Episode :181, Train reward : -21.0, Total Frame : 630240\n",
      "Train Episode :182, Train reward : -21.0, Total Frame : 633540\n",
      "Train Episode :183, Train reward : -21.0, Total Frame : 637324\n",
      "Train Episode :184, Train reward : -20.0, Total Frame : 641713\n",
      "Train Episode :185, Train reward : -21.0, Total Frame : 645009\n",
      "Train Episode :186, Train reward : -20.0, Total Frame : 648359\n",
      "Train Episode :187, Train reward : -21.0, Total Frame : 651535\n",
      "Train Episode :188, Train reward : -20.0, Total Frame : 655382\n",
      "Train Episode :189, Train reward : -21.0, Total Frame : 658662\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "Train Episode :190, Train reward : -21.0, Total Frame : 661958\n",
      "Train Episode :191, Train reward : -21.0, Total Frame : 665982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Episode :192, Train reward : -21.0, Total Frame : 669278\n",
      "Train Episode :193, Train reward : -21.0, Total Frame : 672334\n",
      "Train Episode :194, Train reward : -21.0, Total Frame : 675630\n",
      "Train Episode :195, Train reward : -21.0, Total Frame : 678686\n",
      "Train Episode :196, Train reward : -21.0, Total Frame : 682057\n",
      "Train Episode :197, Train reward : -20.0, Total Frame : 685591\n",
      "Train Episode :198, Train reward : -20.0, Total Frame : 689424\n",
      "Train Episode :199, Train reward : -19.0, Total Frame : 693172\n",
      "Total Step: 3367 \t Total Score: -20.0\n",
      "Train Episode :200, Train reward : -19.0, Total Frame : 696918\n",
      "Train Episode :201, Train reward : -21.0, Total Frame : 700281\n",
      "Train Episode :202, Train reward : -21.0, Total Frame : 703337\n",
      "Train Episode :203, Train reward : -21.0, Total Frame : 706393\n",
      "Train Episode :204, Train reward : -21.0, Total Frame : 709449\n",
      "Train Episode :205, Train reward : -20.0, Total Frame : 712816\n",
      "Train Episode :206, Train reward : -21.0, Total Frame : 715872\n",
      "Train Episode :207, Train reward : -21.0, Total Frame : 718928\n",
      "Train Episode :208, Train reward : -21.0, Total Frame : 721984\n",
      "Train Episode :209, Train reward : -21.0, Total Frame : 725040\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "Train Episode :210, Train reward : -21.0, Total Frame : 728096\n",
      "Train Episode :211, Train reward : -21.0, Total Frame : 731152\n",
      "Train Episode :212, Train reward : -21.0, Total Frame : 734208\n",
      "Train Episode :213, Train reward : -21.0, Total Frame : 737264\n",
      "Train Episode :214, Train reward : -21.0, Total Frame : 740320\n",
      "Train Episode :215, Train reward : -21.0, Total Frame : 743968\n",
      "Train Episode :216, Train reward : -20.0, Total Frame : 747815\n",
      "Train Episode :217, Train reward : -21.0, Total Frame : 751111\n",
      "Train Episode :218, Train reward : -21.0, Total Frame : 754415\n",
      "Train Episode :219, Train reward : -21.0, Total Frame : 757583\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "Train Episode :220, Train reward : -21.0, Total Frame : 760759\n",
      "Train Episode :221, Train reward : -21.0, Total Frame : 764055\n",
      "Train Episode :222, Train reward : -21.0, Total Frame : 767186\n",
      "Train Episode :223, Train reward : -20.0, Total Frame : 770776\n",
      "Train Episode :224, Train reward : -21.0, Total Frame : 773944\n",
      "Train Episode :225, Train reward : -20.0, Total Frame : 777294\n",
      "Train Episode :226, Train reward : -20.0, Total Frame : 780644\n",
      "Train Episode :227, Train reward : -21.0, Total Frame : 783775\n",
      "Train Episode :228, Train reward : -21.0, Total Frame : 786943\n",
      "Train Episode :229, Train reward : -20.0, Total Frame : 790293\n",
      "Total Step: 3056 \t Total Score: -21.0\n",
      "Train Episode :230, Train reward : -21.0, Total Frame : 793829\n",
      "Train Episode :231, Train reward : -21.0, Total Frame : 797125\n",
      "Train Episode :232, Train reward : -21.0, Total Frame : 800533\n",
      "Train Episode :233, Train reward : -20.0, Total Frame : 804260\n",
      "Train Episode :234, Train reward : -20.0, Total Frame : 807963\n",
      "Train Episode :235, Train reward : -21.0, Total Frame : 811719\n",
      "Train Episode :236, Train reward : -21.0, Total Frame : 814962\n",
      "Train Episode :237, Train reward : -21.0, Total Frame : 818498\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c57119362d4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmax_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-0373a7ed9008>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, max_total_frame)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mepisode_frame\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_history\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0mmini_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_frame\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-0373a7ed9008>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, mini_batch)\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mq_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                     \u001b[0mq_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mq_targets_next\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_behavior_net_action\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# 예측치(pred)와 목표치(true)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_frame = 10000000\n",
    "fitter.train(max_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fitter.frame_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(fitter.behavior_net(fitter.frame_history.get_state()), dim=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = fitter.behavior_net(fitter.frame_history.get_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "S, a, r, S_next, done = fitter.replay_memory.sample(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = fitter.behavior_net(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_new = np.concatenate([S_next, S])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_q_values = fitter.target_net(S_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2933, -0.2924, -0.2957, -0.2936, -0.2954, -0.2942],\n",
       "        [-0.2935, -0.2925, -0.2961, -0.2940, -0.2957, -0.2946]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.2924, -0.2925], device='cuda:0', grad_fn=<MaxBackward0>),\n",
       " tensor([1, 1], device='cuda:0'))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(next_q_values, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2924, -0.2925], device='cuda:0', grad_fn=<MaxBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_q_values.max(dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2904], device='cuda:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_q_values.gather(1, torch.max(next_q_values, dim=1)[1].unsqueeze(1)).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(batch_size):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    next_state = Variable(torch.FloatTensor(np.float32(next_state)))\n",
    "    action     = Variable(torch.LongTensor(action))\n",
    "    reward     = Variable(torch.FloatTensor(reward))\n",
    "    done       = Variable(torch.FloatTensor(done))\n",
    "\n",
    "    q_values      = current_model(state)\n",
    "    next_q_values = target_model(next_state)\n",
    "\n",
    "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value     = next_q_values.max(1)[0]\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    loss = (q_value - expected_q_value.detach()).pow(2).mean()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
