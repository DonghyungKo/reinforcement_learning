{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import the gym module\n",
    "import gym\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=10):\n",
    "        '''첫 번째 트릭 No-Operation. 초기화 후 일정 단계에 이를때까지 아무 행동도 하지않고\n",
    "        게임 초기 상태를 다양하게 하여 특정 시작 상태만 학습하는 것을 방지한다'''\n",
    "\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = self.unwrapped.np_random.randint(\n",
    "                1, self.noop_max + 1)  # pylint: disable=E1101\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a breakout environment\n",
    "env = gym.make('BreakoutNoFrameskip-v4')\n",
    "env = NoopResetEnv(env, noop_max=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    img = cv2.resize(img, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "    return np.expand_dims(img, 0) # (1, 80, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "for _ in range(30):\n",
    "    img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    action = env.action_space.sample()\n",
    "    env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efd53779780>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "for _ in range(30):\n",
    "    s, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.imshow(preprocess(s)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random \n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        self.deque.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.deque, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameHistory(object):\n",
    "    # consecutive frame을 4개씩 병합하여 새로운 x를 하나씩 만드는 클래스\n",
    "    # 만들어진 x는 4개씩 concat하고 s를 만들어서 DQN의 input으로 들어감\n",
    "    def __init__(self, env, k=4):\n",
    "        self.state_deque = deque(maxlen=k) # [x1, x2, x3, x4]\n",
    "        self.frame_history_deque = deque(maxlen=k) #[s1, s2, s3, s4]\n",
    "        self.k = k\n",
    "        \n",
    "        # 최초에는 초기 화면(env.reset)으로 채워둠\n",
    "        self.env = env\n",
    "        self.initial_state = self.preprocess(self.env.reset())\n",
    "        self.reset()\n",
    "        \n",
    "    def preprocess(self, img):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        img = cv2.resize(img, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "        return np.expand_dims(img, 0) # (1, 80, 80)\n",
    "    \n",
    "    def append_frame(self, s):\n",
    "        # if full, aggregate and clear old frames, append new frame to history queue\n",
    "        self.frame_history_deque.append(self.preprocess(s))\n",
    "        \n",
    "        if len(self) == self.k:\n",
    "            self.state_deque.append(self.aggregate_frame())\n",
    "        return\n",
    "        \n",
    "    def aggregate_frame(self):\n",
    "        if len(self) < self.k:\n",
    "            raise ValueError('not enough frames in history, expected %s, but got %s'%(self.k, len(self)))\n",
    "        \n",
    "        # element-wise maximum to aggregate\n",
    "        frame_history = [self.frame_history_deque.popleft() for _ in range(self.k)]\n",
    "        return np.maximum.reduce(frame_history[-2:]) #마지막 2 프레임 사용 \n",
    "    \n",
    "    def get_state(self):\n",
    "        S = np.array([self.state_deque[i] for i in range(self.k)]) # S = [x1, x2, x3, x4], (4, 1, 80, 80)\n",
    "        S = np.swapaxes(S, 0, 1) # (1, 4, 80, 80)\n",
    "        return S\n",
    "    \n",
    "    def reset(self):\n",
    "        self.frame_history_deque = deque(maxlen=k)\n",
    "        self.state_deque = deque(maxlen=k)\n",
    "        \n",
    "        for _ in range(self.k):\n",
    "            self.state_deque.append(self.initial_state)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_history_deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_dim, n_action):\n",
    "        super(DQN, self).__init__()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, 32, (8, 8), stride=4), nn.ReLU(), # conv 1\n",
    "            nn.Conv2d(32, 64, (4, 4), stride=2), nn.ReLU(), # conv 2\n",
    "            nn.Conv2d(64, 64, (3, 3), stride=1), nn.ReLU() # conv 3\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*7*7, 512), nn.ReLU(), # hidden layer\n",
    "            nn.Linear(512, n_action)\n",
    "        )\n",
    "        \n",
    "    def to_tensor(self, img):\n",
    "        img = torch.tensor(img, dtype=torch.float32, device=self.device)#.cuda(non_blocking=True) # to tensor\n",
    "        img /= 255                                   # normalize into 0-1\n",
    "        while img.dim() < 4 :                        # 4-dim\n",
    "            img = img.unsqueeze(0)\n",
    "        return img\n",
    "        \n",
    "    def forward(self, frames):\n",
    "        frames = self.to_tensor(frames) \n",
    "        conved = self.conv(frames)\n",
    "        conved = conved.view(conved.size(0), -1)\n",
    "        output = self.fc(conved)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "def weight_init(m):\n",
    "    '''\n",
    "    Usage:\n",
    "        model = Model()\n",
    "        model.apply(weight_init)\n",
    "    '''\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        init.normal_(m.bias.data)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "k = 4 ## number of skipped frame\n",
    "#############\n",
    "\n",
    "behavior_net = DQN(in_dim=k, n_action=4)\n",
    "target_net = DQN(in_dim=k, n_action=4)\n",
    "\n",
    "behavior_net.to(behavior_net.device) # model to cuda\n",
    "target_net.to(target_net.device)     # model to cuda\n",
    "\n",
    "behavior_net.load_state_dict(torch.load('BreakoutDoubleDQN_state_dict_', map_location='cpu'))\n",
    "target_net.load_state_dict(torch.load('BreakoutDoubleDQN_state_dict_', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter():\n",
    "    def __init__(self, env, behavior_net, target_net, k=4, train_method='DQN'):\n",
    "        self.env = env\n",
    "        self.train_method = train_method\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        # networks\n",
    "        self.behavior_net = behavior_net\n",
    "        self.target_net = target_net\n",
    "        self.behavior_net.to(self.behavior_net.device) # model to cuda\n",
    "        self.target_net.to(self.target_net.device)     # model to cuda\n",
    "        \n",
    "        # train parameters\n",
    "        self.gamma = 0.99 #as written in paper\n",
    "        self.criterion = nn.SmoothL1Loss() # huber loss (error-clipping)\n",
    "        self.optim = torch.optim.RMSprop(self.behavior_net.parameters(), lr=0.00025, alpha=0.95, eps=0.01) #as written in paper in Rainbow\n",
    "        self.k = k\n",
    "        \n",
    "        # train details\n",
    "        self.total_frame = 0\n",
    "        self.total_episode = 0\n",
    "        \n",
    "        self.test_e = 0.05\n",
    "        self.batch_size = 32\n",
    "        self.max_replay = 250000 # not as written in paper\n",
    "        self.min_replay = 50000 # not as written in the paper\n",
    "        self.replay_memory = ReplayMemory(self.max_replay)\n",
    "        self.frame_history = FrameHistory(env=self.env, k=self.k)\n",
    "        \n",
    "        self.train_reward_ls = []\n",
    "        self.test_reward_ls = []\n",
    "    \n",
    "    # train-epsilon\n",
    "    @property\n",
    "    def train_e(self):\n",
    "        return np.max([1 - 9.0*1e-07*self.total_frame, 0.1])\n",
    "    \n",
    "    \n",
    "    def reset_episode(self):\n",
    "        # game(episode) begins\n",
    "        self.env.reset()  \n",
    "        self.frame_history.reset() #frame history reset\n",
    "        self.current_episode_frame = 0         # each episode의 frame 수 \n",
    "        self.current_episode_reward = 0        # each episode의 reward 합\n",
    "        self.current_life = 5\n",
    "    \n",
    "    def choose_action(self, S, e):\n",
    "        # at the beginning of an episode, do something\n",
    "        if self.current_episode_frame == 0:\n",
    "            a = 1\n",
    "        else:\n",
    "            # Choose an action by e-greedy\n",
    "            if np.random.rand(1) < e :\n",
    "                a = self.env.action_space.sample()\n",
    "            else:\n",
    "                q_behavior = self.behavior_net(S)\n",
    "                a = torch.argmax(q_behavior).item()\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def run_k_frames(self, e):\n",
    "        # choose action!\n",
    "        S = self.frame_history.get_state()\n",
    "        a = self.choose_action(S, e)\n",
    "        r_sum = 0\n",
    "        \n",
    "        # repeat the action k-times\n",
    "        for _ in range(self.k):\n",
    "            s_next, r, done, info = self.env.step(a)\n",
    "            self.frame_history.append_frame(s_next)\n",
    "            r_sum += self.clip_reward(r) # store clipped reward for future training replay(experience)\n",
    "            \n",
    "            # accumulate history\n",
    "            self.current_episode_frame += 1\n",
    "            self.current_episode_reward += r\n",
    "            \n",
    "            # if episode(game) ends, return done(True)\n",
    "            if done: \n",
    "                return done\n",
    "            \n",
    "        # concat last k-frames into a next state S_next\n",
    "        S_next = self.frame_history.get_state()\n",
    "        \n",
    "        # Terminal when lose life\n",
    "        if info['ale.lives'] < self.current_life:\n",
    "            self.current_life = info['ale.lives']\n",
    "            terminal = True\n",
    "            self.replay_memory.append(S, a, r_sum, S_next, terminal) # save replay(experience)\n",
    "        \n",
    "        else:\n",
    "            self.replay_memory.append(S, a, r_sum, S_next, done) # save replay(experience)\n",
    "            \n",
    "        return done\n",
    "\n",
    "    \n",
    "    def train(self, max_total_frame):\n",
    "        while self.total_frame < max_total_frame :\n",
    "            # episode starts!\n",
    "            self.reset_episode()\n",
    "            \n",
    "            done = False\n",
    "            while not done:\n",
    "                done = self.run_k_frames(e=self.train_e)\n",
    "                \n",
    "                # training when enough replay-memory, every k-frames\n",
    "                if len(self.replay_memory) > self.min_replay:\n",
    "                    self.train_batch(self.batch_size)   \n",
    "                    self.total_frame += self.k\n",
    "                    \n",
    "                    # update target_net every 10,000 updates\n",
    "                    if self.total_frame%(self.k*10000)== 0 :\n",
    "                        self.update_target()\n",
    "                \n",
    "            # single-episode(game) is now done\n",
    "            # if not enough memory, save more\n",
    "            if len(self.replay_memory) < self.min_replay:\n",
    "                print('Not enough replay yet, expected more than %s, but %s instead'%(self.min_replay, len(self.replay_memory)))\n",
    "            \n",
    "            # if enough memory, print train result\n",
    "            else:\n",
    "                print('Train Episode :%s, Total Frame : %s, Train reward : %s,'%(self.total_episode, self.total_frame, self.current_episode_reward))\n",
    "                self.total_episode += 1\n",
    "                self.train_reward_ls.append(self.current_episode_reward)\n",
    "            \n",
    "                # testing, every 10 episodes when enough replay\n",
    "                if self.total_episode%10 == 0:\n",
    "                    self.test()           \n",
    "\n",
    "    def train_batch(self, batch_size):\n",
    "        # get mini-batch from replay-memory\n",
    "        S, A, R, S_next, D = self.replay_memory.sample(batch_size)\n",
    "        A = self.to_tensor(A, dtype=torch.long)\n",
    "        R = self.to_tensor(R, dtype=torch.float32)\n",
    "        D = self.to_tensor(D, dtype=torch.float32)\n",
    "        \n",
    "        q_behaviors = self.behavior_net(S)                # Q-values for every possible actions\n",
    "        q_behavior = self.select_indices(q_behaviors, A) # select Q-value for given actions\n",
    "        \n",
    "        if self.train_method=='DQN':\n",
    "            q_targets_next = self.target_net(S_next)   # Q-values of every possible actions next state (targetDQN)\n",
    "            q_target_next = q_targets_next.max(1)[0]   # max Q-values of next state\n",
    "            q_target = R + self.gamma*q_target_next*(1-D) \n",
    "        \n",
    "        elif self.train_method=='DoubleDQN':\n",
    "            next_actions = torch.argmax(self.behavior_net(S_next), dim=1) # choose argmax behavior actions at S_next\n",
    "            \n",
    "            q_targets_next = self.target_net(S_next) # cal Q-values of every possible actions next state (targetDQN)\n",
    "            q_target_next = self.select_indices(q_targets_next, next_actions) # select Q-value for next behavior actions\n",
    "            q_target = R + self.gamma*q_target_next*(1-D)\n",
    "        \n",
    "        # update weights\n",
    "        self.optim.zero_grad()\n",
    "        loss = self.criterion(q_target, q_behavior)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return\n",
    "        \n",
    "    def to_tensor(self, x, dtype=torch.float):\n",
    "        return torch.tensor(x, dtype=dtype, device=self.device)\n",
    "    \n",
    "    def select_indices(self, tensor, indices, dim=1):\n",
    "        if type(indices) != torch.Tensor:\n",
    "            indices = self.to_tensor(indices, dtype=torch.long)\n",
    "        if indices.dim() < 2 :\n",
    "            indices = indices.unsqueeze(1)\n",
    "        return tensor.gather(dim, indices).squeeze(1)\n",
    "    \n",
    "    def clip_reward(self, r):\n",
    "        return np.sign(r)\n",
    "    \n",
    "    \n",
    "    def update_target(self):\n",
    "        print('Update Target')\n",
    "        self.target_net.load_state_dict(self.behavior_net.state_dict())\n",
    "        return \n",
    "    \n",
    "    def test(self):\n",
    "        self.reset_episode()\n",
    "        done=False\n",
    "        while not done:\n",
    "            done = self.run_k_frames(e=fitter.test_e) # e-greedy\n",
    "                \n",
    "        self.test_reward_ls.append(self.current_episode_reward)\n",
    "        print('※Test※ \\t Frames: %s \\t Score: %s'%(self.current_episode_frame, self.current_episode_reward))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = Fitter(env, behavior_net, target_net, k=k, train_method='DoubleDQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "※Test※ \t Frames: 3785 \t Score: 57.0\n"
     ]
    }
   ],
   "source": [
    "fitter.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(fitter):\n",
    "    fitter.reset_episode()\n",
    "    e = 0.02\n",
    "    env.render()\n",
    "    done=False\n",
    "    while not done:\n",
    "        fitter.run_k_frames(e)\n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "    env.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_game(fitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
