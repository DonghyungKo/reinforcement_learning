{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import the gym module\n",
    "import gym\n",
    "import numpy as np\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a breakout environment\n",
    "env = gym.make('BreakoutDeterministic-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "    img = np.mean(img, axis=2).astype(np.uint8) # to gray, uint8 for low memory\n",
    "    img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "    img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    a = env.action_space.sample()\n",
    "    s2, _, done, _ = env.step(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff35582d7b8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff35573eeb8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(preprocess(s2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random \n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "    \n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        self.deque.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.deque, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameSkipper(object):\n",
    "    # consecutive frame을 4개씩 병합하여 새로운 x를 하나씩 만드는 클래스\n",
    "    # 만들어진 x는 4개씩 concat하고 s를 만들어서 DQN의 input으로 들어감\n",
    "    def __init__(self, k_skip=4):\n",
    "        self.state_deque = deque(maxlen=k_skip) # [x1, x2, x3, x4]\n",
    "        self.frame_ls = []\n",
    "        self.k_skip = k_skip\n",
    "        \n",
    "        # 최초에는 초기 화면(env.reset)으로 채워둠\n",
    "        self.env = gym.make('BreakoutDeterministic-v4')\n",
    "        padding_state = preprocess(self.env.reset())\n",
    "        for _ in range(k_skip):\n",
    "            self.state_deque.append(padding_state)\n",
    "    \n",
    "    def preprocess(self, img):\n",
    "        # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "        img = np.mean(img, axis=2).astype(np.uint8) # to gray, uint8 for low memory\n",
    "        img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "        img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "        return img\n",
    "    \n",
    "    def aggregate_frame(self):\n",
    "        # element-wise maximum to aggregate\n",
    "        return np.maximum.reduce(self.frame_ls)\n",
    "    \n",
    "    def append_frame(self, s):\n",
    "        self.frame_ls.append(self.preprocess(s))\n",
    "        # 4개의 프레임을 skip한 후에는, 4개를 합쳐서(element-wise maximum) x를 만들고 저장\n",
    "        if len(self) == self.k_skip:\n",
    "            x = self.aggregate_frame()\n",
    "            self.state_deque.append(x)\n",
    "            self.frame_ls = []  # frame을 다시 처음부터 저장\n",
    "            \n",
    "    def get_state(self):\n",
    "        S = np.array([self.state_deque[i] for i in range(self.k_skip)]) # S = [x1, x2, x3, x4], (4, 1, 80, 80)\n",
    "        S = np.swapaxes(S, 0, 1) # (1, 4, 80, 80)\n",
    "        return S\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_dim, n_action):\n",
    "        super(DQN, self).__init__()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, 32, (8, 8), stride=4), nn.ReLU(), # conv 1\n",
    "            nn.Conv2d(32, 64, (4, 4), stride=2), nn.ReLU(), # conv 2\n",
    "            nn.Conv2d(64, 64, (3, 3), stride=1), nn.ReLU() # conv 3\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*6*6, 512), nn.ReLU(), # hidden layer\n",
    "            nn.Linear(512, n_action)\n",
    "        )\n",
    "        \n",
    "    def to_tensor(self, img):\n",
    "        img = torch.tensor(img, dtype=torch.float32, device=self.device).cuda(non_blocking=True) # to tensor\n",
    "        img /= 255                                   # normalize into 0-1\n",
    "        while img.dim() < 4 :                        # 4-dim\n",
    "            img = img.unsqueeze(0)\n",
    "        return img\n",
    "        \n",
    "    def forward(self, frames):\n",
    "        frames = self.to_tensor(frames) \n",
    "        conved = self.conv(frames)\n",
    "        conved = conved.view(conved.size(0), -1)\n",
    "        output = self.fc(conved)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2304, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behavior_net = DQN(in_dim=4, n_action=4)\n",
    "target_net = DQN(in_dim=4, n_action=4)\n",
    "\n",
    "behavior_net.to(behavior_net.device) # model to cuda\n",
    "target_net.to(target_net.device)     # model to cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_decay(n_total_frame):\n",
    "    return np.max([1 - 9.0*1e-07*n_total_frame, 0.1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grow_batch_size(current_memory):\n",
    "    # grow batch size from 32 to 128 as respect to memory size\n",
    "    memory_ratio = current_memory / max_memory\n",
    "    new_batch_size = int(32 + 96*memory_ratio)\n",
    "    return new_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter():\n",
    "    def __init__(self, env, behavior_net, target_net, train_method='DQN'):\n",
    "        self.env = env\n",
    "        self.train_method = train_method\n",
    "        \n",
    "        # networks\n",
    "        self.behavior_net = behavior_net\n",
    "        self.target_net = target_net\n",
    "        self.behavior_net.to(self.behavior_net.device) # model to cuda\n",
    "        self.target_net.to(self.target_net.device)     # model to cuda\n",
    "        \n",
    "        # train parameters\n",
    "        self.gamma = 0.99 #as written in paper\n",
    "        self.criterion = nn.SmoothL1Loss() # huber loss (error-clipping)\n",
    "        self.optim = torch.optim.RMSprop(self.behavior_net.parameters(), lr=0.00025) #as written in paper\n",
    "        \n",
    "        # train details\n",
    "        self.total_frame = 0\n",
    "        self.total_episode = 0\n",
    "        self.batch_size = 32\n",
    "        self.memory_size = 1000000 # as written in paper\n",
    "        self.min_replay = 50000 # as written in the paper\n",
    "        self.replay_memory = ReplayMemory(self.memory_size)\n",
    "        self.reward_ls = []\n",
    "    \n",
    "    def train(self, max_total_frame):\n",
    "        while self.total_frame < max_total_frame :\n",
    "            self.total_episode += 1\n",
    "            \n",
    "            frame_skipper = FrameSkipper(k_skip=4) #과거 k개의 frame을 하나의 state로 합쳐주는 클래스\n",
    "            s = env.reset()  # small s represents each frame\n",
    "            n_step = 0       # each episode의 step의 횟수 \n",
    "        \n",
    "            done = False\n",
    "            while not done:\n",
    "                self.total_frame += 1\n",
    "                n_step += 1\n",
    "                e = epsilon_decay(self.total_frame)\n",
    "\n",
    "                # start of every 4 consecutive frames, choose action!                \n",
    "                if n_step%4 == 1:\n",
    "                    # S = [x1, x2, x3, x4] for neural-network input\n",
    "                    # x represents feature(element-wise max) from every 4 consecutive frames(4*s)\n",
    "                    S = frame_skipper.get_state()\n",
    "                    self.S = S\n",
    "                    r_sum = 0    # sum of reward for the following 4 frames\n",
    "                    \n",
    "                    # if it is the first frame of the game, Do Something!\n",
    "                    if n_step == 1:\n",
    "                        a = random.choice([1,2,3])\n",
    "                    \n",
    "                    # else, Choose an action by e-greedy\n",
    "                    else:\n",
    "                        if np.random.rand(1) < e :\n",
    "                            a = env.action_space.sample()\n",
    "                        else:\n",
    "                            q_pred = self.behavior_net(S)\n",
    "                            a = torch.argmax(q_pred).item()\n",
    "                \n",
    "                # repeat the same action 4 times\n",
    "                s_next, r, done, _, = env.step(a)\n",
    "                frame_skipper.append_frame(s_next)\n",
    "                r_sum += r\n",
    "\n",
    "                # end of every 4 consecutive frames\n",
    "                # genenrate new x, and update S\n",
    "                if n_step%4 == 0 :\n",
    "                    S_new = frame_skipper.get_state()\n",
    "                    self.replay_memory.append(S, a, r_sum, S_new, done)\n",
    "                \n",
    "                # no training when not enough replay\n",
    "                if len(self.replay_memory) < self.min_replay:\n",
    "                    continue\n",
    "                else:\n",
    "                    # train\n",
    "                    batch_size = grow_batch_size(len(self.replay_memory))\n",
    "                    mini_batch = replay_memory.sample(batch_size)\n",
    "                    self.train_batch(mini_batch)                \n",
    "                # single-episode(game) is now done\n",
    "            \n",
    "            \n",
    "            # no updating target, no testing when not enough replay\n",
    "            if len(self.replay_memory) < self.min_replay:\n",
    "                continue\n",
    "\n",
    "            # update target, do test for every 10 episodes(games)\n",
    "            if self.total_episode%10 == 0:\n",
    "                print('total_frame: %s'%self.total_frame)\n",
    "                fitter.update_target()\n",
    "                reward = fitter.test()\n",
    "                self.reward_ls.append(reward)  \n",
    "                \n",
    "                \n",
    "    def train_batch(self, mini_batch):\n",
    "        batch_size = len(mini_batch)\n",
    "        S = np.array([tup[0] for tup in mini_batch]).squeeze(1) # State\n",
    "        A = np.array([tup[1] for tup in mini_batch]) # actions\n",
    "        R = np.array([tup[2] for tup in mini_batch]) # rewards\n",
    "        S_next = np.array([tup[3] for tup in mini_batch]).squeeze(1) # next_State\n",
    "        D = np.array([tup[4] for tup in mini_batch]) # dones\n",
    "        \n",
    "        q_targets = self.target_net(S) # Q-values of current state with targetDDQN\n",
    "        q_targets_next = self.target_net(S_next) # Q-values of next state from targetDDQN\n",
    "        \n",
    "        if self.train_method=='DQN':\n",
    "            for i in range(batch_size):\n",
    "                a, r, done = A[i], R[i], D[i]\n",
    "                if done:\n",
    "                    q_targets[i, a] = r\n",
    "                else:\n",
    "                    q_targets[i, a] = r + self.gamma*torch.max(q_targets_next[i])\n",
    "        \n",
    "        elif self.train_method=='DDQN':\n",
    "            next_behavior_net_actions = torch.argmax(self.behaviorDDQN(S_next), dim=1) # choose argmax actions from behaviorDDQN in S_next\n",
    "                    \n",
    "            for i in range(batch_size):\n",
    "                a, r, done = A[i], R[i], D[i]\n",
    "                next_behavior_net_action = next_behavior_net_actions[i].item() # choose argmax actions from behaviorDDQN in S_next\n",
    "                if done:\n",
    "                    q_targets[i, a] = r\n",
    "                else:\n",
    "                    q_targets[i, a] = r + self.gamma*q_targets_next[i, next_behavior_net_action]\n",
    "            \n",
    "        # 예측치(pred)와 목표치(true)\n",
    "        q_behaviors = self.behavior_net(S)\n",
    "        \n",
    "        self.optim.zero_grad()\n",
    "        loss = self.criterion(q_targets, q_behaviors)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return\n",
    "\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.behavior_net.state_dict())\n",
    "        return \n",
    "    \n",
    "    \n",
    "    def test(self):\n",
    "        frame_skipper = FrameSkipper()\n",
    "        initial_state = self.env.reset()  # save initial state for comparison\n",
    "        s = self.env.reset()\n",
    "        n_step = 0\n",
    "        r_sum = 0\n",
    "        \n",
    "        done=False\n",
    "        while not done:\n",
    "            # 초기 화면에서는 1,2,3 중 임의의 액션을 실행\n",
    "            # 게임이 시작된 후, 공을 놓치면, life가 깍이고 다시 초기화면으로 돌아옴 \n",
    "            if np.array_equal(s, initial_state):\n",
    "                a = random.choice([1, 2, 3])\n",
    "            else:\n",
    "                S = frame_skipper.get_state()\n",
    "                a = torch.argmax(self.behavior_net(S)).item()\n",
    "            \n",
    "            s, r, done, _ = self.env.step(a)\n",
    "            frame_skipper.append(s)\n",
    "            r_sum += r\n",
    "            n_step += 1\n",
    "            \n",
    "        print('Total Step: %s \\t Total Score: %s'%(n_step, r_sum))\n",
    "        return r_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = Fitter(env, behavior_net, target_net, train_method='DQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a breakout environment\n",
    "env = gym.make('BreakoutDeterministic-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_frame = 10000000\n",
    "fitter.train(max_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8c294d8390>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# how skip-frame works\n",
    "plt.imshow(fitter.S[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
