{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_snake\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('snake-v0')\n",
    "env.unit_gap = 0\n",
    "env.grid_size = (30, 30)\n",
    "env.reward_eat = 1\n",
    "env.reward_win = 10\n",
    "env.reward_lose = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6412fff7f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQsAAAD8CAYAAABgtYFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADs1JREFUeJzt3W+MXFd9xvHvg/OHlkRN0jiR6xglgFsRpGKilWspFaLQQuI3DlKpzAuwUCSjNpFAoi8MSCWV+gKqQiSkNtQoEQZRgltAsaq0xXVT8YokdmocO27IAilZbNnhXwhFCrXz64s5C4O93j3Z3ZlZW9+PNJp7z5w79zd3N4/PvfdMNlWFJC3kZZMuQNL5wbCQ1MWwkNTFsJDUxbCQ1MWwkNRlZGGR5JYkTyaZTrJjVPuRNB4ZxTyLJKuAbwJ/BMwAjwLvrKonln1nksZiVCOLjcB0VX27qn4O3A9sGdG+JI3BRSN637XAM0PrM8Dvnatzrk5x/YgqkTRwgO9X1erFbj6qsMgcbb9yvpNkO7AdgFcC+0dUiaSB8D9L2XxUpyEzwLqh9euAY8MdqmpnVU1V1RSLzjpJ4zKqsHgUWJ/khiSXAFuBPSPal6QxGMlpSFWdSnIn8G/AKuC+qjoyin1JGo9RXbOgqh4EHhzV+0saL2dwSupiWEjqYlhI6mJYSOpiWEjqYlhI6mJYSOpiWEjqYlhI6jKyGZy6sGWu7xU3/t2qC5MjC0ldDAtJXQwLSV0MC0ldDAtJXQwLSV0MC0ldDAtJXQwLSV0MC0ldDAtJXQwLSV0MC0ld/NapFuXFXz/3a/N8IVXnMUcWkroYFpK6GBaSuizpmkWSp4HngdPAqaqaSnIV8EXgeuBp4E+q6kdLK1PSpC3HyOIPqmpDVU219R3AvqpaD+xr65LOc6M4DdkC7GrLu4DbRrAPSWO21FunBXw1SQF/X1U7gWur6jhAVR1Pcs1Si9TKk/+ddAUat6WGxc1VdawFwt4k/927YZLtwHYAXrnEKiSN3JJOQ6rqWHs+CXwF2AicSLIGoD2fPMe2O6tqqqqmWL2UKiSNw6LDIskrklw+uwy8FTgM7AG2tW7bgAeWWqSkyVvKaci1wFcy+GszFwH/UFX/muRRYHeS24HvAu9YepmSJm3RYVFV3wZeP0f7D4C3LKUoSSuPMzgldTEsJHUxLCR1MSwkdTEsJHUxLCR1MSwkdTEsJHUxLCR1MSwkdTEsJHUxLCR1MSwkdTEsJHUxLCR1MSwkdfEPI0ujkjr3a3X+/floRxaSuhgWkroYFpK6GBaSuhgWkroYFpK6eOtUGpXz8PbofBxZSOpiWEjqYlhI6mJYSOqyYFgkuS/JySSHh9quSrI3yVPt+crWniSfTDKd5FCSm0ZZvKTx6RlZfAa45Yy2HcC+qloP7GvrALcC69tjO3DP8pQpadIWDIuq+hrwwzOatwC72vIu4Lah9s/WwNeBK5KsWa5iJU3OYq9ZXFtVxwHa8zWtfS3wzFC/mdZ2liTbk+xPsp9nF1mFpLFZ7gucc81CmfNL/VW1s6qmqmqK1ctchaRlt9iwODF7etGeT7b2GWDdUL/rgGOLL0/SSrHYsNgDbGvL24AHhtrf3e6KbAKemz1dkXR+W/C7IUm+ALwJuDrJDPAR4KPA7iS3A98F3tG6PwhsBqaBnwHvGUHNkiYgVfP8fwLHVcRUiv2TrkK6wIUDVTW12M2dwSmpi2EhqYthIamLYSGpi2EhqYthIamLYSGpi2EhqYthIamLYSGpi2EhqYthIamLYSGpi2EhqYthIamLYSGpi2EhqYthIamLYSGpi2EhqYthIamLYSGpi2EhqYthIamLYSGpi2EhqYthIanLgmGR5L4kJ5McHmq7K8n3khxsj81Dr30wyXSSJ5O8bVSFSxqvnpHFZ4Bb5mi/u6o2tMeDAEluBLYCr2vb/F2SVctVrKTJWTAsquprwA87328LcH9VvVBV3wGmgY1LqE/SCrGUaxZ3JjnUTlOubG1rgWeG+sy0trMk2Z5kf5L9PLuEKiSNxWLD4h7g1cAG4Djw8daeOfrWXG9QVTuraqqqpli9yCokjc2iwqKqTlTV6ap6Efg0vzzVmAHWDXW9Dji2tBIlrQSLCoska4ZW3w7M3inZA2xNcmmSG4D1wCNLK1HSSnDRQh2SfAF4E3B1khngI8CbkmxgcIrxNPBegKo6kmQ38ARwCrijqk6PpnRJ45SqOS8pjLeIqRT7J12FdIELB6pqarGbO4NTUhfDQlIXw0JSF8NCUhfDQlIXw0JSF8NCUhfDQlIXw0JSF8NCUhfDQlIXw0JSF8NCUhfDQlIXw0JSF8NCUhfDQlIXw0JSF8NCUhfDQlIXw0JSF8NCUhfDQlIXw0JSF8NCUhfDQlIXw0JSlwXDIsm6JA8lOZrkSJL3tfarkuxN8lR7vrK1J8knk0wnOZTkplF/CEmj1zOyOAV8oKpeC2wC7khyI7AD2FdV64F9bR3gVmB9e2wH7ln2qiWN3YJhUVXHq+qxtvw8cBRYC2wBdrVuu4Db2vIW4LM18HXgiiRrlr1ySWP1kq5ZJLkeeAPwMHBtVR2HQaAA17Rua4FnhjabaW2SzmPdYZHkMuBLwPur6ifzdZ2jreZ4v+1J9ifZz7O9VUialK6wSHIxg6D4fFV9uTWfmD29aM8nW/sMsG5o8+uAY2e+Z1XtrKqpqppi9WLLlzQuPXdDAtwLHK2qTwy9tAfY1pa3AQ8Mtb+73RXZBDw3e7oi6fx1UUefm4F3AY8nOdjaPgR8FNid5Hbgu8A72msPApuBaeBnwHuWtWJJE5Gqsy4njL+IqRT7J12FdIELB6pqarGbO4NTUhfDQlIXw0JSF8NCUhfDQlIXw0JSF8NCUhfDQlIXw0JSF8NCUhfDQlIXw0JSF8NCUhfDQlIXw0JSF8NCUhfDQlIXw0JSF8NCUhfDQlIXw0JSF8NCUhfDQlIXw0JSF8NCUhfDQlIXw0JSl56/or4uyUNJjiY5kuR9rf2uJN9LcrA9Ng9t88Ek00meTPK2UX4ASePR81fUTwEfqKrHklwOHEiyt712d1X9zXDnJDcCW4HXAb8F/HuS366q08tZuKTxWnBkUVXHq+qxtvw8cBRYO88mW4D7q+qFqvoOMA1sXI5iJU3OS7pmkeR64A3Aw63pziSHktyX5MrWthZ4ZmizGeYPF0nnge6wSHIZ8CXg/VX1E+Ae4NXABuA48PHZrnNsXnO83/Yk+5Ps59mXXLekMesKiyQXMwiKz1fVlwGq6kRVna6qF4FP88tTjRlg3dDm1wHHznzPqtpZVVNVNcXqpXwESePQczckwL3A0ar6xFD7mqFubwcOt+U9wNYklya5AVgPPLJ8JUuahJ67ITcD7wIeT3KwtX0IeGeSDQxOMZ4G3gtQVUeS7AaeYHAn5Q7vhEjnv1SddTlh/EVMpdg/6SqkC1w4UFVTi93cGZySuhgWkroYFpK6GBaSuhgWkroYFpK6GBaSuhgWkroYFpK6GBaSuhgWkroYFpK6GBaSuhgWkroYFpK6GBaSuhgWkroYFpK6GBaSuhgWkroYFpK6GBaSuhgWkroYFpK6GBaSuhgWkroYFpK6GBaSuiwYFklenuSRJN9IciTJX7b2G5I8nOSpJF9Mcklrv7StT7fXrx/tR5A0Dj0jixeAN1fV64ENwC1JNgEfA+6uqvXAj4DbW//bgR9V1WuAu1s/See5BcOiBn7aVi9ujwLeDPxTa98F3NaWt7R12utvSZJlq1jSRFzU0ynJKuAA8Brgb4FvAT+uqlOtywywti2vBZ4BqKpTSZ4DfhP4/hnvuR3Y3lZ/SvjBmX0m7GqsZz4rrR5YeTWttHp+Zykbd4VFVZ0GNiS5AvgK8Nq5urXnuUYRdVZD1U5g5+x6kv1VNdVTzzhYz/xWWj2w8mpaifUsZfuXdDekqn4M/CewCbgiyWzYXAcca8szwLpW3EXAbwA/XEqRkiav527I6jaiIMmvAX8IHAUeAv64ddsGPNCW97R12uv/UVVnjSwknV96TkPWALvadYuXAbur6p+TPAHcn+SvgP8C7m397wU+l2SawYhia2ctOxfuMlbWM7+VVg+svJouqHriP/qSejiDU1KXiYdFkluSPNlmfO6YUA1PJ3k8ycHZK8ZJrkqyt81Q3ZvkyhHXcF+Sk0kOD7XNWUMGPtmO2aEkN42pnruSfK8dp4NJNg+99sFWz5NJ3jaCetYleSjJ0TaT+H2tfSLHaJ56JnKMxjLTuqom9gBWMZiz8SrgEuAbwI0TqONp4Ooz2v4a2NGWdwAfG3ENbwRuAg4vVAOwGfgXBrepNwEPj6meu4A/n6Pvje1ndylwQ/uZrlrmetYAN7Xly4Fvtv1O5BjNU89EjlH7nJe15YuBh9vn3g1sbe2fAv60Lf8Z8Km2vBX44kL7mPTIYiMwXVXfrqqfA/czmAG6EgzPRB2eoToSVfU1zr7FfK4atgCfrYGvM7iNvWYM9ZzLFuD+qnqhqr4DTDP42S5nPcer6rG2/DyDO3JrmdAxmqeecxnpMWqfc6QzrScdFr+Y7dkMzwQdpwK+muRAm1kKcG1VHYfBLwZwzQTqOlcNkzxud7Zh/X1Dp2ZjracNmd/A4F/PiR+jM+qBCR2jJKuSHAROAnt5CTOtgdmZ1uc06bDomu05BjdX1U3ArcAdSd44gRpeikkdt3uAVzP4QuFx4OPjrifJZcCXgPdX1U/m6zqOmuaoZ2LHqKpOV9UGBpMkN7IMM62HTTosfjHbsxmeCTo2VXWsPZ9kMJ19I3Bidtjank+Ou655apjIcauqE+0X8kXg0/xyGD2WepJczOA/zM9X1Zdb88SO0Vz1TPoYtRpGMtN60mHxKLC+XbG9hMGFlj3jLCDJK5JcPrsMvBU4zK/ORB2eoTpO56phD/DudsV/E/Dc7FB8lM445387g+M0W8/WdoX9BmA98Mgy7zsMJvwdrapPDL00kWN0rnomdYwyjpnWy3mFeJFXcTczuJL8LeDDE9j/qxhcpf4GcGS2Bgbnb/uAp9rzVSOu4wsMhq3/xyD1bz9XDQyGkLPf/n0cmBpTPZ9r+zvUftnWDPX/cKvnSeDWEdTz+wyGyYeAg+2xeVLHaJ56JnKMgN9lMJP6EIOA+ouh3+9HGFxQ/Ufg0tb+8rY+3V5/1UL7cAanpC6TPg2RdJ4wLCR1MSwkdTEsJHUxLCR1MSwkdTEsJHUxLCR1+X/X/eAYTZ3D+AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, _, _, _ = env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6412f94d30>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocess(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    img = cv2.resize(img, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "    return np.expand_dims(img, 0) # (1, 80, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random \n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        self.deque.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.deque, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameHistory(object):\n",
    "    # consecutive frame을 4개씩 병합하여 새로운 x를 하나씩 만드는 클래스\n",
    "    # 만들어진 x는 4개씩 concat하고 s를 만들어서 DQN의 input으로 들어감\n",
    "    def __init__(self, env, k=4):\n",
    "        self.state_deque = deque(maxlen=k) # [x1, x2, x3, x4]\n",
    "        self.frame_history_deque = deque(maxlen=k) #[s1, s2, s3, s4]\n",
    "        self.k = k\n",
    "        \n",
    "        # 최초에는 초기 화면(env.reset)으로 채워둠\n",
    "        self.env = env\n",
    "        self.initial_state = self.preprocess(self.env.reset())\n",
    "        self.reset()\n",
    "        \n",
    "    def preprocess(self, img):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        img = cv2.resize(img, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "        return np.expand_dims(img, 0) # (1, 80, 80)\n",
    "    \n",
    "    def append_frame(self, s):\n",
    "        # if full, aggregate and clear old frames, append new frame to history queue\n",
    "        self.frame_history_deque.append(self.preprocess(s))\n",
    "        \n",
    "        if len(self) == self.k:\n",
    "            self.state_deque.append(self.aggregate_frame())\n",
    "        return\n",
    "        \n",
    "    def aggregate_frame(self):\n",
    "        if len(self) < self.k:\n",
    "            raise ValueError('not enough frames in history, expected %s, but got %s'%(self.k, len(self)))\n",
    "        \n",
    "        # element-wise maximum to aggregate\n",
    "        frame_history = [self.frame_history_deque.popleft() for _ in range(self.k)]\n",
    "        return np.maximum.reduce(frame_history[-2:]) #마지막 2 프레임 사용 \n",
    "    \n",
    "    def get_state(self):\n",
    "        S = np.array([self.state_deque[i] for i in range(self.k)]) # S = [x1, x2, x3, x4], (4, 1, 80, 80)\n",
    "        S = np.swapaxes(S, 0, 1) # (1, 4, 80, 80)\n",
    "        return S\n",
    "    \n",
    "    def reset(self):\n",
    "        self.frame_history_deque = deque(maxlen=k)\n",
    "        self.state_deque = deque(maxlen=k)\n",
    "        \n",
    "        for _ in range(self.k):\n",
    "            self.state_deque.append(self.initial_state)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_history_deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(nn.Module):\n",
    "    # Dueling DQN\n",
    "    def __init__(self, in_dim, n_action):\n",
    "        super(DDQN, self).__init__()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, 32, (8, 8), stride=4), nn.ReLU(), # conv 1\n",
    "            nn.Conv2d(32, 64, (4, 4), stride=2), nn.ReLU(), # conv 2\n",
    "            nn.Conv2d(64, 64, (3, 3), stride=1), nn.ReLU() # conv 3\n",
    "        )\n",
    "        # V(s) : value-stream\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(64*7*7, 512), nn.ReLU(), # hidden layer\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "        # A(s,a) : action-stream\n",
    "        self.action_stream = nn.Sequential(\n",
    "            nn.Linear(64*7*7, 512), nn.ReLU(),\n",
    "            nn.Linear(512, n_action)\n",
    "        )\n",
    "        \n",
    "    def prep_for_input(self, img):\n",
    "        img = torch.tensor(img, dtype=torch.float32, device=self.device).cuda(non_blocking=True) # to tensor\n",
    "        img /= 255                                   # normalize into 0-1\n",
    "        while img.dim() < 4 :                        # 4-dim\n",
    "            img = img.unsqueeze(0)\n",
    "        return img\n",
    "        \n",
    "    def forward(self, frames):\n",
    "        frames = self.prep_for_input(frames) \n",
    "        conved = self.conv(frames)\n",
    "        conved = conved.view(conved.size(0), -1)\n",
    "        \n",
    "        # key of Dueling DQN : Q(s,a_i) = V(S) + A(s,a_i) - mean(A)\n",
    "        value = self.value_stream(conved) # V(s), dim=(batch_size, 1)\n",
    "        action = self.action_stream(conved) # A(s,a), dim=(batch_size, n_action)\n",
    "        \n",
    "        output = value + (action - torch.mean(action, dim=1, keepdim=True)) # V(s) + (A(s,a_i) - mean(A)), dim=(batch_size, n_action)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DDQN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (value_stream): Sequential(\n",
       "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (action_stream): Sequential(\n",
       "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############\n",
    "k = 4 ## number of skipped frame\n",
    "#############\n",
    "\n",
    "model = DDQN(in_dim=k, n_action=4)\n",
    "model.to(model.device) # model to cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter():\n",
    "    def __init__(self, env, behavior_net, target_net, k=4, train_method='DQN'):\n",
    "        self.env = env\n",
    "        self.train_method = train_method\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        self.save_path = ('BreakoutDuelingDQN_state_dict')\n",
    "\n",
    "        # networks\n",
    "        self.behavior_net = behavior_net\n",
    "        self.target_net = target_net\n",
    "        self.behavior_net.to(self.behavior_net.device) # model to cuda\n",
    "        self.target_net.to(self.target_net.device)     # model to cuda\n",
    "        \n",
    "        # train parameters\n",
    "        self.gamma = 0.99 #as written in paper\n",
    "        self.criterion = nn.SmoothL1Loss() # huber loss (error-clipping)\n",
    "        self.optim = torch.optim.RMSprop(self.behavior_net.parameters(), lr=0.00025, alpha=0.95, eps=0.01) #as written in paper in Rainbow\n",
    "        self.k = k\n",
    "        \n",
    "        # train details\n",
    "        self.total_frame = 0\n",
    "        self.total_episode = 0\n",
    "        \n",
    "        self.test_e = 0.05\n",
    "        self.batch_size = 32\n",
    "        self.max_replay = 250000 # not as written in paper\n",
    "        self.min_replay = 50000 # not as written in the paper\n",
    "        self.replay_memory = ReplayMemory(self.max_replay)\n",
    "        self.frame_history = FrameHistory(env=self.env, k=self.k)\n",
    "        \n",
    "        self.train_reward_ls = []\n",
    "        self.test_reward_ls = []\n",
    "    \n",
    "    # train-epsilon\n",
    "    @property\n",
    "    def train_e(self):\n",
    "        return np.max([1 - 9.0*1e-07*self.total_frame, 0.1])\n",
    "    \n",
    "    \n",
    "    def reset_episode(self):\n",
    "        # game(episode) begins\n",
    "        self.env.reset()  \n",
    "        self.frame_history.reset() #frame history reset\n",
    "        self.current_episode_frame = 0         # each episode의 frame 수 \n",
    "        self.current_episode_reward = 0        # each episode의 reward 합\n",
    "        self.current_life = 5\n",
    "    \n",
    "    def choose_action(self, S, e):\n",
    "        # at the beginning of an episode, do something\n",
    "        if self.current_episode_frame == 0:\n",
    "            a = 1\n",
    "        else:\n",
    "            # Choose an action by e-greedy\n",
    "            if np.random.rand(1) < e :\n",
    "                a = self.env.action_space.sample()\n",
    "            else:\n",
    "                q_behavior = self.behavior_net(S)\n",
    "                a = torch.argmax(q_behavior).item()\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def run_k_frames(self, e):\n",
    "        # choose action!\n",
    "        S = self.frame_history.get_state()\n",
    "        a = self.choose_action(S, e)\n",
    "        r_sum = 0\n",
    "        \n",
    "        # repeat the action k-times\n",
    "        for _ in range(self.k):\n",
    "            s_next, r, done, info = self.env.step(a)\n",
    "            self.frame_history.append_frame(s_next)\n",
    "            r_sum += self.clip_reward(r) # store clipped reward for future training replay(experience)\n",
    "            \n",
    "            # accumulate history\n",
    "            self.current_episode_frame += 1\n",
    "            self.current_episode_reward += r\n",
    "            \n",
    "            # if episode(game) ends, return done(True)\n",
    "            if done: \n",
    "                return done\n",
    "            \n",
    "        # concat last k-frames into a next state S_next\n",
    "        S_next = self.frame_history.get_state()\n",
    "        \n",
    "        # Terminal when lose life\n",
    "        if info['ale.lives'] < self.current_life:\n",
    "            self.current_life = info['ale.lives']\n",
    "            terminal = True\n",
    "            self.replay_memory.append(S, a, r_sum, S_next, terminal) # save replay(experience)\n",
    "        \n",
    "        else:\n",
    "            self.replay_memory.append(S, a, r_sum, S_next, done) # save replay(experience)\n",
    "            \n",
    "        return done\n",
    "\n",
    "    \n",
    "    def train(self, max_total_frame):\n",
    "        while self.total_frame < max_total_frame :\n",
    "            # episode starts!\n",
    "            self.reset_episode()\n",
    "            \n",
    "            done = False\n",
    "            while not done:\n",
    "                done = self.run_k_frames(e=self.train_e)\n",
    "                \n",
    "                # training when enough replay-memory, every k-frames\n",
    "                if len(self.replay_memory) > self.min_replay:\n",
    "                    self.train_batch(self.batch_size)   \n",
    "                    self.total_frame += self.k\n",
    "                    \n",
    "                    # update target_net every 10,000 updates\n",
    "                    if self.total_frame%(self.k*10000)== 0 :\n",
    "                        self.update_target()\n",
    "                        self.save()\n",
    "                \n",
    "            # single-episode(game) is now done\n",
    "            # if not enough memory, save more\n",
    "            if len(self.replay_memory) < self.min_replay:\n",
    "                print('Not enough replay yet, expected more than %s, but %s instead'%(self.min_replay, len(self.replay_memory)))\n",
    "            \n",
    "            # if enough memory, print train result\n",
    "            else:\n",
    "                print('Train Episode :%s, Total Frame : %s, Train reward : %s,'%(self.total_episode, self.total_frame, self.current_episode_reward))\n",
    "                self.total_episode += 1\n",
    "                self.train_reward_ls.append(self.current_episode_reward)\n",
    "            \n",
    "                # testing, every 10 episodes when enough replay\n",
    "                if self.total_episode%10 == 0:\n",
    "                    self.test()           \n",
    "\n",
    "    def train_batch(self, batch_size):\n",
    "        # get mini-batch from replay-memory\n",
    "        S, A, R, S_next, D = self.replay_memory.sample(batch_size)\n",
    "        A = self.to_tensor(A, dtype=torch.long)\n",
    "        R = self.to_tensor(R, dtype=torch.float32)\n",
    "        D = self.to_tensor(D, dtype=torch.float32)\n",
    "        \n",
    "        q_behaviors = self.behavior_net(S)                # Q-values for every possible actions\n",
    "        q_behavior = self.select_indices(q_behaviors, A) # select Q-value for given actions\n",
    "        \n",
    "        if self.train_method=='DQN':\n",
    "            q_targets_next = self.target_net(S_next)   # Q-values of every possible actions next state (targetDQN)\n",
    "            q_target_next = q_targets_next.max(1)[0]   # max Q-values of next state\n",
    "            q_target = R + self.gamma*q_target_next*(1-D) \n",
    "        \n",
    "        elif self.train_method=='DoubleDQN':\n",
    "            next_actions = torch.argmax(self.behavior_net(S_next), dim=1) # choose argmax behavior actions at S_next\n",
    "            \n",
    "            q_targets_next = self.target_net(S_next) # cal Q-values of every possible actions next state (targetDQN)\n",
    "            q_target_next = self.select_indices(q_targets_next, next_actions) # select Q-value for next behavior actions\n",
    "            q_target = R + self.gamma*q_target_next*(1-D)\n",
    "        \n",
    "        # update weights\n",
    "        self.optim.zero_grad()\n",
    "        loss = self.criterion(q_target, q_behavior)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return\n",
    "        \n",
    "    def to_tensor(self, x, dtype=torch.float):\n",
    "        return torch.tensor(x, dtype=dtype, device=self.device)\n",
    "    \n",
    "    def select_indices(self, tensor, indices, dim=1):\n",
    "        if type(indices) != torch.Tensor:\n",
    "            indices = self.to_tensor(indices, dtype=torch.long)\n",
    "        if indices.dim() < 2 :\n",
    "            indices = indices.unsqueeze(1)\n",
    "        return tensor.gather(dim, indices).squeeze(1)\n",
    "    \n",
    "    def clip_reward(self, r):\n",
    "        return np.sign(r)\n",
    "    \n",
    "    \n",
    "    def update_target(self):\n",
    "        print('Update Target')\n",
    "        self.target_net.load_state_dict(self.behavior_net.state_dict())\n",
    "        return \n",
    "    \n",
    "    def test(self):\n",
    "        self.reset_episode()\n",
    "        done=False\n",
    "        while not done:\n",
    "            done = self.run_k_frames(e=fitter.test_e) # e-greedy\n",
    "                \n",
    "        self.test_reward_ls.append(self.current_episode_reward)\n",
    "        print('※Test※ \\t Frames: %s \\t Score: %s'%(self.current_episode_frame, self.current_episode_reward))\n",
    "        return\n",
    "     \n",
    "    def save(self):\n",
    "        torch.save(self.behavior_net.state_dict(), self.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = Fitter(env, actor_critic, k=k)\n",
    "#fitter.replay_memory = replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Episode :100, Total Frame : 2800, Train reward : -1,\n",
      "Train Episode :200, Total Frame : 5600, Train reward : -1,\n",
      "Train Episode :300, Total Frame : 8400, Train reward : -1,\n",
      "Train Episode :400, Total Frame : 11200, Train reward : 0,\n",
      "Train Episode :500, Total Frame : 14000, Train reward : -1,\n",
      "Train Episode :600, Total Frame : 16800, Train reward : -1,\n",
      "Train Episode :700, Total Frame : 19600, Train reward : -1,\n",
      "Train Episode :800, Total Frame : 22400, Train reward : -1,\n",
      "Train Episode :900, Total Frame : 25200, Train reward : -1,\n",
      "Train Episode :1000, Total Frame : 28000, Train reward : -1,\n",
      "※Test※ \t Frames: 27 \t Score: -1\n",
      "Train Episode :1100, Total Frame : 30800, Train reward : -1,\n",
      "Train Episode :1200, Total Frame : 33600, Train reward : -1,\n",
      "Train Episode :1300, Total Frame : 36400, Train reward : -1,\n",
      "Train Episode :1400, Total Frame : 39200, Train reward : -1,\n",
      "Train Episode :1500, Total Frame : 42000, Train reward : -1,\n",
      "Train Episode :1600, Total Frame : 44800, Train reward : -1,\n",
      "Train Episode :1700, Total Frame : 47600, Train reward : -1,\n",
      "Train Episode :1800, Total Frame : 50684, Train reward : -1,\n",
      "Train Episode :1900, Total Frame : 53544, Train reward : -1,\n",
      "Train Episode :2000, Total Frame : 56444, Train reward : -1,\n",
      "※Test※ \t Frames: 26 \t Score: -1\n",
      "Train Episode :2100, Total Frame : 59328, Train reward : -1,\n",
      "Train Episode :2200, Total Frame : 62128, Train reward : -1,\n",
      "Train Episode :2300, Total Frame : 64928, Train reward : 0,\n",
      "Train Episode :2400, Total Frame : 67728, Train reward : -1,\n",
      "Train Episode :2500, Total Frame : 70528, Train reward : -1,\n",
      "Train Episode :2600, Total Frame : 73736, Train reward : -1,\n",
      "Train Episode :2700, Total Frame : 76536, Train reward : -1,\n",
      "Train Episode :2800, Total Frame : 79336, Train reward : -1,\n",
      "Train Episode :2900, Total Frame : 82192, Train reward : -1,\n",
      "Train Episode :3000, Total Frame : 85032, Train reward : -1,\n",
      "※Test※ \t Frames: 27 \t Score: -1\n",
      "Train Episode :3100, Total Frame : 87832, Train reward : 0,\n",
      "Train Episode :3200, Total Frame : 90632, Train reward : -1,\n",
      "Train Episode :3300, Total Frame : 93432, Train reward : -1,\n",
      "Train Episode :3400, Total Frame : 96232, Train reward : -1,\n",
      "Train Episode :3500, Total Frame : 99032, Train reward : -1,\n",
      "Train Episode :3600, Total Frame : 101832, Train reward : -1,\n",
      "Train Episode :3700, Total Frame : 104632, Train reward : -1,\n",
      "Train Episode :3800, Total Frame : 107432, Train reward : -1,\n",
      "Train Episode :3900, Total Frame : 110232, Train reward : -1,\n",
      "Train Episode :4000, Total Frame : 113032, Train reward : -1,\n",
      "※Test※ \t Frames: 27 \t Score: -1\n",
      "Train Episode :4100, Total Frame : 115832, Train reward : -1,\n",
      "Train Episode :4200, Total Frame : 118632, Train reward : -1,\n",
      "Train Episode :4300, Total Frame : 121432, Train reward : -1,\n",
      "Train Episode :4400, Total Frame : 124232, Train reward : -1,\n",
      "Train Episode :4500, Total Frame : 127032, Train reward : -1,\n",
      "Train Episode :4600, Total Frame : 129832, Train reward : -1,\n",
      "Train Episode :4700, Total Frame : 132632, Train reward : -1,\n",
      "Train Episode :4800, Total Frame : 135432, Train reward : -1,\n",
      "Train Episode :4900, Total Frame : 138232, Train reward : -1,\n",
      "Train Episode :5000, Total Frame : 141032, Train reward : -1,\n",
      "※Test※ \t Frames: 27 \t Score: -1\n",
      "Train Episode :5100, Total Frame : 143832, Train reward : -1,\n",
      "Train Episode :5200, Total Frame : 146632, Train reward : -1,\n",
      "Train Episode :5300, Total Frame : 149432, Train reward : -1,\n",
      "Train Episode :5400, Total Frame : 152232, Train reward : -1,\n",
      "Train Episode :5500, Total Frame : 155156, Train reward : -1,\n",
      "Train Episode :5600, Total Frame : 157956, Train reward : -1,\n",
      "Train Episode :5700, Total Frame : 160756, Train reward : -1,\n",
      "Train Episode :5800, Total Frame : 163556, Train reward : -1,\n",
      "Train Episode :5900, Total Frame : 166356, Train reward : -1,\n",
      "Train Episode :6000, Total Frame : 169260, Train reward : -1,\n",
      "※Test※ \t Frames: 27 \t Score: -1\n",
      "Train Episode :6100, Total Frame : 172012, Train reward : -1,\n",
      "Train Episode :6200, Total Frame : 174812, Train reward : -1,\n",
      "Train Episode :6300, Total Frame : 177612, Train reward : -1,\n",
      "Train Episode :6400, Total Frame : 180412, Train reward : -1,\n",
      "Train Episode :6500, Total Frame : 183212, Train reward : -1,\n",
      "Train Episode :6600, Total Frame : 186012, Train reward : -1,\n",
      "Train Episode :6700, Total Frame : 188812, Train reward : -1,\n",
      "Train Episode :6800, Total Frame : 191612, Train reward : -1,\n",
      "Train Episode :6900, Total Frame : 194412, Train reward : -1,\n",
      "Train Episode :7000, Total Frame : 197212, Train reward : -1,\n",
      "※Test※ \t Frames: 27 \t Score: -1\n",
      "Train Episode :7100, Total Frame : 200012, Train reward : -1,\n",
      "Train Episode :7200, Total Frame : 202812, Train reward : -1,\n",
      "Train Episode :7300, Total Frame : 205612, Train reward : -1,\n",
      "Train Episode :7400, Total Frame : 208412, Train reward : 0,\n",
      "Train Episode :7500, Total Frame : 211212, Train reward : -1,\n",
      "Train Episode :7600, Total Frame : 214012, Train reward : -1,\n",
      "Train Episode :7700, Total Frame : 216812, Train reward : -1,\n",
      "Train Episode :7800, Total Frame : 219612, Train reward : -1,\n",
      "Train Episode :7900, Total Frame : 222412, Train reward : -1,\n",
      "Train Episode :8000, Total Frame : 225212, Train reward : -1,\n",
      "※Test※ \t Frames: 27 \t Score: -1\n",
      "Train Episode :8100, Total Frame : 228012, Train reward : -1,\n",
      "Train Episode :8200, Total Frame : 230812, Train reward : -1,\n",
      "Train Episode :8300, Total Frame : 233612, Train reward : -1,\n",
      "Train Episode :8400, Total Frame : 236412, Train reward : -1,\n",
      "Train Episode :8500, Total Frame : 239212, Train reward : -1,\n",
      "Train Episode :8600, Total Frame : 242012, Train reward : -1,\n",
      "Train Episode :8700, Total Frame : 244812, Train reward : -1,\n",
      "Train Episode :8800, Total Frame : 247612, Train reward : -1,\n",
      "Train Episode :8900, Total Frame : 250412, Train reward : -1,\n",
      "Train Episode :9000, Total Frame : 253212, Train reward : -1,\n",
      "※Test※ \t Frames: 27 \t Score: -1\n",
      "Train Episode :9100, Total Frame : 256012, Train reward : -1,\n",
      "Train Episode :9200, Total Frame : 258812, Train reward : -1,\n",
      "Train Episode :9300, Total Frame : 261612, Train reward : -1,\n",
      "Train Episode :9400, Total Frame : 264412, Train reward : -1,\n",
      "Train Episode :9500, Total Frame : 267212, Train reward : -1,\n",
      "Train Episode :9600, Total Frame : 270012, Train reward : -1,\n",
      "Train Episode :9700, Total Frame : 272812, Train reward : -1,\n",
      "Train Episode :9800, Total Frame : 275612, Train reward : -1,\n",
      "Train Episode :9900, Total Frame : 278412, Train reward : -1,\n",
      "Train Episode :10000, Total Frame : 281212, Train reward : -1,\n",
      "※Test※ \t Frames: 27 \t Score: -1\n",
      "Train Episode :10100, Total Frame : 284012, Train reward : -1,\n",
      "Train Episode :10200, Total Frame : 286812, Train reward : -1,\n",
      "Train Episode :10300, Total Frame : 289612, Train reward : -1,\n",
      "Train Episode :10400, Total Frame : 292412, Train reward : -1,\n",
      "Train Episode :10500, Total Frame : 295212, Train reward : -1,\n",
      "Train Episode :10600, Total Frame : 298012, Train reward : -1,\n",
      "Train Episode :10700, Total Frame : 300812, Train reward : -1,\n",
      "Train Episode :10800, Total Frame : 303612, Train reward : -1,\n",
      "Train Episode :10900, Total Frame : 306412, Train reward : 0,\n",
      "Train Episode :11000, Total Frame : 309212, Train reward : -1,\n",
      "※Test※ \t Frames: 27 \t Score: -1\n",
      "Train Episode :11100, Total Frame : 312012, Train reward : -1,\n",
      "Train Episode :11200, Total Frame : 314812, Train reward : -1,\n",
      "Train Episode :11300, Total Frame : 317612, Train reward : -1,\n",
      "Train Episode :11400, Total Frame : 320412, Train reward : -1,\n",
      "Train Episode :11500, Total Frame : 323212, Train reward : -1,\n",
      "Train Episode :11600, Total Frame : 326012, Train reward : -1,\n",
      "Train Episode :11700, Total Frame : 328812, Train reward : -1,\n",
      "Train Episode :11800, Total Frame : 331612, Train reward : -1,\n",
      "Train Episode :11900, Total Frame : 334412, Train reward : -1,\n",
      "Train Episode :12000, Total Frame : 337212, Train reward : -1,\n",
      "※Test※ \t Frames: 27 \t Score: -1\n",
      "Train Episode :12100, Total Frame : 340012, Train reward : -1,\n",
      "Train Episode :12200, Total Frame : 342812, Train reward : -1,\n",
      "Train Episode :12300, Total Frame : 345612, Train reward : -1,\n",
      "Train Episode :12400, Total Frame : 348412, Train reward : -1,\n",
      "Train Episode :12500, Total Frame : 351212, Train reward : -1,\n",
      "Train Episode :12600, Total Frame : 354012, Train reward : -1,\n",
      "Train Episode :12700, Total Frame : 356812, Train reward : -1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Episode :12800, Total Frame : 359612, Train reward : -1,\n",
      "Train Episode :12900, Total Frame : 362412, Train reward : -1,\n",
      "Train Episode :13000, Total Frame : 365212, Train reward : -1,\n",
      "※Test※ \t Frames: 27 \t Score: -1\n",
      "Train Episode :13100, Total Frame : 368012, Train reward : -1,\n",
      "Train Episode :13200, Total Frame : 370812, Train reward : 0,\n",
      "Train Episode :13300, Total Frame : 373612, Train reward : -1,\n",
      "Train Episode :13400, Total Frame : 376412, Train reward : -1,\n",
      "Train Episode :13500, Total Frame : 379212, Train reward : -1,\n",
      "Train Episode :13600, Total Frame : 382012, Train reward : -1,\n",
      "Train Episode :13700, Total Frame : 384812, Train reward : -1,\n",
      "Train Episode :13800, Total Frame : 387612, Train reward : -1,\n",
      "Train Episode :13900, Total Frame : 390412, Train reward : -1,\n",
      "Train Episode :14000, Total Frame : 393212, Train reward : 0,\n",
      "※Test※ \t Frames: 27 \t Score: -1\n",
      "Train Episode :14100, Total Frame : 396012, Train reward : -1,\n",
      "Train Episode :14200, Total Frame : 398812, Train reward : -1,\n",
      "Train Episode :14300, Total Frame : 401612, Train reward : -1,\n",
      "Train Episode :14400, Total Frame : 404412, Train reward : -1,\n",
      "Train Episode :14500, Total Frame : 407212, Train reward : -1,\n",
      "Train Episode :14600, Total Frame : 410012, Train reward : -1,\n",
      "Train Episode :14700, Total Frame : 412812, Train reward : -1,\n",
      "Train Episode :14800, Total Frame : 415612, Train reward : 0,\n",
      "Train Episode :14900, Total Frame : 418412, Train reward : -1,\n",
      "Train Episode :15000, Total Frame : 421212, Train reward : -1,\n",
      "※Test※ \t Frames: 27 \t Score: -1\n",
      "Train Episode :15100, Total Frame : 424012, Train reward : -1,\n",
      "Train Episode :15200, Total Frame : 426812, Train reward : -1,\n",
      "Train Episode :15300, Total Frame : 429612, Train reward : -1,\n",
      "Train Episode :15400, Total Frame : 432412, Train reward : -1,\n",
      "Train Episode :15500, Total Frame : 435212, Train reward : -1,\n",
      "Train Episode :15600, Total Frame : 438012, Train reward : -1,\n",
      "Train Episode :15700, Total Frame : 440812, Train reward : -1,\n",
      "Train Episode :15800, Total Frame : 443612, Train reward : -1,\n",
      "Train Episode :15900, Total Frame : 446412, Train reward : -1,\n",
      "Train Episode :16000, Total Frame : 449212, Train reward : -1,\n",
      "※Test※ \t Frames: 27 \t Score: -1\n",
      "Train Episode :16100, Total Frame : 452012, Train reward : -1,\n",
      "Train Episode :16200, Total Frame : 454812, Train reward : -1,\n",
      "Train Episode :16300, Total Frame : 457612, Train reward : -1,\n",
      "Train Episode :16400, Total Frame : 460412, Train reward : -1,\n",
      "Train Episode :16500, Total Frame : 463212, Train reward : -1,\n",
      "Train Episode :16600, Total Frame : 466012, Train reward : -1,\n",
      "Train Episode :16700, Total Frame : 468812, Train reward : -1,\n",
      "Train Episode :16800, Total Frame : 471612, Train reward : -1,\n",
      "Train Episode :16900, Total Frame : 474412, Train reward : -1,\n",
      "Train Episode :17000, Total Frame : 477212, Train reward : -1,\n",
      "※Test※ \t Frames: 27 \t Score: -1\n",
      "Train Episode :17100, Total Frame : 480012, Train reward : -1,\n",
      "Train Episode :17200, Total Frame : 482812, Train reward : -1,\n",
      "Train Episode :17300, Total Frame : 485612, Train reward : -1,\n",
      "Train Episode :17400, Total Frame : 488412, Train reward : -1,\n",
      "Train Episode :17500, Total Frame : 491212, Train reward : -1,\n",
      "Train Episode :17600, Total Frame : 494012, Train reward : -1,\n",
      "Train Episode :17700, Total Frame : 496812, Train reward : -1,\n",
      "Train Episode :17800, Total Frame : 499612, Train reward : -1,\n",
      "Train Episode :17900, Total Frame : 502412, Train reward : -1,\n",
      "Train Episode :18000, Total Frame : 505212, Train reward : -1,\n",
      "※Test※ \t Frames: 27 \t Score: -1\n",
      "Train Episode :18100, Total Frame : 508012, Train reward : -1,\n",
      "Train Episode :18200, Total Frame : 510812, Train reward : 0,\n",
      "Train Episode :18300, Total Frame : 513612, Train reward : -1,\n",
      "Train Episode :18400, Total Frame : 516412, Train reward : -1,\n",
      "Train Episode :18500, Total Frame : 519212, Train reward : -1,\n",
      "Train Episode :18600, Total Frame : 522012, Train reward : -1,\n",
      "Train Episode :18700, Total Frame : 524812, Train reward : -1,\n",
      "Train Episode :18800, Total Frame : 527612, Train reward : -1,\n",
      "Train Episode :18900, Total Frame : 530412, Train reward : -1,\n",
      "Train Episode :19000, Total Frame : 533212, Train reward : -1,\n",
      "※Test※ \t Frames: 27 \t Score: -1\n",
      "Train Episode :19100, Total Frame : 536012, Train reward : -1,\n",
      "Train Episode :19200, Total Frame : 538812, Train reward : -1,\n",
      "Train Episode :19300, Total Frame : 541612, Train reward : -1,\n",
      "Train Episode :19400, Total Frame : 544412, Train reward : -1,\n",
      "Train Episode :19500, Total Frame : 547212, Train reward : -1,\n",
      "Train Episode :19600, Total Frame : 550012, Train reward : -1,\n",
      "Train Episode :19700, Total Frame : 552812, Train reward : -1,\n",
      "Train Episode :19800, Total Frame : 555612, Train reward : -1,\n",
      "Train Episode :19900, Total Frame : 558412, Train reward : -1,\n",
      "Train Episode :20000, Total Frame : 561212, Train reward : -1,\n",
      "※Test※ \t Frames: 27 \t Score: -1\n",
      "Train Episode :20100, Total Frame : 564012, Train reward : -1,\n",
      "Train Episode :20200, Total Frame : 566812, Train reward : -1,\n",
      "Train Episode :20300, Total Frame : 569612, Train reward : -1,\n",
      "Train Episode :20400, Total Frame : 572412, Train reward : -1,\n",
      "Train Episode :20500, Total Frame : 575212, Train reward : -1,\n",
      "Train Episode :20600, Total Frame : 578012, Train reward : -1,\n",
      "Train Episode :20700, Total Frame : 580812, Train reward : -1,\n",
      "Train Episode :20800, Total Frame : 583612, Train reward : -1,\n",
      "Train Episode :20900, Total Frame : 586412, Train reward : -1,\n",
      "Train Episode :21000, Total Frame : 589212, Train reward : -1,\n",
      "※Test※ \t Frames: 27 \t Score: -1\n",
      "Train Episode :21100, Total Frame : 592012, Train reward : -1,\n",
      "Train Episode :21200, Total Frame : 594812, Train reward : -1,\n",
      "Train Episode :21300, Total Frame : 597612, Train reward : -1,\n",
      "Train Episode :21400, Total Frame : 600412, Train reward : -1,\n"
     ]
    }
   ],
   "source": [
    "max_frame = 10000000\n",
    "fitter.train(max_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
