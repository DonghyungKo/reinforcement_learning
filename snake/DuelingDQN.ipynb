{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_snake\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('snake-v0')\n",
    "env.unit_gap = 0\n",
    "env.grid_size = (30, 30)\n",
    "env.reward_eat = 1\n",
    "env.reward_win = 10\n",
    "env.reward_lose = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f503afa47b8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, _, _, _ = env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f503af3bd30>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocess(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    img = cv2.resize(img, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "    return np.expand_dims(img, 0) # (1, 80, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random \n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        self.deque.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.deque, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameHistory(object):\n",
    "    # consecutive frame을 4개씩 병합하여 새로운 x를 하나씩 만드는 클래스\n",
    "    # 만들어진 x는 4개씩 concat하고 s를 만들어서 DQN의 input으로 들어감\n",
    "    def __init__(self, env, k=4):\n",
    "        self.state_deque = deque(maxlen=k) # [x1, x2, x3, x4]\n",
    "        self.frame_history_deque = deque(maxlen=k) #[s1, s2, s3, s4]\n",
    "        self.k = k\n",
    "        \n",
    "        # 최초에는 초기 화면(env.reset)으로 채워둠\n",
    "        self.env = env\n",
    "        self.initial_state = self.preprocess(self.env.reset())\n",
    "        self.reset()\n",
    "        \n",
    "    def preprocess(self, img):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        img = cv2.resize(img, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "        return np.expand_dims(img, 0) # (1, 80, 80)\n",
    "    \n",
    "    def append_frame(self, s):\n",
    "        # if full, aggregate and clear old frames, append new frame to history queue\n",
    "        self.frame_history_deque.append(self.preprocess(s))\n",
    "        \n",
    "        if len(self) == self.k:\n",
    "            self.state_deque.append(self.aggregate_frame())\n",
    "        return\n",
    "        \n",
    "    def aggregate_frame(self):\n",
    "        if len(self) < self.k:\n",
    "            raise ValueError('not enough frames in history, expected %s, but got %s'%(self.k, len(self)))\n",
    "        \n",
    "        # element-wise maximum to aggregate\n",
    "        frame_history = [self.frame_history_deque.popleft() for _ in range(self.k)]\n",
    "        return np.maximum.reduce(frame_history[-2:]) #마지막 2 프레임 사용 \n",
    "    \n",
    "    def get_state(self):\n",
    "        S = np.array([self.state_deque[i] for i in range(self.k)]) # S = [x1, x2, x3, x4], (4, 1, 80, 80)\n",
    "        S = np.swapaxes(S, 0, 1) # (1, 4, 80, 80)\n",
    "        return S\n",
    "    \n",
    "    def reset(self):\n",
    "        self.frame_history_deque = deque(maxlen=k)\n",
    "        self.state_deque = deque(maxlen=k)\n",
    "        \n",
    "        for _ in range(self.k):\n",
    "            self.state_deque.append(self.initial_state)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_history_deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(nn.Module):\n",
    "    # Dueling DQN\n",
    "    def __init__(self, in_dim, n_action):\n",
    "        super(DDQN, self).__init__()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, 32, (8, 8), stride=4), nn.ReLU(), # conv 1\n",
    "            nn.Conv2d(32, 64, (4, 4), stride=2), nn.ReLU(), # conv 2\n",
    "            nn.Conv2d(64, 64, (3, 3), stride=1), nn.ReLU() # conv 3\n",
    "        )\n",
    "        # V(s) : value-stream\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(64*7*7, 512), nn.ReLU(), # hidden layer\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "        # A(s,a) : action-stream\n",
    "        self.action_stream = nn.Sequential(\n",
    "            nn.Linear(64*7*7, 512), nn.ReLU(),\n",
    "            nn.Linear(512, n_action)\n",
    "        )\n",
    "        \n",
    "    def prep_for_input(self, img):\n",
    "        img = torch.tensor(img, dtype=torch.float32, device=self.device).cuda(non_blocking=True) # to tensor\n",
    "        img /= 255                                   # normalize into 0-1\n",
    "        while img.dim() < 4 :                        # 4-dim\n",
    "            img = img.unsqueeze(0)\n",
    "        return img\n",
    "        \n",
    "    def forward(self, frames):\n",
    "        frames = self.prep_for_input(frames) \n",
    "        conved = self.conv(frames)\n",
    "        conved = conved.view(conved.size(0), -1)\n",
    "        \n",
    "        # key of Dueling DQN : Q(s,a_i) = V(S) + A(s,a_i) - mean(A)\n",
    "        value = self.value_stream(conved) # V(s), dim=(batch_size, 1)\n",
    "        action = self.action_stream(conved) # A(s,a), dim=(batch_size, n_action)\n",
    "        \n",
    "        output = value + (action - torch.mean(action, dim=1, keepdim=True)) # V(s) + (A(s,a_i) - mean(A)), dim=(batch_size, n_action)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DDQN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (value_stream): Sequential(\n",
       "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (action_stream): Sequential(\n",
       "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############\n",
    "k = 1 ## number of skipped frame\n",
    "#############\n",
    "\n",
    "behavior_net = DDQN(in_dim=k, n_action=4)\n",
    "target_net = DDQN(in_dim=k, n_action=4)\n",
    "\n",
    "behavior_net.to(behavior_net.device) # model to cuda\n",
    "target_net.to(target_net.device)     # model to cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter():\n",
    "    def __init__(self, env, behavior_net, target_net, k=4, train_method='DQN'):\n",
    "        self.env = env\n",
    "        self.train_method = train_method\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        self.save_path = ('Snake_DDQN_weight')\n",
    "\n",
    "        # networks\n",
    "        self.behavior_net = behavior_net\n",
    "        self.target_net = target_net\n",
    "        self.behavior_net.to(self.behavior_net.device) # model to cuda\n",
    "        self.target_net.to(self.target_net.device)     # model to cuda\n",
    "        \n",
    "        # train parameters\n",
    "        self.gamma = 0.99 #as written in paper\n",
    "        self.criterion = nn.SmoothL1Loss() # huber loss (error-clipping)\n",
    "        self.optim = torch.optim.RMSprop(self.behavior_net.parameters(), lr=0.00025, alpha=0.95, eps=0.01) #as written in paper in Rainbow\n",
    "        self.k = k\n",
    "        \n",
    "        # train details\n",
    "        self.total_frame = 0\n",
    "        self.total_episode = 0\n",
    "        \n",
    "        self.test_e = 0.\n",
    "        self.batch_size = 32\n",
    "        self.max_replay = 250000 # not as written in paper\n",
    "        self.min_replay = 50000 # not as written in the paper\n",
    "        self.replay_memory = ReplayMemory(self.max_replay)\n",
    "        self.frame_history = FrameHistory(env=self.env, k=self.k)\n",
    "        \n",
    "        self.train_reward_ls = []\n",
    "        self.test_reward_ls = []\n",
    "    \n",
    "    # train-epsilon\n",
    "    @property\n",
    "    def train_e(self):\n",
    "        return np.max([1 - 9.0*1e-07*self.total_frame, 0.1])\n",
    "    \n",
    "    \n",
    "    def reset_episode(self):\n",
    "        # game(episode) begins\n",
    "        self.env.reset()  \n",
    "        self.frame_history.reset() #frame history reset\n",
    "        self.current_episode_frame = 0         # each episode의 frame 수 \n",
    "        self.current_episode_reward = 0        # each episode의 reward 합\n",
    "    \n",
    "    def choose_action(self, S, e):\n",
    "        # Choose an action by e-greedy\n",
    "        if np.random.rand(1) < e :\n",
    "            a = self.env.action_space.sample()\n",
    "        else:\n",
    "            q_behavior = self.behavior_net(S)\n",
    "            a = torch.argmax(q_behavior).item()\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def run_k_frames(self, e):\n",
    "        # choose action!\n",
    "        S = self.frame_history.get_state()\n",
    "        a = self.choose_action(S, e)\n",
    "        r_sum = 0\n",
    "        \n",
    "        # repeat the action k-times\n",
    "        for _ in range(self.k):\n",
    "            s_next, r, done, info = self.env.step(a)\n",
    "            self.frame_history.append_frame(s_next)\n",
    "            r_sum += r \n",
    "            \n",
    "            # accumulate history\n",
    "            self.current_episode_frame += 1\n",
    "            self.current_episode_reward += r\n",
    "            \n",
    "            # if episode(game) ends, return done(True)\n",
    "            if done: \n",
    "                return done\n",
    "            \n",
    "        # concat last k-frames into a next state S_next\n",
    "        S_next = self.frame_history.get_state()\n",
    "        \n",
    "        # save replay(experience)\n",
    "        self.replay_memory.append(S, a, r_sum, S_next, done) \n",
    "            \n",
    "        return done\n",
    "\n",
    "    \n",
    "    def train(self, max_total_frame):\n",
    "        while self.total_frame < max_total_frame :\n",
    "            # episode starts!\n",
    "            self.reset_episode()\n",
    "            \n",
    "            done = False\n",
    "            while not done:\n",
    "                done = self.run_k_frames(e=self.train_e)\n",
    "                \n",
    "                # training when enough replay-memory, every k-frames\n",
    "                if len(self.replay_memory) > self.min_replay:\n",
    "                    self.train_batch(self.batch_size)   \n",
    "                    self.total_frame += self.k\n",
    "                    \n",
    "                    # update target_net every 10,000 updates\n",
    "                    if self.total_frame%(self.k*10000)== 0 :\n",
    "                        self.update_target()\n",
    "                        self.save()\n",
    "                \n",
    "            # single-episode(game) is now done\n",
    "            # if not enough memory, save more\n",
    "            if len(self.replay_memory) < self.min_replay:\n",
    "                print('Not enough replay yet, expected more than %s, but %s instead'%(self.min_replay, len(self.replay_memory)))\n",
    "            \n",
    "            # if enough memory, print train result\n",
    "            else:\n",
    "                self.total_episode += 1\n",
    "                \n",
    "                if self.total_episode%100 == 0:\n",
    "                    print('Train Episode :%s, Total Frame : %s, Train reward : %s,'%(self.total_episode, self.total_frame, self.current_episode_reward))\n",
    "                    self.train_reward_ls.append(self.current_episode_reward)\n",
    "            \n",
    "                # testing, every 10 episodes when enough replay\n",
    "                if self.total_episode%1000 == 0:\n",
    "                    self.test()           \n",
    "\n",
    "    def train_batch(self, batch_size):\n",
    "        # get mini-batch from replay-memory\n",
    "        S, A, R, S_next, D = self.replay_memory.sample(batch_size)\n",
    "        A = self.to_tensor(A, dtype=torch.long)\n",
    "        R = self.to_tensor(R, dtype=torch.float32)\n",
    "        D = self.to_tensor(D, dtype=torch.float32)\n",
    "        \n",
    "        q_behaviors = self.behavior_net(S)                # Q-values for every possible actions\n",
    "        q_behavior = self.select_indices(q_behaviors, A) # select Q-value for given actions\n",
    "        \n",
    "        if self.train_method=='DQN':\n",
    "            q_targets_next = self.target_net(S_next)   # Q-values of every possible actions next state (targetDQN)\n",
    "            q_target_next = q_targets_next.max(1)[0]   # max Q-values of next state\n",
    "            q_target = R + self.gamma*q_target_next*(1-D) \n",
    "        \n",
    "        elif self.train_method=='DoubleDQN':\n",
    "            next_actions = torch.argmax(self.behavior_net(S_next), dim=1) # choose argmax behavior actions at S_next\n",
    "            \n",
    "            q_targets_next = self.target_net(S_next) # cal Q-values of every possible actions next state (targetDQN)\n",
    "            q_target_next = self.select_indices(q_targets_next, next_actions) # select Q-value for next behavior actions\n",
    "            q_target = R + self.gamma*q_target_next*(1-D)\n",
    "        \n",
    "        # update weights\n",
    "        self.optim.zero_grad()\n",
    "        loss = self.criterion(q_target, q_behavior)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return\n",
    "        \n",
    "    def to_tensor(self, x, dtype=torch.float):\n",
    "        return torch.tensor(x, dtype=dtype, device=self.device)\n",
    "    \n",
    "    def select_indices(self, tensor, indices, dim=1):\n",
    "        if type(indices) != torch.Tensor:\n",
    "            indices = self.to_tensor(indices, dtype=torch.long)\n",
    "        if indices.dim() < 2 :\n",
    "            indices = indices.unsqueeze(1)\n",
    "        return tensor.gather(dim, indices).squeeze(1)\n",
    "    \n",
    "    def clip_reward(self, r):\n",
    "        return np.sign(r)\n",
    "    \n",
    "    \n",
    "    def update_target(self):\n",
    "        print('Update Target')\n",
    "        self.target_net.load_state_dict(self.behavior_net.state_dict())\n",
    "        return \n",
    "    \n",
    "    def test(self):\n",
    "        self.reset_episode()\n",
    "        done=False\n",
    "        while not done:\n",
    "            done = self.run_k_frames(e=fitter.test_e) # e-greedy\n",
    "                \n",
    "        self.test_reward_ls.append(self.current_episode_reward)\n",
    "        print('※Test※ \\t Frames: %s \\t Score: %s'%(self.current_episode_frame, self.current_episode_reward))\n",
    "        return\n",
    "     \n",
    "    def save(self):\n",
    "        torch.save(self.behavior_net.state_dict(), self.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = Fitter(env, behavior_net, target_net, k=k, train_method='DoubleDQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update Target\n",
      "Train Episode :5100, Total Frame : 397006, Train reward : -1,\n",
      "Update Target\n",
      "Train Episode :5200, Total Frame : 406832, Train reward : -1,\n",
      "Update Target\n",
      "Train Episode :5300, Total Frame : 418059, Train reward : -1,\n",
      "Update Target\n",
      "Train Episode :5400, Total Frame : 427120, Train reward : -1,\n",
      "Update Target\n",
      "Train Episode :5500, Total Frame : 437012, Train reward : -1,\n",
      "Update Target\n",
      "Train Episode :5600, Total Frame : 448213, Train reward : -1,\n",
      "Update Target\n",
      "Train Episode :5700, Total Frame : 458075, Train reward : 0,\n",
      "Update Target\n",
      "Train Episode :5800, Total Frame : 469113, Train reward : -1,\n",
      "Update Target\n",
      "Update Target\n",
      "Train Episode :5900, Total Frame : 485573, Train reward : -1,\n",
      "Update Target\n"
     ]
    }
   ],
   "source": [
    "max_frame = 50000000\n",
    "fitter.train(max_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
