{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.envs.registration import register\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random \n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        if np.ndim(state) and np.ndim(next_state) == 1:\n",
    "            state = np.expand_dims(state, 0)\n",
    "            next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.deque.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.deque, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def pop_episode(self):\n",
    "        state, action, reward, next_state, done = zip(*[self.deque.popleft() for _ in range(len(self))])\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def reset(self):\n",
    "        [self.deque.pop() for _ in range(len(self))]\n",
    "        return\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ActorCritic, self).__init__()     \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.feature_stream = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        \n",
    "        self.actor_lin = nn.Linear(hidden_dim, output_dim)\n",
    "        self.critic_lin = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def to_tensor(self, x):\n",
    "        return torch.tensor(x, dtype=torch.float)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        if type(state) != torch.tensor:\n",
    "            state = self.to_tensor(state)\n",
    "        \n",
    "        if state.dim() == 1:\n",
    "            state.unsqueeze_(0)\n",
    "        \n",
    "        feature = self.feature_stream(state)\n",
    "        \n",
    "        action_prob = self.actor_lin(feature)\n",
    "        action_prob = torch.softmax(action_prob, dim=-1)\n",
    "        advantage = self.critic_lin(feature).squeeze(1) # dim=1\n",
    "        return action_prob, advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter():\n",
    "    def __init__(self, actor_critic):\n",
    "        self.actor_critic = actor_critic\n",
    "        self.gamma = 0.99\n",
    "        self.lr = 0.001\n",
    "        self.optim = torch.optim.Adam(self.actor_critic.parameters(), lr=self.lr)\n",
    "        self.replay_memory = ReplayMemory(capacity=100000)\n",
    "        \n",
    "    def run_episode(self, n_episode):\n",
    "        r_sum_ls = []\n",
    "        \n",
    "        for i in range(n_episode):\n",
    "            # reset episode\n",
    "            s = env.reset()\n",
    "            r_sum = 0\n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                a_prob, adv = self.actor_critic(s) # pi(a|s)\n",
    "                a_dist = torch.distributions.Categorical(a_prob) # pi(a|s)\n",
    "                a = a_dist.sample().item() # a\n",
    "                s_new, r, done, _ = env.step(a)\n",
    "                r_sum += r\n",
    "                \n",
    "                if r_sum > 10000:\n",
    "                    print('Done, No more training!')\n",
    "                    return\n",
    "                    \n",
    "                self.replay_memory.append(s, a, r, s_new, done)\n",
    "                s = s_new\n",
    "                \n",
    "                \n",
    "            loss = self.compute_loss()\n",
    "            self.train(loss)\n",
    "            \n",
    "            r_sum_ls.append(r_sum)\n",
    "            if i % 100 == 0:\n",
    "                print('Reward Sum = %.02f'%(np.mean(r_sum_ls)))\n",
    "                r_sum_ls = []\n",
    "        return\n",
    "    \n",
    "    def compute_loss(self):\n",
    "        # get mini-batch from replay-memory\n",
    "        S, A, R, S_next, D = self.replay_memory.pop_episode()\n",
    "        A = torch.tensor(A, dtype=torch.long)\n",
    "        R = torch.tensor(R, dtype=torch.float)\n",
    "        \n",
    "        # compute loss and gradient descent\n",
    "        a_prob, v = actor_critic(S) # pi(a_t|s_t)\n",
    "        a_dist = torch.distributions.Categorical(a_prob) # pi(a_t|s_t)\n",
    "        log_a_prob = a_dist.log_prob(A) # A : true actions\n",
    "        _, v_next = actor_critic(S_next)\n",
    "        \n",
    "        TD_error = R + self.gamma*v_next - v\n",
    "        actor_loss = torch.mean(-log_a_prob*TD_error.data)\n",
    "        critic_loss = torch.mean(TD_error*TD_error.data)\n",
    "        loss = actor_loss + critic_loss\n",
    "        return loss\n",
    "        \n",
    "    def train(self, loss):\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 4\n",
    "hidden_dim = 128\n",
    "out_dim = env.action_space.n\n",
    "\n",
    "actor_critic = ActorCritic(input_dim, hidden_dim, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = Fitter(actor_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Sum = 20.00\n",
      "Reward Sum = 30.39\n",
      "Reward Sum = 47.31\n",
      "Reward Sum = 61.37\n",
      "Reward Sum = 134.87\n",
      "Reward Sum = 145.64\n",
      "Reward Sum = 193.74\n",
      "Reward Sum = 133.22\n",
      "Reward Sum = 88.26\n",
      "Reward Sum = 74.99\n",
      "Reward Sum = 71.77\n",
      "Reward Sum = 65.80\n",
      "Reward Sum = 67.36\n",
      "Reward Sum = 67.80\n",
      "Reward Sum = 63.94\n",
      "Reward Sum = 73.69\n",
      "Reward Sum = 69.79\n",
      "Reward Sum = 76.21\n",
      "Reward Sum = 73.93\n",
      "Reward Sum = 76.96\n",
      "Reward Sum = 82.47\n",
      "Reward Sum = 82.57\n",
      "Reward Sum = 77.90\n",
      "Reward Sum = 75.64\n",
      "Reward Sum = 79.32\n",
      "Reward Sum = 91.99\n",
      "Reward Sum = 72.03\n",
      "Reward Sum = 78.56\n",
      "Reward Sum = 64.42\n",
      "Reward Sum = 82.66\n",
      "Reward Sum = 57.89\n",
      "Reward Sum = 75.38\n",
      "Reward Sum = 74.88\n",
      "Reward Sum = 52.86\n",
      "Reward Sum = 57.71\n",
      "Reward Sum = 65.59\n",
      "Reward Sum = 66.36\n",
      "Reward Sum = 60.53\n",
      "Reward Sum = 52.50\n",
      "Reward Sum = 48.70\n",
      "Reward Sum = 44.36\n",
      "Reward Sum = 39.80\n",
      "Reward Sum = 39.58\n",
      "Reward Sum = 34.11\n",
      "Reward Sum = 50.28\n",
      "Reward Sum = 62.98\n",
      "Reward Sum = 62.11\n",
      "Reward Sum = 75.83\n",
      "Reward Sum = 107.37\n",
      "Reward Sum = 92.62\n",
      "Reward Sum = 153.43\n",
      "Reward Sum = 194.17\n",
      "Reward Sum = 183.69\n",
      "Reward Sum = 178.54\n",
      "Reward Sum = 226.54\n",
      "Reward Sum = 228.03\n",
      "Reward Sum = 254.61\n",
      "Reward Sum = 177.12\n",
      "Reward Sum = 190.01\n",
      "Reward Sum = 92.60\n",
      "Reward Sum = 40.19\n",
      "Reward Sum = 9.25\n",
      "Reward Sum = 9.21\n",
      "Reward Sum = 9.35\n",
      "Reward Sum = 9.40\n",
      "Reward Sum = 9.36\n",
      "Reward Sum = 9.36\n",
      "Reward Sum = 9.44\n",
      "Reward Sum = 9.43\n",
      "Reward Sum = 9.40\n",
      "Reward Sum = 9.49\n",
      "Reward Sum = 9.26\n",
      "Reward Sum = 9.21\n",
      "Reward Sum = 9.34\n",
      "Reward Sum = 9.34\n",
      "Reward Sum = 9.29\n",
      "Reward Sum = 9.47\n",
      "Reward Sum = 9.39\n",
      "Reward Sum = 9.36\n",
      "Reward Sum = 9.29\n",
      "Reward Sum = 9.44\n",
      "Reward Sum = 9.35\n",
      "Reward Sum = 9.32\n",
      "Reward Sum = 9.28\n",
      "Reward Sum = 9.37\n",
      "Reward Sum = 9.30\n",
      "Reward Sum = 34.37\n",
      "Reward Sum = 20.20\n",
      "Reward Sum = 9.39\n",
      "Reward Sum = 9.44\n",
      "Reward Sum = 9.21\n",
      "Reward Sum = 9.35\n",
      "Reward Sum = 9.32\n",
      "Reward Sum = 9.39\n",
      "Reward Sum = 9.32\n",
      "Reward Sum = 9.30\n",
      "Reward Sum = 9.47\n",
      "Reward Sum = 9.30\n",
      "Reward Sum = 9.43\n",
      "Reward Sum = 9.41\n"
     ]
    }
   ],
   "source": [
    "n_episode = 10000\n",
    "\n",
    "fitter.run_episode(n_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
