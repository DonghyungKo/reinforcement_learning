{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.envs.registration import register\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random \n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        if np.ndim(state) and np.ndim(next_state) == 1:\n",
    "            state = np.expand_dims(state, 0)\n",
    "            next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.deque.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    #def sample(self, batch_size):\n",
    "    #    state, action, reward, next_state, done = zip(*random.sample(self.deque, batch_size))\n",
    "    #    return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def pop_episode(self):\n",
    "        state, action, reward, next_state, done = zip(*[self.deque.popleft() for _ in range(len(self))])\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def reset(self):\n",
    "        [self.deque.pop() for _ in range(len(self))]\n",
    "        return\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(PolicyNet, self).__init__()     \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def convert_to_tensor(self, x):\n",
    "        return torch.tensor(x, dtype=torch.float)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        if type(state) != torch.tensor:\n",
    "            state = self.convert_to_tensor(state)\n",
    "        \n",
    "        if state.dim() == 1:\n",
    "            state.unsqueeze_(0)\n",
    "        \n",
    "        out = self.lin(state)\n",
    "        softmax = torch.softmax(out, dim=1)\n",
    "        return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 4\n",
    "hidden_dim = 64\n",
    "out_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = PolicyNet(input_dim, hidden_dim, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter():\n",
    "    def __init__(self, policy_net):\n",
    "        self.gamma = 0.99\n",
    "        self.policy_net = policy_net\n",
    "        self.lr = 0.01\n",
    "        self.optim = torch.optim.Adam(self.policy_net.parameters(), lr=self.lr)\n",
    "        self.replay_memory = ReplayMemory(capacity=10000)    \n",
    "        \n",
    "    def run_episode(self):\n",
    "        # reset episode\n",
    "        s = env.reset()\n",
    "        self.replay_memory.reset()\n",
    "        self.r_sum = 0\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            a_prob = self.policy_net(s) # pi(a|s)\n",
    "            a_dist = torch.distributions.Categorical(a_prob) # pi(a|s)\n",
    "            a = a_dist.sample().item() # a\n",
    "            s_new, r, done, _ = env.step(a)\n",
    "            self.r_sum += r\n",
    "            \n",
    "            if done :\n",
    "                r = -10\n",
    "            \n",
    "            self.replay_memory.append(s, a, r, s_new, done)\n",
    "            s = s_new\n",
    "            \n",
    "            if self.r_sum > 10000:\n",
    "                print('very nice!')\n",
    "                return True\n",
    "            \n",
    "        print('Total reward : %s'%self.r_sum)\n",
    "        return\n",
    "    \n",
    "    def compute_loss(self):\n",
    "        # get mini-batch from replay-memory\n",
    "        S, A, R, S_next, D = self.replay_memory.pop_episode()\n",
    "        A = torch.tensor(A, dtype=torch.long)\n",
    "        \n",
    "        # calculate Gt\n",
    "        gt_ls = []\n",
    "        for i, r in enumerate(reversed(R)):\n",
    "            if i == 0:\n",
    "                gt = r\n",
    "            else:\n",
    "                gt = self.gamma*previous_gt + r\n",
    "            \n",
    "            gt_ls.insert(0, gt)\n",
    "            previous_gt = gt\n",
    "            \n",
    "        # normalize Gt to At\n",
    "        gt_mean = np.mean(gt_ls)\n",
    "        gt_std = np.std(gt_ls)\n",
    "        for i, gt in enumerate(gt_ls):\n",
    "            gt_ls[i] = (gt - gt_mean)/gt_std\n",
    "        \n",
    "        gt_tensor = torch.tensor(gt_ls)\n",
    "\n",
    "        # compute loss and gradient descent\n",
    "        a_prob = policy_net(S) # pi(a_t|s_t)\n",
    "        a_dist = torch.distributions.Categorical(a_prob) # pi(a_t|s_t)\n",
    "        log_policy_dist = a_dist.log_prob(A) # A : true actions\n",
    "\n",
    "        loss = torch.sum(-log_policy_dist * gt_tensor)  # -score_function * at\n",
    "        return loss\n",
    "        \n",
    "    def train(self, n_episode):\n",
    "        for i in range(n_episode):\n",
    "            terminal = self.run_episode()\n",
    "            if terminal :\n",
    "                print('학습을 종료합니다.')\n",
    "                break\n",
    "            loss = self.compute_loss()\n",
    "            \n",
    "            # train\n",
    "            self.optim.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = Fitter(policy_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward : 16.0\n",
      "Total reward : 33.0\n",
      "Total reward : 10.0\n",
      "Total reward : 18.0\n",
      "Total reward : 13.0\n",
      "Total reward : 52.0\n",
      "Total reward : 44.0\n",
      "Total reward : 20.0\n",
      "Total reward : 45.0\n",
      "Total reward : 19.0\n",
      "Total reward : 37.0\n",
      "Total reward : 22.0\n",
      "Total reward : 41.0\n",
      "Total reward : 14.0\n",
      "Total reward : 57.0\n",
      "Total reward : 38.0\n",
      "Total reward : 27.0\n",
      "Total reward : 39.0\n",
      "Total reward : 15.0\n",
      "Total reward : 43.0\n",
      "Total reward : 46.0\n",
      "Total reward : 121.0\n",
      "Total reward : 58.0\n",
      "Total reward : 59.0\n",
      "Total reward : 21.0\n",
      "Total reward : 17.0\n",
      "Total reward : 47.0\n",
      "Total reward : 57.0\n",
      "Total reward : 63.0\n",
      "Total reward : 152.0\n",
      "Total reward : 38.0\n",
      "Total reward : 15.0\n",
      "Total reward : 73.0\n",
      "Total reward : 76.0\n",
      "Total reward : 21.0\n",
      "Total reward : 30.0\n",
      "Total reward : 31.0\n",
      "Total reward : 70.0\n",
      "Total reward : 13.0\n",
      "Total reward : 24.0\n",
      "Total reward : 180.0\n",
      "Total reward : 13.0\n",
      "Total reward : 18.0\n",
      "Total reward : 18.0\n",
      "Total reward : 101.0\n",
      "Total reward : 37.0\n",
      "Total reward : 38.0\n",
      "Total reward : 17.0\n",
      "Total reward : 41.0\n",
      "Total reward : 59.0\n",
      "Total reward : 44.0\n",
      "Total reward : 41.0\n",
      "Total reward : 20.0\n",
      "Total reward : 67.0\n",
      "Total reward : 24.0\n",
      "Total reward : 35.0\n",
      "Total reward : 89.0\n",
      "Total reward : 46.0\n",
      "Total reward : 38.0\n",
      "Total reward : 57.0\n",
      "Total reward : 38.0\n",
      "Total reward : 27.0\n",
      "Total reward : 41.0\n",
      "Total reward : 87.0\n",
      "Total reward : 30.0\n",
      "Total reward : 50.0\n",
      "Total reward : 63.0\n",
      "Total reward : 49.0\n",
      "Total reward : 37.0\n",
      "Total reward : 64.0\n",
      "Total reward : 44.0\n",
      "Total reward : 67.0\n",
      "Total reward : 80.0\n",
      "Total reward : 115.0\n",
      "Total reward : 117.0\n",
      "Total reward : 82.0\n",
      "Total reward : 163.0\n",
      "Total reward : 137.0\n",
      "Total reward : 112.0\n",
      "Total reward : 282.0\n",
      "Total reward : 548.0\n",
      "Total reward : 37.0\n",
      "Total reward : 145.0\n",
      "Total reward : 62.0\n",
      "Total reward : 55.0\n",
      "Total reward : 117.0\n",
      "Total reward : 276.0\n",
      "Total reward : 259.0\n",
      "Total reward : 205.0\n",
      "Total reward : 155.0\n",
      "Total reward : 78.0\n",
      "Total reward : 182.0\n",
      "Total reward : 468.0\n",
      "Total reward : 332.0\n",
      "Total reward : 86.0\n",
      "Total reward : 369.0\n",
      "Total reward : 307.0\n",
      "Total reward : 158.0\n",
      "Total reward : 156.0\n",
      "Total reward : 454.0\n",
      "Total reward : 602.0\n",
      "Total reward : 655.0\n",
      "Total reward : 718.0\n",
      "Total reward : 556.0\n",
      "Total reward : 249.0\n",
      "Total reward : 160.0\n",
      "Total reward : 144.0\n",
      "Total reward : 332.0\n",
      "Total reward : 166.0\n",
      "Total reward : 206.0\n",
      "Total reward : 364.0\n",
      "Total reward : 129.0\n",
      "Total reward : 152.0\n",
      "Total reward : 165.0\n",
      "Total reward : 184.0\n",
      "Total reward : 144.0\n",
      "Total reward : 159.0\n",
      "Total reward : 106.0\n",
      "Total reward : 264.0\n",
      "Total reward : 108.0\n",
      "Total reward : 114.0\n",
      "Total reward : 120.0\n",
      "Total reward : 83.0\n",
      "Total reward : 111.0\n",
      "Total reward : 71.0\n",
      "Total reward : 98.0\n",
      "Total reward : 80.0\n",
      "Total reward : 65.0\n",
      "Total reward : 73.0\n",
      "Total reward : 63.0\n",
      "Total reward : 47.0\n",
      "Total reward : 69.0\n",
      "Total reward : 60.0\n",
      "Total reward : 73.0\n",
      "Total reward : 74.0\n",
      "Total reward : 96.0\n",
      "Total reward : 81.0\n",
      "Total reward : 111.0\n",
      "Total reward : 69.0\n",
      "Total reward : 117.0\n",
      "Total reward : 80.0\n",
      "Total reward : 99.0\n",
      "Total reward : 61.0\n",
      "Total reward : 107.0\n",
      "Total reward : 101.0\n",
      "Total reward : 48.0\n",
      "Total reward : 69.0\n",
      "Total reward : 54.0\n",
      "Total reward : 103.0\n",
      "Total reward : 91.0\n",
      "Total reward : 131.0\n",
      "Total reward : 108.0\n",
      "Total reward : 62.0\n",
      "Total reward : 93.0\n",
      "Total reward : 196.0\n",
      "Total reward : 65.0\n",
      "Total reward : 165.0\n",
      "Total reward : 99.0\n",
      "Total reward : 90.0\n",
      "Total reward : 83.0\n",
      "Total reward : 111.0\n",
      "Total reward : 111.0\n",
      "Total reward : 275.0\n",
      "Total reward : 99.0\n",
      "Total reward : 121.0\n",
      "Total reward : 122.0\n",
      "Total reward : 196.0\n",
      "Total reward : 215.0\n",
      "Total reward : 114.0\n",
      "Total reward : 162.0\n",
      "Total reward : 114.0\n",
      "Total reward : 279.0\n",
      "Total reward : 257.0\n",
      "Total reward : 285.0\n",
      "Total reward : 211.0\n",
      "Total reward : 1162.0\n",
      "Total reward : 84.0\n",
      "Total reward : 171.0\n",
      "Total reward : 197.0\n",
      "Total reward : 522.0\n",
      "Total reward : 158.0\n",
      "Total reward : 318.0\n",
      "Total reward : 501.0\n",
      "Total reward : 1115.0\n",
      "Total reward : 2405.0\n",
      "very nice!\n",
      "학습을 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "n_episode = 1000\n",
    "\n",
    "fitter.train(n_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
