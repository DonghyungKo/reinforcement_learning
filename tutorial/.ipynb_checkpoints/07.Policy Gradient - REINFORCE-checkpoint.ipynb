{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.envs.registration import register\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random \n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        if np.ndim(state) and np.ndim(next_state) == 1:\n",
    "            state = np.expand_dims(state, 0)\n",
    "            next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.deque.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    #def sample(self, batch_size):\n",
    "    #    state, action, reward, next_state, done = zip(*random.sample(self.deque, batch_size))\n",
    "    #    return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def pop_episode(self):\n",
    "        state, action, reward, next_state, done = zip(*[self.deque.popleft() for _ in range(len(self))])\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def reset(self):\n",
    "        [self.deque.pop() for _ in range(len(self))]\n",
    "        return\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(PolicyNet, self).__init__()     \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def convert_to_tensor(self, x):\n",
    "        return torch.tensor(x, dtype=torch.float)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        if type(state) != torch.tensor:\n",
    "            state = self.convert_to_tensor(state)\n",
    "        \n",
    "        if state.dim() == 1:\n",
    "            state.unsqueeze_(0)\n",
    "        \n",
    "        out = self.lin(state)\n",
    "        softmax = torch.softmax(out, dim=1)\n",
    "        return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = env.observation_space.shape[0] #4\n",
    "hidden_dim = 64\n",
    "out_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = PolicyNet(input_dim, hidden_dim, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter():\n",
    "    def __init__(self, policy_net):\n",
    "        self.gamma = 0.99\n",
    "        self.policy_net = policy_net\n",
    "        self.lr = 0.01\n",
    "        self.optim = torch.optim.Adam(self.policy_net.parameters(), lr=self.lr)\n",
    "        self.replay_memory = ReplayMemory(capacity=10000)    \n",
    "        \n",
    "    def run_episode(self):\n",
    "        # reset episode\n",
    "        s = env.reset()\n",
    "        self.replay_memory.reset()\n",
    "        self.r_sum = 0\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            a_prob = self.policy_net(s) # pi(a|s)\n",
    "            a_dist = torch.distributions.Categorical(a_prob) # pi(a|s)\n",
    "            a = a_dist.sample().item() # a\n",
    "            s_new, r, done, _ = env.step(a)\n",
    "            self.r_sum += r\n",
    "            \n",
    "            if done :\n",
    "                r = -10\n",
    "            \n",
    "            self.replay_memory.append(s, a, r, s_new, done)\n",
    "            s = s_new\n",
    "            \n",
    "            if self.r_sum > 10000:\n",
    "                print('very nice!')\n",
    "                return True\n",
    "            \n",
    "        print('Total reward : %s'%self.r_sum)\n",
    "        return\n",
    "    \n",
    "    def compute_loss(self):\n",
    "        # get mini-batch from replay-memory\n",
    "        S, A, R, S_next, D = self.replay_memory.pop_episode()\n",
    "        A = torch.tensor(A, dtype=torch.long)\n",
    "        \n",
    "        # calculate Gt\n",
    "        gt_ls = []\n",
    "        for i, r in enumerate(reversed(R)):\n",
    "            if i == 0:\n",
    "                gt = r\n",
    "            else:\n",
    "                gt = self.gamma*previous_gt + r\n",
    "            gt_ls.insert(0, gt)\n",
    "            previous_gt = gt\n",
    "            \n",
    "        # normalize Gt\n",
    "        gt_mean = np.mean(gt_ls)\n",
    "        gt_std = np.std(gt_ls)\n",
    "        for i, gt in enumerate(gt_ls):\n",
    "            gt_ls[i] = (gt - gt_mean) / gt_std\n",
    "        \n",
    "        gt_tensor = torch.tensor(gt_ls)\n",
    "\n",
    "        # compute loss and gradient descent\n",
    "        a_prob = policy_net(S) # pi(a_t|s_t)\n",
    "        a_dist = torch.distributions.Categorical(a_prob) # pi(a_t|s_t)\n",
    "        log_policy_dist = a_dist.log_prob(A) # A : true actions\n",
    "\n",
    "        loss = torch.sum(-log_policy_dist * gt_tensor)  # -score_function * gt\n",
    "        return loss\n",
    "        \n",
    "    def train(self, n_episode):\n",
    "        for i in range(n_episode):\n",
    "            terminal = self.run_episode()\n",
    "            if terminal :\n",
    "                print('학습을 종료합니다.')\n",
    "                break\n",
    "            loss = self.compute_loss()\n",
    "            \n",
    "            # train\n",
    "            self.optim.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = Fitter(policy_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward : 21.0\n",
      "Total reward : 23.0\n",
      "Total reward : 12.0\n",
      "Total reward : 18.0\n",
      "Total reward : 15.0\n",
      "Total reward : 12.0\n",
      "Total reward : 13.0\n",
      "Total reward : 14.0\n",
      "Total reward : 17.0\n",
      "Total reward : 15.0\n",
      "Total reward : 21.0\n",
      "Total reward : 12.0\n",
      "Total reward : 36.0\n",
      "Total reward : 36.0\n",
      "Total reward : 20.0\n",
      "Total reward : 16.0\n",
      "Total reward : 22.0\n",
      "Total reward : 18.0\n",
      "Total reward : 76.0\n",
      "Total reward : 31.0\n",
      "Total reward : 58.0\n",
      "Total reward : 18.0\n",
      "Total reward : 50.0\n",
      "Total reward : 65.0\n",
      "Total reward : 87.0\n",
      "Total reward : 61.0\n",
      "Total reward : 81.0\n",
      "Total reward : 31.0\n",
      "Total reward : 44.0\n",
      "Total reward : 78.0\n",
      "Total reward : 26.0\n",
      "Total reward : 93.0\n",
      "Total reward : 46.0\n",
      "Total reward : 50.0\n",
      "Total reward : 41.0\n",
      "Total reward : 66.0\n",
      "Total reward : 26.0\n",
      "Total reward : 52.0\n",
      "Total reward : 35.0\n",
      "Total reward : 39.0\n",
      "Total reward : 81.0\n",
      "Total reward : 38.0\n",
      "Total reward : 69.0\n",
      "Total reward : 36.0\n",
      "Total reward : 47.0\n",
      "Total reward : 118.0\n",
      "Total reward : 78.0\n",
      "Total reward : 77.0\n",
      "Total reward : 79.0\n",
      "Total reward : 32.0\n",
      "Total reward : 52.0\n",
      "Total reward : 51.0\n",
      "Total reward : 51.0\n",
      "Total reward : 37.0\n",
      "Total reward : 44.0\n",
      "Total reward : 30.0\n",
      "Total reward : 51.0\n",
      "Total reward : 58.0\n",
      "Total reward : 51.0\n",
      "Total reward : 76.0\n",
      "Total reward : 36.0\n",
      "Total reward : 78.0\n",
      "Total reward : 74.0\n",
      "Total reward : 86.0\n",
      "Total reward : 86.0\n",
      "Total reward : 42.0\n",
      "Total reward : 103.0\n",
      "Total reward : 132.0\n",
      "Total reward : 63.0\n",
      "Total reward : 76.0\n",
      "Total reward : 152.0\n",
      "Total reward : 62.0\n",
      "Total reward : 55.0\n",
      "Total reward : 62.0\n",
      "Total reward : 96.0\n",
      "Total reward : 135.0\n",
      "Total reward : 70.0\n",
      "Total reward : 134.0\n",
      "Total reward : 112.0\n",
      "Total reward : 166.0\n",
      "Total reward : 199.0\n",
      "Total reward : 168.0\n",
      "Total reward : 401.0\n",
      "Total reward : 509.0\n",
      "Total reward : 534.0\n",
      "Total reward : 184.0\n",
      "Total reward : 197.0\n",
      "Total reward : 131.0\n",
      "Total reward : 138.0\n",
      "Total reward : 111.0\n",
      "Total reward : 20.0\n",
      "Total reward : 113.0\n",
      "Total reward : 135.0\n",
      "Total reward : 18.0\n",
      "Total reward : 181.0\n",
      "Total reward : 134.0\n",
      "Total reward : 18.0\n",
      "Total reward : 228.0\n",
      "Total reward : 222.0\n",
      "Total reward : 70.0\n",
      "Total reward : 146.0\n",
      "Total reward : 32.0\n",
      "Total reward : 280.0\n",
      "Total reward : 274.0\n",
      "Total reward : 281.0\n",
      "Total reward : 318.0\n",
      "Total reward : 350.0\n",
      "Total reward : 269.0\n",
      "Total reward : 422.0\n",
      "Total reward : 528.0\n",
      "Total reward : 464.0\n",
      "Total reward : 393.0\n",
      "Total reward : 507.0\n",
      "Total reward : 499.0\n",
      "Total reward : 2300.0\n",
      "Total reward : 1295.0\n",
      "Total reward : 2937.0\n",
      "Total reward : 1179.0\n",
      "Total reward : 252.0\n",
      "Total reward : 597.0\n",
      "Total reward : 359.0\n",
      "Total reward : 991.0\n",
      "Total reward : 379.0\n",
      "Total reward : 1041.0\n",
      "Total reward : 407.0\n",
      "Total reward : 267.0\n",
      "Total reward : 154.0\n",
      "Total reward : 106.0\n",
      "Total reward : 155.0\n",
      "Total reward : 126.0\n",
      "Total reward : 164.0\n",
      "Total reward : 89.0\n",
      "Total reward : 106.0\n",
      "Total reward : 82.0\n",
      "Total reward : 77.0\n",
      "Total reward : 110.0\n",
      "Total reward : 60.0\n",
      "Total reward : 131.0\n",
      "Total reward : 124.0\n",
      "Total reward : 109.0\n",
      "Total reward : 130.0\n",
      "Total reward : 105.0\n",
      "Total reward : 109.0\n",
      "Total reward : 155.0\n",
      "Total reward : 189.0\n",
      "Total reward : 170.0\n",
      "Total reward : 189.0\n",
      "Total reward : 190.0\n",
      "Total reward : 267.0\n",
      "Total reward : 269.0\n",
      "Total reward : 443.0\n",
      "Total reward : 497.0\n",
      "Total reward : 669.0\n",
      "Total reward : 5571.0\n",
      "very nice!\n",
      "학습을 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "n_episode = 1000\n",
    "\n",
    "fitter.train(n_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
