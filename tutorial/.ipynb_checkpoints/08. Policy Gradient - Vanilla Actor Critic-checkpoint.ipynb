{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.envs.registration import register\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random \n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        if np.ndim(state) and np.ndim(next_state) == 1:\n",
    "            state = np.expand_dims(state, 0)\n",
    "            next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.deque.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.deque, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def pop_episode(self):\n",
    "        state, action, reward, next_state, done = zip(*[self.deque.popleft() for _ in range(len(self))])\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def reset(self):\n",
    "        [self.deque.pop() for _ in range(len(self))]\n",
    "        return\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ActorCritic, self).__init__()     \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.feature_stream = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        \n",
    "        self.actor_lin = nn.Linear(hidden_dim, output_dim)\n",
    "        self.critic_lin = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def to_tensor(self, x):\n",
    "        return torch.tensor(x, dtype=torch.float)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        if type(state) != torch.tensor:\n",
    "            state = self.to_tensor(state)\n",
    "        \n",
    "        if state.dim() == 1:\n",
    "            state.unsqueeze_(0)\n",
    "        \n",
    "        feature = self.feature_stream(state)\n",
    "        \n",
    "        action_prob = self.actor_lin(feature)\n",
    "        action_prob = torch.softmax(action_prob, dim=-1)\n",
    "        advantage = self.critic_lin(feature).squeeze(1) # dim=1\n",
    "        return action_prob, advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter():\n",
    "    def __init__(self, actor_critic):\n",
    "        self.actor_critic = actor_critic\n",
    "        self.gamma = 0.99\n",
    "        self.lr = 0.001\n",
    "        self.optim = torch.optim.Adam(self.actor_critic.parameters(), lr=self.lr)\n",
    "        self.replay_memory = ReplayMemory(capacity=100000)\n",
    "        \n",
    "    def run_episode(self, n_episode):\n",
    "        r_sum_ls = []\n",
    "        \n",
    "        for i in range(n_episode):\n",
    "            # reset episode\n",
    "            s = env.reset()\n",
    "            r_sum = 0\n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                a_prob, adv = self.actor_critic(s) # pi(a|s)\n",
    "                a_dist = torch.distributions.Categorical(a_prob) # pi(a|s)\n",
    "                a = a_dist.sample().item() # a\n",
    "                s_new, r, done, _ = env.step(a)\n",
    "                r_sum += r\n",
    "                \n",
    "                if r_sum > 10000:\n",
    "                    print('Done, No more training!')\n",
    "                    return\n",
    "                    \n",
    "                self.replay_memory.append(s, a, r, s_new, done)\n",
    "                s = s_new\n",
    "                \n",
    "                \n",
    "            loss = self.compute_loss()\n",
    "            self.train(loss)\n",
    "            \n",
    "            r_sum_ls.append(r_sum)\n",
    "            if i % 100 == 0:\n",
    "                print('Reward Sum = %.02f'%(np.mean(r_sum_ls)))\n",
    "                r_sum_ls = []\n",
    "        return\n",
    "    \n",
    "    def compute_loss(self):\n",
    "        # get mini-batch from replay-memory\n",
    "        S, A, R, S_next, D = self.replay_memory.pop_episode()\n",
    "        A = torch.tensor(A, dtype=torch.long)\n",
    "        R = torch.tensor(R, dtype=torch.float)\n",
    "        \n",
    "        # compute loss and gradient descent\n",
    "        a_prob, v = actor_critic(S) # pi(a_t|s_t)\n",
    "        a_dist = torch.distributions.Categorical(a_prob) # pi(a_t|s_t)\n",
    "        log_a_prob = a_dist.log_prob(A) # A : true actions\n",
    "        _, v_next = actor_critic(S_next)\n",
    "        \n",
    "        TD_error = R + self.gamma*v_next - v\n",
    "        actor_loss = torch.mean(-log_a_prob*TD_error.data)\n",
    "        critic_loss = torch.mean(TD_error*TD_error.data)\n",
    "        loss = actor_loss + critic_loss\n",
    "        return loss\n",
    "        \n",
    "    def train(self, loss):\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 4\n",
    "hidden_dim = 128\n",
    "out_dim = env.action_space.n\n",
    "\n",
    "actor_critic = ActorCritic(input_dim, hidden_dim, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = Fitter(actor_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Sum = 14.00\n",
      "Reward Sum = 30.01\n",
      "Reward Sum = 35.11\n",
      "Reward Sum = 114.11\n",
      "Reward Sum = 91.09\n",
      "Reward Sum = 119.15\n",
      "Reward Sum = 103.92\n",
      "Reward Sum = 126.86\n",
      "Reward Sum = 146.68\n",
      "Reward Sum = 123.08\n",
      "Reward Sum = 125.30\n",
      "Reward Sum = 103.57\n",
      "Reward Sum = 111.17\n",
      "Reward Sum = 91.85\n",
      "Reward Sum = 91.20\n",
      "Reward Sum = 103.68\n",
      "Reward Sum = 79.19\n",
      "Reward Sum = 81.78\n",
      "Reward Sum = 92.90\n",
      "Reward Sum = 79.97\n",
      "Reward Sum = 86.00\n",
      "Reward Sum = 94.51\n",
      "Reward Sum = 89.13\n",
      "Reward Sum = 77.91\n",
      "Reward Sum = 81.70\n",
      "Reward Sum = 82.23\n",
      "Reward Sum = 88.66\n",
      "Reward Sum = 90.37\n",
      "Reward Sum = 72.17\n",
      "Reward Sum = 77.80\n",
      "Reward Sum = 70.82\n",
      "Reward Sum = 89.99\n",
      "Reward Sum = 60.00\n",
      "Reward Sum = 53.63\n",
      "Reward Sum = 50.95\n",
      "Reward Sum = 50.51\n",
      "Reward Sum = 42.71\n",
      "Reward Sum = 46.07\n",
      "Reward Sum = 47.69\n",
      "Reward Sum = 44.25\n",
      "Reward Sum = 42.06\n",
      "Reward Sum = 53.08\n",
      "Reward Sum = 69.05\n",
      "Reward Sum = 91.80\n",
      "Reward Sum = 78.04\n",
      "Reward Sum = 49.10\n",
      "Reward Sum = 81.26\n",
      "Reward Sum = 75.40\n",
      "Reward Sum = 76.89\n",
      "Reward Sum = 93.82\n",
      "Reward Sum = 142.54\n",
      "Reward Sum = 156.16\n",
      "Reward Sum = 136.33\n",
      "Reward Sum = 144.27\n",
      "Reward Sum = 138.15\n",
      "Reward Sum = 152.40\n",
      "Reward Sum = 102.07\n",
      "Reward Sum = 9.41\n",
      "Reward Sum = 9.36\n",
      "Reward Sum = 67.07\n",
      "Reward Sum = 174.78\n",
      "Reward Sum = 176.90\n",
      "Reward Sum = 173.90\n",
      "Reward Sum = 179.23\n",
      "Reward Sum = 181.18\n",
      "Reward Sum = 203.71\n",
      "Reward Sum = 200.93\n",
      "Reward Sum = 197.55\n",
      "Reward Sum = 250.85\n",
      "Reward Sum = 256.54\n",
      "Reward Sum = 264.90\n",
      "Reward Sum = 259.52\n",
      "Reward Sum = 298.70\n",
      "Reward Sum = 303.66\n",
      "Reward Sum = 310.39\n",
      "Reward Sum = 329.69\n",
      "Reward Sum = 381.95\n",
      "Reward Sum = 445.04\n",
      "Reward Sum = 403.82\n",
      "Reward Sum = 488.72\n",
      "Reward Sum = 574.48\n",
      "Reward Sum = 633.33\n",
      "Reward Sum = 741.38\n",
      "Reward Sum = 804.60\n",
      "Reward Sum = 918.68\n",
      "Reward Sum = 958.59\n",
      "Reward Sum = 1022.90\n",
      "Reward Sum = 1280.67\n",
      "Reward Sum = 1428.35\n",
      "Reward Sum = 1423.35\n",
      "Done, No more training!\n"
     ]
    }
   ],
   "source": [
    "n_episode = 10000\n",
    "\n",
    "fitter.run_episode(n_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
