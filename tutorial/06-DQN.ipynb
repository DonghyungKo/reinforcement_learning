{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Correlation between samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Non-stationary targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "1. Go deep\n",
    "2. #### **experience replay**\n",
    "3. Seperate Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.envs.registration import register\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = env.observation_space.shape[0]\n",
    "hidden_dim = 128\n",
    "output_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(DQN, self).__init__()     \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def convert_to_tensor(sefl, x):\n",
    "        return torch.tensor(x, dtype=torch.float)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        if type(state) != torch.tensor:\n",
    "            state = self.convert_to_tensor(state)\n",
    "        \n",
    "        if state.dim() == 1:\n",
    "            state.unsqueeze_(0)\n",
    "        \n",
    "        out = self.lin(state)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter():\n",
    "    def __init__(self, behaviorDQN, targetDQN):\n",
    "        self.gamma = 0.9\n",
    "        self.behaviorDQN = behaviorDQN\n",
    "        self.targetDQN = targetDQN\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optim = torch.optim.Adam(self.behaviorDQN.parameters(), lr=0.1)\n",
    "        \n",
    "    def train(self, train_batch):\n",
    "        x_stack = torch.empty([0, self.behaviorDQN.input_dim])\n",
    "        y_stack = torch.empty([0, self.behaviorDQN.output_dim])\n",
    "\n",
    "        for s, a, r, s_new, done in train_batch:\n",
    "            q_target = self.targetDQN(s)\n",
    "            q_target1 = self.targetDQN(s_new)\n",
    "\n",
    "            if done:\n",
    "                q_target[0, a] = r\n",
    "            else:\n",
    "                q_target[0, a] = r + self.gamma*torch.max(q_target1)\n",
    "            \n",
    "            s_tensor = torch.tensor(s, dtype=torch.float).unsqueeze(0)\n",
    "            x_stack = torch.cat((x_stack, s_tensor), dim=0)\n",
    "            y_stack = torch.cat([y_stack, q_target], dim=0)\n",
    "        \n",
    "        # 예측치(pred)와 목표치(true)\n",
    "        q_behavior = self.behaviorDQN(x_stack)\n",
    "        q_true = y_stack\n",
    "        \n",
    "        self.optim.zero_grad()\n",
    "        loss = self.criterion(q_true, q_behavior)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return loss\n",
    "    \n",
    "    def update_targetDQN(self):\n",
    "        self.targetDQN.load_state_dict(self.behaviorDQN.state_dict())\n",
    "        return \n",
    "    \n",
    "    def bot_test(self):\n",
    "        s = env.reset()\n",
    "        n_step = 0\n",
    "        while True:\n",
    "            a = torch.argmax(self.behaviorDQN(s)).item()\n",
    "            s, r, done, _ = env.step(a)\n",
    "            n_step += 1\n",
    "            if done:\n",
    "                print('Total score: %s'%n_step)\n",
    "                return n_step\n",
    "            \n",
    "            if n_step > 10000:\n",
    "                print('Very nice!')\n",
    "                self.bot_play()\n",
    "                \n",
    "    def bot_play(self):\n",
    "        s = env.reset()\n",
    "        n_step = 0\n",
    "        while True:\n",
    "            a = torch.argmax(self.behaviorDQN(s)).item()\n",
    "            env.render()\n",
    "            s, r, done, _ = env.step(a)\n",
    "            n_step += 1\n",
    "            if done:\n",
    "                print('Total score: %s'%n_step)\n",
    "                env.close()\n",
    "                return n_step\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviorDQN = DQN(input_dim, hidden_dim, output_dim)\n",
    "targetDQN = DQN(input_dim, hidden_dim, output_dim)\n",
    "fitter = Fitter(behaviorDQN, targetDQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store replay train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donghyungko/anaconda3/envs/RL/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score: 301\n",
      "Total score: 122\n",
      "Total score: 45\n",
      "Total score: 41\n",
      "Total score: 69\n",
      "Total score: 63\n",
      "Total score: 57\n",
      "Total score: 87\n",
      "Total score: 54\n",
      "Total score: 95\n",
      "Total score: 109\n",
      "Total score: 180\n",
      "Total score: 130\n",
      "Total score: 158\n",
      "Total score: 110\n",
      "Total score: 175\n",
      "Total score: 99\n",
      "Total score: 123\n",
      "Total score: 139\n",
      "Total score: 33\n",
      "Total score: 127\n",
      "Total score: 23\n",
      "Total score: 248\n",
      "Total score: 228\n",
      "Total score: 1704\n",
      "Total score: 368\n",
      "Total score: 281\n",
      "Total score: 498\n",
      "Total score: 640\n",
      "Total score: 227\n",
      "Total score: 181\n",
      "Total score: 165\n",
      "Total score: 149\n",
      "Total score: 109\n",
      "Very nice!\n",
      "Total score: 176292\n",
      "Total score: 10002\n",
      "Very nice!\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "train_epoch = 1000\n",
    "replay_memory = deque()\n",
    "max_replay = 50000\n",
    "reward_ls = []\n",
    "\n",
    "for i in range(train_epoch):    \n",
    "    s = env.reset()\n",
    "    e_rate = 1. / ((i/10)+1)  # exploration rate (random action)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Choose an action by e-greedy\n",
    "        q_pred = fitter.behaviorDQN(s)\n",
    "        \n",
    "        if np.random.rand(1) < e_rate :\n",
    "            a = env.action_space.sample()\n",
    "        else:\n",
    "            a = torch.argmax(q_pred).item()\n",
    "        \n",
    "        # action 실행 후, replay_memory에 기록\n",
    "        s_new, r, done, _ = env.step(a)\n",
    "        \n",
    "        if done:\n",
    "            r = -10\n",
    "        \n",
    "        replay_memory.append([s, a , r, s_new, done])\n",
    "        if len(replay_memory) > max_replay:\n",
    "            replay_memory.popleft()\n",
    "        \n",
    "        s = s_new\n",
    "               \n",
    "    # train every 10 epoch\n",
    "    if i%10 == 1:\n",
    "        fitter.update_targetDQN()\n",
    "        \n",
    "        for _ in range(50):\n",
    "            batch_size = np.min([10, len(replay_memory)])\n",
    "            mini_batch = random.sample(replay_memory, batch_size)\n",
    "            loss = fitter.train(mini_batch)\n",
    "    \n",
    "    if i%10 == 1:\n",
    "        reward = fitter.bot_test()\n",
    "        reward_ls.append(reward)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
