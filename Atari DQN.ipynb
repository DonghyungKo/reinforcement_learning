{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import the gym module\n",
    "import gym\n",
    "import numpy as np\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a breakout environment\n",
    "env = gym.make('BreakoutDeterministic-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "    img = np.mean(img, axis=2).astype(np.uint8) # to gray, uint8 for low memory\n",
    "    img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "    img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "ss = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(s, ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(s, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    a = env.action_space.sample()\n",
    "    s2, _, done, _ = env.step(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6c18b78ef0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6c18b0f630>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(preprocess(s2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random \n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "    \n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        self.deque.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.deque, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryTransformer(object):\n",
    "    def __init__(self, n_history=4):\n",
    "        self.n_history = n_history\n",
    "        self.deque = deque(maxlen=n_history)\n",
    "        self.env = gym.make('BreakoutDeterministic-v4')\n",
    "        \n",
    "        # 최초에는 처음 화면(env.reset) 0으로 채워둠\n",
    "        padding_state = preprocess(self.env.reset())\n",
    "        for _ in range(n_history):\n",
    "            self.deque.append(padding_state)\n",
    "    \n",
    "    def preprocess(self, img):\n",
    "        # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "        img = np.mean(img, axis=2).astype(np.uint8) # to gray, uint8 for low memory\n",
    "        img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "        img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "        return img\n",
    "    \n",
    "    def transform(self, s):\n",
    "        # transform할 때 마다, 현재 state를 기록하고, 과거 state를 밀어내면서 저장함\n",
    "        self.deque.append(self.preprocess(s))\n",
    "        return np.stack([self.deque[i] for i in range(self.n_history)], axis=1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariBreakout(nn.Module):\n",
    "    def __init__(self, in_dim, n_action):\n",
    "        super(AtariBreakout, self).__init__()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, 32, (8, 8), stride=4), nn.ReLU(), # conv 1\n",
    "            nn.Conv2d(32, 64, (4, 4), stride=2), nn.ReLU(), # conv 2\n",
    "            nn.Conv2d(64, 64, (3, 3), stride=1), nn.ReLU() # conv 3\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*6*6, 512), nn.ReLU(), # hidden layer\n",
    "            nn.Linear(512, n_action)\n",
    "        )\n",
    "        \n",
    "    def to_tensor(self, img):\n",
    "        img = torch.tensor(img, dtype=torch.float32, device=self.device).cuda(non_blocking=True) # to tensor\n",
    "        img /= 255                                   # normalize into 0-1\n",
    "        while img.dim() < 4 :                        # 4-dim\n",
    "            img = img.unsqueeze(0)\n",
    "        return img\n",
    "        \n",
    "    def forward(self, frames):\n",
    "        frames = self.to_tensor(frames) \n",
    "        conved = self.conv(frames)\n",
    "        conved = conved.view(conved.size(0), -1)\n",
    "        output = self.fc(conved)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtariBreakout(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2304, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviorAtari = AtariBreakout(in_dim=4, n_action=4)\n",
    "targetAtari = AtariBreakout(in_dim=4, n_action=4)\n",
    "\n",
    "behaviorAtari.to(behaviorAtari.device) # model to cuda\n",
    "targetAtari.to(targetAtari.device)     # model to cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter():\n",
    "    def __init__(self, behaviorAtari, targetAtari, n_history=4):\n",
    "        self.behaviorAtari = behaviorAtari\n",
    "        self.targetAtari = targetAtari\n",
    "        self.out_dim = env.action_space.n\n",
    "        self.env = gym.make('BreakoutDeterministic-v4')\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        self.criterion = nn.SmoothL1Loss() #huber loss \n",
    "        self.optim = torch.optim.RMSprop(self.behaviorAtari.parameters(), lr=0.0025)\n",
    "        \n",
    "    def train(self, train_batch):\n",
    "        batch_size = len(train_batch)\n",
    "        S = np.array([tup[0] for tup in train_batch]).squeeze(1) # states\n",
    "        A = np.array([tup[1] for tup in train_batch]) # actions\n",
    "        R = np.array([tup[2] for tup in train_batch]) # rewards\n",
    "        S_next = np.array([tup[3] for tup in train_batch]).squeeze(1) # next_states \n",
    "        D = np.array([tup[4] for tup in train_batch]) # dones\n",
    "        \n",
    "        q_targets = self.targetAtari(S) # cal Q of current state with targetDQN\n",
    "        q_targets1 = self.targetAtari(S_next) # cal Q of next state with targetDQN\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            a, r, done = A[i], R[i], D[i]\n",
    "            \n",
    "            if done:\n",
    "                q_targets[i, a] = r\n",
    "            else:\n",
    "                q_targets[i, a] = r + self.gamma*torch.max(q_targets1[i])\n",
    "            \n",
    "        # 예측치(pred)와 목표치(true)\n",
    "        q_behavior = self.behaviorAtari(S)\n",
    "        \n",
    "        self.optim.zero_grad()\n",
    "        loss = self.criterion(q_targets, q_behavior)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.targetAtari.load_state_dict(self.behaviorAtari.state_dict())\n",
    "        return \n",
    "    \n",
    "    def test(self):\n",
    "        temp_hist_transformer = HistoryTransformer()\n",
    "        initial_state = self.env.reset()\n",
    "        s = self.env.reset()\n",
    "        n_step = 0\n",
    "        r_sum = 0\n",
    "        done=False\n",
    "        \n",
    "        while not done:\n",
    "            # 초기 화면에서는 1,2,3 중 임의의 액션을 실행\n",
    "            # 게임이 시작된 후, 공을 놓치면, life가 깍이고 다시 초기화면으로 돌아옴 \n",
    "            if np.array_equal(s, initial_state):\n",
    "                a = random.choice([1, 2, 3])\n",
    "            else:\n",
    "                s_trans = temp_hist_transformer.transform(s)\n",
    "                a = torch.argmax(self.behaviorAtari(s_trans)).item()\n",
    "            \n",
    "            s, r, done, _ = self.env.step(a)\n",
    "            r_sum += r\n",
    "            n_step += 1\n",
    "            \n",
    "        print('Total Step: %s \\t Total Score: %s'%(n_step, r_sum))\n",
    "        return r_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = Fitter(behaviorAtari, targetAtari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a breakout environment\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "n_frame = 0\n",
    "n_episode = 0\n",
    "batch_size = 32\n",
    "\n",
    "memory_size = 1000000\n",
    "replay_memory = ReplayMemory(memory_size)\n",
    "\n",
    "min_replay = 50000 # 50,000 in the paper\n",
    "reward_ls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_decay(episode_idx):\n",
    "    return np.max([1 - 9.0*1e-07*episode_idx, 0.1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_Frame : 1142478\n",
      "Total Step: 100000 \t Total Score: 0.0\n",
      "N_Frame : 1142814\n",
      "Total Step: 100000 \t Total Score: 0.0\n",
      "N_Frame : 1143095\n",
      "Total Step: 100000 \t Total Score: 0.0\n",
      "N_Frame : 1143501\n",
      "Total Step: 100000 \t Total Score: 0.0\n",
      "N_Frame : 1143795\n",
      "Total Step: 259 \t Total Score: 4.0\n",
      "N_Frame : 1144043\n",
      "Total Step: 100000 \t Total Score: 0.0\n",
      "N_Frame : 1144432\n",
      "Total Step: 100000 \t Total Score: 0.0\n",
      "N_Frame : 1144632\n",
      "Total Step: 151 \t Total Score: 1.0\n",
      "N_Frame : 1144864\n",
      "Total Step: 100000 \t Total Score: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-6e85b6bce5a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mmini_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-db8c465e1d7d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_batch)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# dones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mq_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargetAtari\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# cal Q of current state with targetDQN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mq_targets1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargetAtari\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_next\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# cal Q of next state with targetDQN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-812bfbf03248>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mconved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mconved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconved\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconved\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-812bfbf03248>\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# to tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m255\u001b[0m                                   \u001b[0;31m# normalize into 0-1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m:\u001b[0m                        \u001b[0;31m# 4-dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while n_frame < 15000000 :\n",
    "    n_episode += 1\n",
    "    hist_transformer = HistoryTransformer() # history recorder (past 4 state)\n",
    "    s = env.reset()\n",
    "    s = hist_transformer.transform(s)\n",
    "    \n",
    "    e = epsilon_decay(n_frame)\n",
    "    done = False\n",
    "    n_step = 0\n",
    "    \n",
    "    while not done:\n",
    "        n_frame += 1\n",
    "        n_step += 1\n",
    "        \n",
    "        # 초기 화면에서도 역시 일단 움직여야 게임이 시작됨\n",
    "        if n_step == 1:\n",
    "            a = random.choice([1, 2, 3])\n",
    "        else:\n",
    "            # Choose an action by e-greedy\n",
    "            q_pred = fitter.behaviorAtari(s)\n",
    "            \n",
    "            if np.random.rand(1) < e :\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                a = torch.argmax(q_pred).item()\n",
    "        \n",
    "        # action 실행 후, replay_memory에 기록\n",
    "        s_next, r, done, _ = env.step(a)\n",
    "        if done:  \n",
    "            r = -10\n",
    "        \n",
    "        # s_next를 변형 후 저장\n",
    "        s_next = hist_transformer.transform(s_next)\n",
    "        replay_memory.append(s, a , r, s_next, done)\n",
    "        \n",
    "        s = s_next\n",
    "        \n",
    "        # minimum replay가 min_replay클 때까지는 학습 안함\n",
    "        if len(replay_memory) < min_replay:\n",
    "            continue\n",
    "        \n",
    "        # 4-th step 마다, 동일한 batch를 4번씩 학습\n",
    "        if n_step%4 == 0:\n",
    "            for _ in range(4):\n",
    "                if len(replay_memory) == memory_size:\n",
    "                    batch_size = 128\n",
    "                mini_batch = replay_memory.sample(batch_size)\n",
    "                loss = fitter.train(mini_batch)\n",
    "    \n",
    "\n",
    "    # minimum replay가 min_replay클 때까지는 테스트와 업데이트 안함\n",
    "    if len(replay_memory) < min_replay:\n",
    "        continue\n",
    "\n",
    "    # update target every 10 episode\n",
    "    if n_episode%10 == 0:\n",
    "        fitter.update_target()\n",
    "        \n",
    "    # test every 10 episode\n",
    "    if n_episode%1 == 0:\n",
    "        print('N_Frame : %s'%n_frame)\n",
    "        reward = fitter.test()\n",
    "        reward_ls.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
